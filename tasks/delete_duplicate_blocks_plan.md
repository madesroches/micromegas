# Plan: Delete Duplicate Blocks UDF

## Background

The ingestion server can insert duplicate blocks because the `blocks` table has no unique constraints on `block_id` or any combination of fields. Before making the ingestion server idempotent, we need a way to clean up existing duplicates.

## Goal

Create an undocumented async scalar UDF `delete_duplicate_blocks()` that:
1. Finds duplicate blocks within the query time range (specified out of band via session context)
2. Keeps one copy (the one with the earliest `insert_time`)
3. Deletes the duplicate rows from the `blocks` table (no blob deletion needed - duplicates point to same blob)
4. Returns a status message indicating how many duplicates were removed

## Duplicate Definition

A duplicate is defined as multiple rows with the same `block_id`. We keep the row with the earliest `insert_time` and delete all others.

**Rationale**: The `block_id` is a UUID generated by the source application and should be unique. The same block arriving multiple times (e.g., due to retries) will have the same `block_id` but different `insert_time` values.

## Implementation Steps

### Step 1: Create the UDF file

Create `/rust/analytics/src/lakehouse/delete_duplicate_blocks_udf.rs`

**Structure**:
- Struct `DeleteDuplicateBlocks` implementing `AsyncScalarUDFImpl`
- Takes no arguments - query time range is passed via constructor (like `ViewInstanceTableFunction`)
- Stores `query_range: Option<TimeRange>` and `lake: Arc<DataLakeConnection>`
- Returns `Utf8` (status message)
- Uses `Volatility::Volatile` since it modifies data

### Step 2: Implement deletion SQL

**Note**: No blob deletion needed - duplicate rows point to the same blob in object storage.

Single query to find and delete duplicates, keeping the earliest by `insert_time`:

```sql
WITH dups AS (
    SELECT block_id, MIN(insert_time) as keep_time
    FROM blocks
    WHERE insert_time >= $1 AND insert_time < $2
    GROUP BY block_id
    HAVING COUNT(*) > 1
)
DELETE FROM blocks b
USING dups d
WHERE b.block_id = d.block_id
AND b.insert_time != d.keep_time
```

1. **Use transactions** for atomicity
2. **Return count** of deleted rows (from `DELETE` result)

### Step 3: Register the UDF

In `rust/analytics/src/lakehouse/query.rs`:
1. Add import for the new module
2. Add UDF registration in `register_lakehouse_functions()`

In `rust/analytics/src/lakehouse/mod.rs`:
1. Add module declaration

### Step 4: Manual testing

Test manually against a live database with duplicates. No unit tests - would require a live PostgreSQL instance.

## Files to Create/Modify

| File | Action |
|------|--------|
| `rust/analytics/src/lakehouse/delete_duplicate_blocks_udf.rs` | Create |
| `rust/analytics/src/lakehouse/mod.rs` | Modify (add module) |
| `rust/analytics/src/lakehouse/query.rs` | Modify (register UDF) |

## UDF Signature

```sql
delete_duplicate_blocks() -> VARCHAR
```

The time range is specified out of band via the query context (passed to the UDF constructor during session setup, similar to `ViewInstanceTableFunction`).

**Example usage**:
```sql
-- Time range set via Python client: client.query(sql, begin, end)
SELECT delete_duplicate_blocks();
-- Returns: "Deleted 42 duplicate blocks"
```

## Reference Files

- Pattern for async UDF: `rust/analytics/src/lakehouse/retire_partition_by_metadata_udf.rs`
- Pattern for receiving query_range: `rust/analytics/src/lakehouse/view_instance_table_function.rs`
- Block schema: `rust/ingestion/src/sql_telemetry_db.rs`
- UDF registration: `rust/analytics/src/lakehouse/query.rs`

## Considerations

1. **Time range via constructor**: The UDF receives `query_range: Option<TimeRange>` in its constructor (like `ViewInstanceTableFunction`), not as SQL arguments
2. **No blob deletion**: Duplicate rows point to the same blob - only delete DB rows
3. **Idempotency**: The UDF itself should be idempotent - running it twice on the same range should be safe
4. **Logging**: Add span instrumentation for observability
5. **No documentation**: As requested, this is an internal/undocumented extension
6. **Transaction safety**: Use a single transaction so failures roll back completely
