{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Micromegas Documentation","text":"<p>Welcome to Micromegas, a unified observability platform designed for high-performance telemetry collection and analytics.</p> <p>\u2192 Source Code on GitHub \u2192 Get Started</p>"},{"location":"#what-is-micromegas","title":"What is Micromegas?","text":"<p>Micromegas is a comprehensive observability solution that provides:</p> <ul> <li>High-performance instrumentation with 20ns overhead per event</li> <li>Unified data collection for logs, metrics, spans, and traces</li> <li>Cost-efficient storage using object storage for raw data</li> <li>Powerful SQL analytics built on Apache DataFusion</li> <li>Real-time and historical analysis capabilities</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#high-performance","title":"\ud83d\ude80 High Performance","text":"<ul> <li>Ultra-low overhead telemetry collection (20ns per event)</li> <li>Supports up to 100k events/second per process</li> <li>Thread-local storage for minimal performance impact</li> </ul>"},{"location":"#cost-effective","title":"\ud83d\udcb0 Cost Effective","text":"<ul> <li>Raw data stored in cheap object storage (S3/GCS)</li> <li>Metadata in PostgreSQL for fast queries</li> <li>Pay only for what you query with on-demand ETL</li> </ul>"},{"location":"#unified-observability","title":"\ud83d\udd0d Unified Observability","text":"<ul> <li>Logs, metrics, traces, and spans in a single queryable format</li> <li>SQL interface compatible with existing analytics tools</li> <li>Grafana plugin for visualization and dashboards</li> </ul>"},{"location":"#modern-architecture","title":"\ud83c\udfd7\ufe0f Modern Architecture","text":"<ul> <li>Apache Arrow FlightSQL for efficient data transfer</li> <li>DataFusion-powered analytics engine</li> <li>Lakehouse architecture based on Parquet (columnar format optimized for analytics workloads)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with Micromegas in just a few steps:</p> <ol> <li>Getting Started Guide - Set up your first Micromegas installation</li> <li>Unreal Engine Integration - Add observability to your Unreal Engine games</li> <li>Query Guide - Learn how to query your observability data</li> <li>Architecture Overview - Understand the system design</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#game-development","title":"Game Development","text":"<p>Monitor Unreal Engine games with integrated telemetry for performance, player behavior, and system health.</p>"},{"location":"#application-performance-monitoring","title":"Application Performance Monitoring","text":"<p>Monitor your applications with detailed performance metrics, error tracking, and distributed tracing.</p>"},{"location":"#infrastructure-observability","title":"Infrastructure Observability","text":"<p>Collect and analyze system metrics, logs, and performance data across your entire infrastructure.</p>"},{"location":"#cost-effective-analytics","title":"Cost-Effective Analytics","text":"<p>Store massive amounts of telemetry data cost-effectively while maintaining fast query performance.</p>"},{"location":"#development-debugging","title":"Development &amp; Debugging","text":"<p>Use high-frequency instrumentation to debug performance issues and understand application behavior.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the guides in this documentation</li> <li>GitHub Issues: Report bugs or request features</li> <li>Community: Join discussions and get support</li> </ul>"},{"location":"#license","title":"License","text":"<p>Micromegas is open source software. See the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to Micromegas","text":"<p>We welcome contributions to the Micromegas project! Here are some ways you can contribute:</p>"},{"location":"contributing/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you find a bug, please open an issue on our GitHub Issues page. Please include:</p> <ul> <li>A clear and concise description of the bug.</li> <li>Steps to reproduce the behavior.</li> <li>Expected behavior.</li> <li>Screenshots or error messages if applicable.</li> <li>Your operating system and Micromegas version.</li> </ul>"},{"location":"contributing/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>We're always looking for ways to improve Micromegas. If you have an idea for a new feature or an improvement to an existing one, please open an issue on our GitHub Issues page. Please include:</p> <ul> <li>A clear and concise description of the enhancement.</li> <li>Why you think it would be valuable to the project.</li> <li>Any potential use cases.</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>We welcome code contributions! If you'd like to contribute code, please follow these steps:</p> <ol> <li>Fork the repository and clone it to your local machine.</li> <li>Create a new branch for your feature or bug fix: <code>git checkout -b feature/your-feature-name</code> or <code>git checkout -b bugfix/your-bug-fix-name</code>.</li> <li>Make your changes and ensure your code adheres to the project's coding style and conventions.</li> <li>Write tests for your changes, if applicable.</li> <li>Run existing tests to ensure nothing is broken.</li> <li>Commit your changes with a clear and concise commit message.</li> <li>Push your branch to your forked repository.</li> <li>Open a Pull Request to the <code>main</code> branch of the Micromegas repository. Please provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>For information on setting up your local development environment, please refer to the Getting Started guide.</p>"},{"location":"contributing/#monorepo-structure","title":"Monorepo Structure","text":"<p>Micromegas uses a monorepo structure with npm workspaces for JavaScript/TypeScript components and Cargo workspaces for Rust components.</p>"},{"location":"contributing/#repository-layout","title":"Repository Layout","text":"<pre><code>micromegas/\n\u251c\u2500\u2500 rust/                    # Rust workspace (main application)\n\u2502   \u251c\u2500\u2500 Cargo.toml          # Root Cargo workspace\n\u2502   \u251c\u2500\u2500 analytics/          # Analytics engine\n\u2502   \u251c\u2500\u2500 tracing/            # Instrumentation library\n\u2502   \u251c\u2500\u2500 telemetry-ingestion-srv/\n\u2502   \u251c\u2500\u2500 flight-sql-srv/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 grafana/                # Grafana datasource plugin\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 pkg/                # Go backend\n\u251c\u2500\u2500 typescript/             # Shared TypeScript packages\n\u2502   \u2514\u2500\u2500 types/              # @micromegas/types package\n\u251c\u2500\u2500 python/                 # Python client\n\u2502   \u2514\u2500\u2500 micromegas/         # Poetry package\n\u251c\u2500\u2500 package.json            # Root npm workspace\n\u2514\u2500\u2500 CONTRIBUTING.md         # This file\n</code></pre>"},{"location":"contributing/#rust-workspace-primary","title":"Rust Workspace (Primary)","text":"<p>The Rust workspace is located in <code>rust/</code> and contains the core Micromegas platform. This is the main workspace of the project.</p> <p>Commands (run from <code>rust/</code> directory): <pre><code>cargo build              # Build all crates\ncargo test               # Run all tests\ncargo fmt                # Format code (REQUIRED before commit)\ncargo clippy --workspace -- -D warnings  # Lint\n</code></pre></p> <p>CI validation script: <pre><code>python3 build/rust_ci.py    # Runs format check, clippy, and tests (from repo root)\n</code></pre></p>"},{"location":"contributing/#python-package","title":"Python Package","text":"<p>The Python client uses Poetry for dependency management.</p> <p>Location: <code>python/micromegas/</code></p> <p>Commands (run from <code>python/micromegas/</code>): <pre><code>poetry install          # Install dependencies\npoetry run pytest       # Run tests\npoetry run black &lt;file&gt; # Format code (REQUIRED before commit)\n</code></pre></p>"},{"location":"contributing/#typescriptjavascript-workspaces","title":"TypeScript/JavaScript Workspaces","text":"<p>The repository uses npm workspaces to manage TypeScript/JavaScript packages, with <code>yarn</code> as the package manager.</p> <ul> <li>Root workspace (<code>package.json</code>): Defines workspaces and shared dev dependencies</li> <li><code>grafana/</code>: Grafana FlightSQL datasource plugin (React + Go backend)</li> <li><code>typescript/types/</code>: Shared TypeScript type definitions (<code>@micromegas/types</code>)</li> </ul> <p>Important: Always use <code>yarn</code>, not <code>npm</code>, to avoid lockfile conflicts.</p>"},{"location":"contributing/#working-with-all-components","title":"Working with All Components","text":""},{"location":"contributing/#installing-dependencies","title":"Installing Dependencies","text":"<p>Rust (from <code>rust/</code> directory): <pre><code>cargo build              # Fetches and compiles Rust dependencies\n</code></pre></p> <p>Python (from <code>python/micromegas/</code> directory): <pre><code>poetry install           # Installs Python dependencies\n</code></pre></p> <p>TypeScript/JavaScript (from repository root, use <code>yarn</code>): <pre><code>yarn install             # Install all workspace dependencies (Grafana plugin, shared types)\n</code></pre></p> <p>Go (for Grafana backend, from <code>grafana/</code> directory): <pre><code>go mod download          # Downloads Go dependencies\n</code></pre></p>"},{"location":"contributing/#building-components","title":"Building Components","text":"<p>Rust workspace: <pre><code>cd rust &amp;&amp; cargo build                   # Build all Rust crates\n</code></pre></p> <p>Python package: <pre><code>cd python/micromegas &amp;&amp; poetry install   # Python doesn't need a build step\n</code></pre></p> <p>TypeScript/JavaScript workspaces (use <code>yarn</code>): <pre><code>yarn workspaces run build                # Build all workspaces (from root)\ncd grafana &amp;&amp; yarn build                 # Grafana plugin only\ncd typescript/types &amp;&amp; yarn build        # Shared types only\n</code></pre></p> <p>For the Grafana plugin development: <pre><code>cd grafana\nyarn build              # Production build\nyarn dev                # Development mode with hot reload\n</code></pre></p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>Rust workspace: <pre><code>cd rust &amp;&amp; cargo test                    # All Rust tests\npython3 build/rust_ci.py                 # Rust CI validation (from root)\n</code></pre></p> <p>Python package: <pre><code>cd python/micromegas &amp;&amp; poetry run pytest  # Python tests\n</code></pre></p> <p>TypeScript/JavaScript workspaces (use <code>yarn</code>): <pre><code>yarn workspaces run test                 # Test all workspaces (from root)\ncd grafana &amp;&amp; yarn test:ci               # Grafana plugin tests only\n</code></pre></p>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Rust workspace: <pre><code>cd rust &amp;&amp; cargo clippy --workspace -- -D warnings\ncd rust &amp;&amp; cargo fmt                     # Format (REQUIRED before commit)\n</code></pre></p> <p>Python package: <pre><code>cd python/micromegas &amp;&amp; poetry run black .\n</code></pre></p> <p>TypeScript/JavaScript workspaces (use <code>yarn</code>): <pre><code>yarn workspaces run lint                 # Lint all workspaces (from root)\ncd grafana &amp;&amp; yarn lint:fix              # Grafana plugin only\n</code></pre></p>"},{"location":"contributing/#grafana-plugin-development","title":"Grafana Plugin Development","text":"<p>The Grafana plugin requires both Node.js and Go:</p> <p>Prerequisites: - Node.js 16+ (18.20.8 recommended) - Go 1.23+ (for backend plugin) - yarn (package manager for this repository) - mage (for Go builds): <code>go install github.com/magefile/mage@latest</code></p> <p>Development workflow: <pre><code>cd grafana\n\n# Install dependencies\nyarn install --ignore-engines\n\n# Build Go backend binaries\nmage -v build\n\n# Start development server with hot reload\nyarn dev\n\n# Run tests\nyarn test:ci\n\n# Run linting\nyarn lint\n\n# Build production bundle\nyarn build\n</code></pre></p> <p>Starting Grafana with the plugin: <pre><code>cd grafana\nyarn server             # Starts Grafana with docker compose (includes --build)\n# Access Grafana at http://localhost:3000\n</code></pre></p>"},{"location":"contributing/#code-style-and-conventions","title":"Code Style and Conventions","text":""},{"location":"contributing/#rust","title":"Rust","text":"<ul> <li>Dependencies in alphabetical order in Cargo.toml files</li> <li>Error handling: Use <code>expect()</code> with descriptive messages in tests, use <code>anyhow</code> in library code</li> <li>Run <code>cargo fmt</code> before any commit</li> <li>Use inline format arguments: <code>format!(\"value: {variable}\")</code></li> <li>Always use <code>prelude::*</code> when importing from prelude modules</li> </ul>"},{"location":"contributing/#typescriptjavascript","title":"TypeScript/JavaScript","text":"<ul> <li>Follow existing ESLint configuration in each workspace</li> <li>Use Prettier for formatting</li> <li>Run <code>npm run lint:fix</code> before committing</li> <li>Prefer functional components and hooks in React code</li> </ul>"},{"location":"contributing/#python","title":"Python","text":"<ul> <li>Use Black for formatting (required before commit)</li> <li>Follow PEP 8 guidelines</li> <li>Use type hints where appropriate</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<ul> <li>Keep messages clear and concise</li> <li>Follow existing commit message patterns in the repository</li> </ul> <p>Thank you for contributing to Micromegas!</p>"},{"location":"cost-effectiveness/","title":"Cost Effectiveness","text":"<p>Micromegas is designed to provide enterprise-grade observability at a fraction of the cost of commercial SaaS platforms by leveraging direct infrastructure costs rather than abstracted pricing models.</p>"},{"location":"cost-effectiveness/#cost-philosophy","title":"Cost Philosophy","text":"<p>Unlike traditional observability platforms that charge per GB ingested, per host, or per user, Micromegas runs on your own infrastructure. Your cost is simply the direct cost of the cloud services you consume.</p>"},{"location":"cost-effectiveness/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Full transparency - See every dollar spent on your cloud bill</li> <li>No vendor margins - Pay only for actual infrastructure usage</li> <li>Predictable scaling - Costs scale linearly with resource consumption</li> <li>Data ownership - Your telemetry data never leaves your cloud account</li> </ul>"},{"location":"cost-effectiveness/#primary-cost-drivers","title":"Primary Cost Drivers","text":"<p>The infrastructure cost for Micromegas comes from standard cloud services:</p>"},{"location":"cost-effectiveness/#compute-services","title":"Compute Services","text":"<ul> <li>Ingestion Service (<code>telemetry-ingestion-srv</code>) - Handles incoming telemetry data</li> <li>Analytics Service (<code>flight-sql-srv</code>) - Serves SQL queries and dashboards</li> <li>Maintenance Daemon (<code>telemetry-admin</code>) - Background data processing and rollups</li> </ul>"},{"location":"cost-effectiveness/#storage-services","title":"Storage Services","text":"<ul> <li>Database (PostgreSQL) - Stores metadata about processes, streams, and data blocks</li> <li>Object Storage (S3/GCS) - Stores raw telemetry payloads and materialized Parquet files</li> </ul>"},{"location":"cost-effectiveness/#supporting-infrastructure","title":"Supporting Infrastructure","text":"<ul> <li>Load Balancers - Route traffic to services</li> <li>Networking - Data transfer and connectivity</li> </ul>"},{"location":"cost-effectiveness/#example-deployment-cost","title":"Example Deployment Cost","text":"<p>Here's a real-world cost breakdown for a production Micromegas deployment:</p>"},{"location":"cost-effectiveness/#data-scale","title":"Data Scale","text":"<ul> <li>Retention Period: 90 days</li> <li>Total Storage: 8.5 TB in 118 million objects</li> <li>Log Entries: 9 billion</li> <li>Metric Events: 275 billion  </li> <li>Trace Events: 165 billion</li> </ul>"},{"location":"cost-effectiveness/#monthly-infrastructure-costs","title":"Monthly Infrastructure Costs","text":"Component Specification Monthly Cost Ingestion Services 2 instances \u00d7 (1 vCPU, 2GB RAM) ~$30 Analytics Service 1 instance \u00d7 (4 vCPU, 8GB RAM) ~$120 Maintenance Daemon 1 instance \u00d7 (4 vCPU, 8GB RAM) ~$120 PostgreSQL Database Aurora Serverless (44GB storage) ~$200 Object Storage 8.5TB S3 Standard + requests ~$500 Load Balancer Application Load Balancer ~$30 Total ~$1,000/month"},{"location":"cost-effectiveness/#scale-perspective","title":"Scale Perspective","text":"<p>This deployment handles:</p> <ul> <li>449 billion total events over 90 days</li> <li>~165 million events per day</li> <li>~1,900 events per second average throughput</li> </ul>"},{"location":"cost-effectiveness/#cost-management-features","title":"Cost Management Features","text":""},{"location":"cost-effectiveness/#on-demand-processing-tail-sampling","title":"On-Demand Processing (Tail Sampling)","text":"<p>Micromegas supports storing all raw telemetry data in low-cost object storage and materializing it for analysis only when needed:</p> <ul> <li>Raw data stored cheaply in S3/GCS</li> <li>Processing costs only when querying specific data</li> <li>Selective materialization based on actual analysis needs</li> </ul>"},{"location":"cost-effectiveness/#flexible-retention-policies","title":"Flexible Retention Policies","text":"<p>Configure retention periods independently for:</p> <ul> <li>Raw telemetry data - Keep longer in cheap storage</li> <li>Materialized views - Shorter retention for frequently accessed data</li> <li>Metadata - Configure based on compliance requirements</li> </ul>"},{"location":"cost-effectiveness/#commercial-platform-comparison","title":"Commercial Platform Comparison","text":""},{"location":"cost-effectiveness/#pricing-model-differences","title":"Pricing Model Differences","text":"Aspect Commercial SaaS Micromegas Cost Basis Per-GB, per-host, per-user Direct infrastructure costs Transparency Opaque vendor margins Full cost visibility Control Limited infrastructure control Complete infrastructure control Scalability Vendor-managed, unpredictable costs Self-managed, predictable scaling Data Ownership Third-party hosted Your cloud account only"},{"location":"cost-effectiveness/#when-micromegas-is-cost-effective","title":"When Micromegas is Cost Effective","text":"<p>The Micromegas model is particularly advantageous when:</p> <ul> <li>High data volumes - Direct infrastructure costs scale better than per-GB pricing</li> <li>Cost predictability is critical for budgeting</li> <li>Data governance requirements favor keeping data in your environment</li> <li>Operational maturity exists to manage distributed systems</li> <li>Long-term retention is needed (cheap object storage vs. expensive SaaS retention)</li> </ul>"},{"location":"cost-effectiveness/#detailed-cost-comparisons","title":"Detailed Cost Comparisons","text":"<p>For in-depth, dollar-for-dollar comparisons with specific platforms:</p> <ul> <li>Micromegas vs. Datadog</li> <li>Micromegas vs. Dynatrace</li> <li>Micromegas vs. Elastic Observability</li> <li>Micromegas vs. Grafana Cloud</li> <li>Micromegas vs. New Relic</li> <li>Micromegas vs. Splunk</li> </ul>"},{"location":"cost-effectiveness/#getting-started-with-cost-optimization","title":"Getting Started with Cost Optimization","text":"<ol> <li>Start small - Deploy minimal infrastructure and scale as needed</li> <li>Monitor usage - Use cloud billing dashboards to track costs</li> <li>Optimize retention - Balance storage costs with analysis needs  </li> <li>Leverage tail sampling - Store everything, process selectively</li> <li>Right-size compute - Match instance types to actual workload demands</li> </ol> <p>The goal is predictable, transparent costs that scale efficiently with your observability needs.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through setting up Micromegas on your local workstation for testing and development purposes.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Docker - For running PostgreSQL</li> <li>Python 3.8+ - For the client API and setup scripts</li> <li>Rust - For building Micromegas services</li> </ul> <p>Optional: - tmux - For managing multiple services in a single terminal (Linux/macOS)</p>"},{"location":"getting-started/#environment-setup","title":"Environment Setup","text":"<p>Set the following environment variables for local development:</p> <pre><code># Database credentials (used by setup scripts)\nexport MICROMEGAS_DB_USERNAME=your_username\nexport MICROMEGAS_DB_PASSWD=your_password\n\n# Service endpoints\nexport MICROMEGAS_TELEMETRY_URL=http://localhost:9000\nexport MICROMEGAS_SQL_CONNECTION_STRING=postgres://your_username:your_password@localhost:5432\n\n# Object storage (replace with your local path)\nexport MICROMEGAS_OBJECT_STORE_URI=file:///path/to/local/storage\n</code></pre> <p>Object Storage Path</p> <p>Choose a local directory for object storage, e.g., <code>/tmp/micromegas-storage</code> or <code>C:\\temp\\micromegas-storage</code> on Windows.</p>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre>"},{"location":"getting-started/#2-start-all-services","title":"2. Start All Services","text":""},{"location":"getting-started/#option-a-using-tmux-linuxmacos","title":"Option A: Using tmux (Linux/macOS)","text":"<p>The easiest way to start all required services is using the development script with tmux:</p> <pre><code># Start all services in a tmux session (debug mode)\npython3 local_test_env/dev.py\n\n# Or in release mode for better performance\npython3 local_test_env/dev.py release\n</code></pre> <p>This will automatically:</p> <ul> <li>Build all Rust services</li> <li>Start PostgreSQL database</li> <li>Start telemetry-ingestion-srv on port 9000</li> <li>Start flight-sql-srv on port 50051</li> <li>Start telemetry-admin service</li> <li>Open a tmux session with all services running in separate panes</li> </ul> <p>Managing Services with tmux</p> <ul> <li>To switch between service panes: <code>Ctrl+B</code> then arrow keys</li> <li>To detach from tmux (leave services running): <code>Ctrl+B</code> then <code>D</code></li> <li>To reattach to tmux: <code>tmux attach -t micromegas</code></li> <li>To stop all services: <code>python3 local_test_env/stop-dev.py</code></li> </ul>"},{"location":"getting-started/#option-b-manual-startup-windows-or-without-tmux","title":"Option B: Manual Startup (Windows or without tmux)","text":"<p>If you're on Windows or prefer not to use tmux, start each service in a separate terminal:</p> <p>Terminal 1: PostgreSQL Database <pre><code>cd local_test_env/db\npython run.py\n</code></pre></p> <p>Terminal 2: Ingestion Server <pre><code>cd rust\ncargo run -p telemetry-ingestion-srv -- --listen-endpoint-http 127.0.0.1:9000\n</code></pre></p> <p>Terminal 3: FlightSQL Server <pre><code>cd rust\ncargo run -p flight-sql-srv -- --disable-auth\n</code></pre></p> <p>Terminal 4: Admin Service <pre><code>cd rust\ncargo run -p telemetry-admin -- crond\n</code></pre></p> <p>Service Roles</p> <ul> <li>PostgreSQL: Stores metadata and service configuration</li> <li>Ingestion Server: Receives telemetry data from applications (port 9000)</li> <li>FlightSQL Server: Provides SQL query interface for analytics (port 50051)</li> <li>Admin Service: Handles background processing and global view materialization</li> </ul>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":""},{"location":"getting-started/#install-python-client","title":"Install Python Client","text":"<pre><code>pip install micromegas\n</code></pre>"},{"location":"getting-started/#test-with-sample-query","title":"Test with Sample Query","text":"<p>Create a test script to verify everything is working:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to local Micromegas instance\nclient = micromegas.connect()\n\n# Set up time range for query\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(days=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute query and display results\ndf = client.query(sql, begin, end)\nprint(f\"Found {len(df)} log entries\")\nprint(df.head())\n</code></pre> <p>If you see a DataFrame with log entries (or an empty DataFrame if no data has been ingested yet), your installation is working correctly!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have Micromegas running locally, you can:</p> <ol> <li>Unreal Engine Integration - Add observability to your Unreal Engine games</li> <li>Learn to Query Data - Explore the SQL interface and available data</li> <li>Understand the Architecture - Learn how Micromegas components work together</li> <li>Instrument Your Application - Start collecting telemetry from your own applications</li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>Connection refused when querying : Make sure all three services are running and the FlightSQL server is listening on the default port.</p> <p>Database connection errors : Verify your PostgreSQL container is running and the connection string environment variable is correct.</p> <p>Empty query results : This is normal for a fresh installation. You'll need to instrument an application to start collecting telemetry data.</p> <p>Build errors : Ensure you have the latest Rust toolchain installed.</p>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the service logs in each terminal for error messages</li> <li>Verify all environment variables are set correctly</li> <li>Create an issue on GitHub with details about your setup</li> </ol>"},{"location":"admin/authentication/","title":"Authentication","text":"<p>Micromegas supports unified authentication across all services using both API keys and OpenID Connect (OIDC).</p>"},{"location":"admin/authentication/#overview","title":"Overview","text":"<p>Both the analytics server (flight-sql-srv) and ingestion server (telemetry-ingestion-srv) support two authentication methods:</p> <ul> <li>OIDC (OpenID Connect) - For human users and service accounts via federated identity providers (Google, Azure AD, Okta, Auth0, etc.)</li> <li>API Keys - Legacy support for simple bearer token authentication</li> </ul> <p>Both methods can be enabled simultaneously, with API key validation tried first (fast path) before falling back to OIDC validation.</p>"},{"location":"admin/authentication/#authentication-methods","title":"Authentication Methods","text":""},{"location":"admin/authentication/#oidc-authentication-recommended","title":"OIDC Authentication (Recommended)","text":"<p>OIDC provides secure federated authentication with automatic token refresh and support for multiple identity providers.</p> <p>Benefits:</p> <ul> <li>Standards-based authentication (OAuth 2.0 / OpenID Connect)</li> <li>Centralized user management via identity provider</li> <li>Automatic token refresh with no manual intervention</li> <li>Support for multiple identity providers simultaneously</li> <li>Full audit trail with user identity (email, subject)</li> <li>Token revocation support via identity provider</li> </ul> <p>Supported Identity Providers:</p> <ul> <li>Google OAuth</li> <li>Microsoft Azure AD</li> <li>Okta</li> <li>Auth0</li> <li>Any standards-compliant OIDC provider</li> </ul>"},{"location":"admin/authentication/#api-keys-legacy","title":"API Keys (Legacy)","text":"<p>Simple bearer token authentication using pre-shared keys.</p> <p>Benefits:</p> <ul> <li>Simple to configure</li> <li>Fast validation (HashMap lookup)</li> <li>No external dependencies</li> </ul> <p>Limitations:</p> <ul> <li>Manual key distribution and rotation</li> <li>No automatic expiration</li> <li>No user identity for audit logging</li> </ul>"},{"location":"admin/authentication/#server-configuration","title":"Server Configuration","text":""},{"location":"admin/authentication/#oidc-configuration","title":"OIDC Configuration","text":"<p>Configure OIDC authentication using environment variables:</p> <pre><code># OIDC Configuration\nexport MICROMEGAS_OIDC_CONFIG='{\n  \"issuers\": [\n    {\n      \"issuer\": \"https://accounts.google.com\",\n      \"audience\": \"your-app-id.apps.googleusercontent.com\"\n    },\n    {\n      \"issuer\": \"https://login.microsoftonline.com/{tenant}/v2.0\",\n      \"audience\": \"api://your-api-id\"\n    }\n  ],\n  \"jwks_refresh_interval_secs\": 3600,\n  \"token_cache_size\": 1000,\n  \"token_cache_ttl_secs\": 300\n}'\n\n# Optional: Configure admin users\nexport MICROMEGAS_ADMINS='[\"alice@example.com\", \"bob@example.com\"]'\n</code></pre> <p>Configuration Fields:</p> Field Description Default <code>issuers</code> Array of OIDC issuer configurations Required <code>issuers[].issuer</code> OIDC issuer URL Required <code>issuers[].audience</code> Expected audience claim (client_id) Required <code>jwks_refresh_interval_secs</code> JWKS cache TTL in seconds 3600 <code>token_cache_size</code> Maximum validated tokens to cache 1000 <code>token_cache_ttl_secs</code> Token cache TTL in seconds 300 <p>Admin Configuration:</p> <p>The <code>MICROMEGAS_ADMINS</code> environment variable is a JSON array of user identifiers (email or subject) that have administrative privileges. Admin users can perform operations like partition management.</p>"},{"location":"admin/authentication/#api-key-configuration","title":"API Key Configuration","text":"<p>Configure API keys using the <code>MICROMEGAS_API_KEYS</code> environment variable with a JSON array:</p> <pre><code>export MICROMEGAS_API_KEYS='[\n  {\"name\": \"service1\", \"key\": \"secret-key-123\"},\n  {\"name\": \"service2\", \"key\": \"secret-key-456\"}\n]'\n</code></pre> <p>Format: - JSON array of objects - Each object has <code>name</code> (identifier for logging) and <code>key</code> (the actual API key) - The <code>key</code> value is sent as the Bearer token by clients - Generate keys with: <code>openssl rand -base64 512</code></p>"},{"location":"admin/authentication/#disable-authentication-development-only","title":"Disable Authentication (Development Only)","text":"<p>For local development and testing, authentication can be disabled:</p> <pre><code># Analytics server\nflight-sql-srv --disable-auth\n\n# Ingestion server\ntelemetry-ingestion-srv --disable-auth\n</code></pre> <p>Security Warning</p> <p>Never disable authentication in production environments. This flag is intended only for local development and testing.</p>"},{"location":"admin/authentication/#client-configuration","title":"Client Configuration","text":""},{"location":"admin/authentication/#python-client-with-oidc","title":"Python Client with OIDC","text":"<p>The Python client supports automatic browser-based login with token persistence and refresh.</p>"},{"location":"admin/authentication/#interactive-use-jupyter-scripts","title":"Interactive Use (Jupyter, Scripts)","text":"<pre><code>from micromegas.auth import OidcAuthProvider\nfrom micromegas.flightsql.client import FlightSQLClient\n\n# First use: Opens browser for authentication\nauth = OidcAuthProvider.login(\n    issuer=\"https://accounts.google.com\",\n    client_id=\"your-app-id.apps.googleusercontent.com\",\n    client_secret=\"your-client-secret\",  # Optional for some providers\n    token_file=\"~/.micromegas/tokens.json\"  # Persists tokens\n)\n\n# Create authenticated client\nclient = FlightSQLClient(\n    \"grpc+tls://analytics.example.com:50051\",\n    auth_provider=auth\n)\n\n# Run queries - tokens auto-refresh before expiration\ndf = client.query(\"SELECT * FROM processes LIMIT 10\")\n</code></pre>"},{"location":"admin/authentication/#subsequent-use-token-reuse","title":"Subsequent Use (Token Reuse)","text":"<pre><code>from micromegas.auth import OidcAuthProvider\nfrom micromegas.flightsql.client import FlightSQLClient\n\n# Load existing tokens - no browser interaction needed\nauth = OidcAuthProvider.from_file(\n    \"~/.micromegas/tokens.json\",\n    client_secret=\"your-client-secret\"  # Optional\n)\n\nclient = FlightSQLClient(\n    \"grpc+tls://analytics.example.com:50051\",\n    auth_provider=auth\n)\n\n# Tokens automatically refresh when needed\nimport datetime\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\ndf = client.query(\"SELECT * FROM log_entries LIMIT 1000\", begin, now)\n</code></pre>"},{"location":"admin/authentication/#token-management","title":"Token Management","text":"<pre><code># Clear saved tokens (logout)\nimport os\nfrom pathlib import Path\n\ntoken_file = Path.home() / \".micromegas\" / \"tokens.json\"\nif token_file.exists():\n    token_file.unlink()\n    print(\"Logged out - tokens cleared\")\n</code></pre>"},{"location":"admin/authentication/#cli-tools-with-oidc","title":"CLI Tools with OIDC","text":"<p>CLI tools automatically support OIDC when environment variables are set:</p> <pre><code># Configure OIDC\nexport MICROMEGAS_OIDC_ISSUER=\"https://accounts.google.com\"\nexport MICROMEGAS_OIDC_CLIENT_ID=\"your-app-id.apps.googleusercontent.com\"\nexport MICROMEGAS_OIDC_CLIENT_SECRET=\"your-client-secret\"  # Optional\nexport MICROMEGAS_ANALYTICS_URI=\"grpc+tls://analytics.example.com:50051\"\n\n# First use: Opens browser for authentication\npython3 -m micromegas.cli.query_processes --since 1h\n\n# Subsequent uses: No browser interaction, uses cached tokens\npython3 -m micromegas.cli.query_process_log &lt;process_id&gt;\n\n# Logout (clear saved tokens)\nmicromegas_logout\n</code></pre> <p>Environment Variables:</p> Variable Description Required <code>MICROMEGAS_OIDC_ISSUER</code> OIDC issuer URL Yes <code>MICROMEGAS_OIDC_CLIENT_ID</code> OAuth client ID Yes <code>MICROMEGAS_OIDC_CLIENT_SECRET</code> OAuth client secret No* <code>MICROMEGAS_TOKEN_FILE</code> Token storage path No (default: ~/.micromegas/tokens.json) <code>MICROMEGAS_ANALYTICS_URI</code> Analytics server URI No (default: grpc://localhost:50051) <p>*Required for some providers (e.g., Google) even with PKCE</p>"},{"location":"admin/authentication/#python-client-with-api-keys-legacy","title":"Python Client with API Keys (Legacy)","text":"<pre><code>from micromegas.flightsql.client import FlightSQLClient\n\nclient = FlightSQLClient(\n    \"grpc://localhost:50051\",\n    headers={\"authorization\": \"Bearer your-api-key\"}\n)\n\ndf = client.query(\"SELECT * FROM processes LIMIT 10\")\n</code></pre> <p>Deprecated API</p> <p>The <code>headers</code> parameter is deprecated. Use <code>auth_provider</code> with <code>OidcAuthProvider</code> instead.</p>"},{"location":"admin/authentication/#ingestion-service-authentication","title":"Ingestion Service Authentication","text":"<p>The telemetry ingestion service (telemetry-ingestion-srv) uses the same authentication infrastructure as the analytics service.</p>"},{"location":"admin/authentication/#server-configuration_1","title":"Server Configuration","text":"<p>The ingestion server uses the same environment variables as the analytics server:</p> <pre><code># Start ingestion server with authentication\nexport MICROMEGAS_API_KEYS='[{\"name\": \"service1\", \"key\": \"secret-key-123\"}]'\nexport MICROMEGAS_OIDC_CONFIG='{\"issuers\": [...]}'\ntelemetry-ingestion-srv\n\n# Or disable auth for development\ntelemetry-ingestion-srv --disable-auth\n</code></pre>"},{"location":"admin/authentication/#rust-client-authentication","title":"Rust Client Authentication","text":"<p>Rust applications sending telemetry can use either API keys or OIDC client credentials.</p>"},{"location":"admin/authentication/#automatic-configuration-recommended","title":"Automatic Configuration (Recommended)","text":"<p>Applications using <code>#[micromegas_main]</code> automatically configure authentication from environment variables:</p> <pre><code>use micromegas::micromegas_main;\nuse micromegas::tracing::prelude::*;\n\n#[micromegas_main(interop_max_level = \"info\")]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    info!(\"Application starting\");\n    // Telemetry automatically authenticated based on environment variables\n    Ok(())\n}\n</code></pre> <p>Environment Variables:</p> Variable Authentication Method Required <code>MICROMEGAS_INGESTION_API_KEY</code> API key (simple) For API key auth <code>MICROMEGAS_OIDC_TOKEN_ENDPOINT</code> OIDC client credentials For OIDC auth <code>MICROMEGAS_OIDC_CLIENT_ID</code> OIDC client credentials For OIDC auth <code>MICROMEGAS_OIDC_CLIENT_SECRET</code> OIDC client credentials For OIDC auth <code>MICROMEGAS_TELEMETRY_URL</code> Ingestion server URL Yes (e.g., http://localhost:9000) <p>Example (API Key): <pre><code>export MICROMEGAS_INGESTION_API_KEY=secret-key-123\nexport MICROMEGAS_TELEMETRY_URL=http://localhost:9000\ncargo run\n</code></pre></p> <p>Example (OIDC Client Credentials): <pre><code>export MICROMEGAS_OIDC_TOKEN_ENDPOINT=https://accounts.google.com/o/oauth2/token\nexport MICROMEGAS_OIDC_CLIENT_ID=my-service@project.iam.gserviceaccount.com\nexport MICROMEGAS_OIDC_CLIENT_SECRET=secret-from-secret-manager\nexport MICROMEGAS_TELEMETRY_URL=http://localhost:9000\ncargo run\n</code></pre></p>"},{"location":"admin/authentication/#manual-configuration","title":"Manual Configuration","text":"<p>For applications not using <code>#[micromegas_main]</code>, configure authentication manually:</p>"},{"location":"admin/authentication/#api-key-authentication-simple","title":"API Key Authentication (Simple)","text":"<pre><code>use micromegas_telemetry_sink::http_event_sink::HttpEventSink;\nuse micromegas_telemetry_sink::api_key_decorator::ApiKeyRequestDecorator;\nuse std::sync::Arc;\n\n// From environment variable\nstd::env::set_var(\"MICROMEGAS_INGESTION_API_KEY\", \"secret-key-123\");\nlet decorator = ApiKeyRequestDecorator::from_env().unwrap();\n\n// Configure HttpEventSink with authentication\nlet sink = HttpEventSink::new(\n    \"http://localhost:9000\",\n    max_queue_size,\n    metadata_retry,\n    blocks_retry,\n    Box::new(move || Arc::new(decorator.clone())),\n);\n</code></pre>"},{"location":"admin/authentication/#oidc-client-credentials-production","title":"OIDC Client Credentials (Production)","text":"<pre><code>use micromegas_telemetry_sink::http_event_sink::HttpEventSink;\nuse micromegas_telemetry_sink::oidc_client_credentials_decorator::OidcClientCredentialsDecorator;\nuse std::sync::Arc;\n\n// Configure OIDC client credentials\nstd::env::set_var(\"MICROMEGAS_OIDC_TOKEN_ENDPOINT\",\n    \"https://accounts.google.com/o/oauth2/token\");\nstd::env::set_var(\"MICROMEGAS_OIDC_CLIENT_ID\",\n    \"my-service@project.iam.gserviceaccount.com\");\nstd::env::set_var(\"MICROMEGAS_OIDC_CLIENT_SECRET\",\n    \"secret-from-secret-manager\");\n\nlet decorator = OidcClientCredentialsDecorator::from_env().unwrap();\n\nlet sink = HttpEventSink::new(\n    \"http://localhost:9000\",\n    max_queue_size,\n    metadata_retry,\n    blocks_retry,\n    Box::new(move || Arc::new(decorator.clone())),\n);\n</code></pre> <p>Authentication Methods Comparison:</p> Method Use Case Token Lifetime Complexity API Key Development, testing No expiration Low Client Credentials Production services ~1 hour (auto-refresh) Medium"},{"location":"admin/authentication/#health-endpoint","title":"Health Endpoint","text":"<p>The <code>/health</code> endpoint remains public for monitoring and liveness checks, even when authentication is enabled.</p> <pre><code># Health check always works without authentication\ncurl http://localhost:9000/health\n</code></pre>"},{"location":"admin/authentication/#setting-up-oidc-providers","title":"Setting Up OIDC Providers","text":""},{"location":"admin/authentication/#google-oauth-setup","title":"Google OAuth Setup","text":"<ol> <li>Go to Google Cloud Console</li> <li>Create a new project or select existing</li> <li>Navigate to APIs &amp; Services \u2192 OAuth consent screen</li> <li>Select \"External\" user type</li> <li>Fill in app name and contact emails</li> <li>Add test users (yourself and team members)</li> <li>Navigate to APIs &amp; Services \u2192 Credentials</li> <li>Click \"+ CREATE CREDENTIALS\" \u2192 \"OAuth client ID\"</li> <li>Application type: \"Desktop app\" (for CLI/local use)</li> <li>Click \"Create\"</li> <li>Copy both credentials:</li> <li>Client ID (ends with <code>.apps.googleusercontent.com</code>)</li> <li>Client Secret</li> <li>Add authorized redirect URIs:</li> <li><code>http://localhost:48080/callback</code></li> </ol> <p>Server Configuration:</p> <pre><code>export MICROMEGAS_OIDC_CONFIG='{\n  \"issuers\": [\n    {\n      \"issuer\": \"https://accounts.google.com\",\n      \"audience\": \"123-abc.apps.googleusercontent.com\"\n    }\n  ]\n}'\n</code></pre> <p>Client Configuration:</p> <pre><code>export MICROMEGAS_OIDC_ISSUER=\"https://accounts.google.com\"\nexport MICROMEGAS_OIDC_CLIENT_ID=\"123-abc.apps.googleusercontent.com\"\nexport MICROMEGAS_OIDC_CLIENT_SECRET=\"GOCSPX-...\"\n</code></pre>"},{"location":"admin/authentication/#azure-ad-setup","title":"Azure AD Setup","text":"<ol> <li>Go to Azure Portal</li> <li>Navigate to Azure Active Directory \u2192 App registrations</li> <li>Click \"New registration\"</li> <li>Name: \"Micromegas Analytics\"</li> <li>Supported account types: Choose based on your needs</li> <li>Redirect URI: \"Public client/native\" - <code>http://localhost:48080/callback</code></li> <li>Note the Application (client) ID</li> <li>Navigate to Authentication</li> <li>Under \"Advanced settings\", set \"Allow public client flows\" to Yes</li> <li>This enables PKCE without requiring a client secret</li> <li>Navigate to API permissions (optional)</li> <li>Add permissions if needed for your organization</li> </ol> <p>Server Configuration:</p> <pre><code>export MICROMEGAS_OIDC_CONFIG='{\n  \"issuers\": [\n    {\n      \"issuer\": \"https://login.microsoftonline.com/{tenant-id}/v2.0\",\n      \"audience\": \"{application-id}\"\n    }\n  ]\n}'\n</code></pre> <p>Client Configuration:</p> <pre><code>export MICROMEGAS_OIDC_ISSUER=\"https://login.microsoftonline.com/{tenant-id}/v2.0\"\nexport MICROMEGAS_OIDC_CLIENT_ID=\"{application-id}\"\n# No MICROMEGAS_OIDC_CLIENT_SECRET needed - Azure AD supports public clients with PKCE\n</code></pre>"},{"location":"admin/authentication/#auth0-setup","title":"Auth0 Setup","text":"<ol> <li>Go to Auth0 Dashboard</li> <li>Create application:</li> <li>Applications \u2192 Create Application</li> <li>Name: \"Micromegas Analytics\"</li> <li>Application type: \"Native\" (for CLI/desktop)</li> <li>Configure application:</li> <li>Allowed Callback URLs: <code>http://localhost:48080/callback</code></li> <li>Allowed Web Origins: <code>http://localhost:48080</code></li> <li>Note the Domain and Client ID</li> <li>For Native apps, client secret is optional (true public client)</li> </ol> <p>Server Configuration:</p> <pre><code>export MICROMEGAS_OIDC_CONFIG='{\n  \"issuers\": [\n    {\n      \"issuer\": \"https://your-tenant.auth0.com/\",\n      \"audience\": \"your-client-id\"\n    }\n  ]\n}'\n</code></pre> <p>Client Configuration:</p> <pre><code>export MICROMEGAS_OIDC_ISSUER=\"https://your-tenant.auth0.com/\"\nexport MICROMEGAS_OIDC_CLIENT_ID=\"your-client-id\"\n# No client_secret needed for Native apps\n</code></pre>"},{"location":"admin/authentication/#security-considerations","title":"Security Considerations","text":""},{"location":"admin/authentication/#token-storage","title":"Token Storage","text":"<p>Tokens are stored at <code>~/.micromegas/tokens.json</code> with secure file permissions (0600 - owner read/write only).</p> <p>Token File Contents:</p> <ul> <li>Access token (JWT)</li> <li>Refresh token</li> <li>ID token</li> <li>Expiration time</li> <li>Issuer and client ID</li> </ul> <p>Token File Security</p> <p>Never commit token files to version control or share them. Tokens provide full access to your analytics data.</p>"},{"location":"admin/authentication/#token-refresh","title":"Token Refresh","text":"<p>The Python client automatically refreshes tokens when they approach expiration (5-minute buffer). This ensures: - No mid-query authentication failures - Transparent token management - Thread-safe concurrent query support</p>"},{"location":"admin/authentication/#token-revocation","title":"Token Revocation","text":"<p>To revoke access:</p> <ol> <li>User accounts: Disable the user in your identity provider (Google, Azure AD, etc.)</li> <li>Service accounts: Disable or delete the service account in your identity provider</li> <li>Immediate revocation: Restart the analytics server to clear the token validation cache</li> </ol> <p>Revocation Timing:</p> <ul> <li>New tokens will be rejected immediately after disabling the account</li> <li>Existing cached tokens remain valid for up to 5 minutes (configurable via <code>token_cache_ttl_secs</code>)</li> <li>Total revocation time: Cache TTL (5 min) + Token lifetime (typically 60 min) = ~65 minutes worst case</li> </ul> <p>For faster revocation, use shorter token cache TTL or restart the analytics server.</p>"},{"location":"admin/authentication/#admin-privileges","title":"Admin Privileges","text":"<p>Admin users (configured via <code>MICROMEGAS_ADMINS</code>) have elevated privileges for administrative operations. Only grant admin access to trusted users.</p> <p>Admin Capabilities:</p> <ul> <li>Partition management functions</li> <li>Schema migration operations</li> <li>Administrative SQL functions</li> </ul>"},{"location":"admin/authentication/#httpstls","title":"HTTPS/TLS","text":"<p>Always use TLS for production deployments:</p> <pre><code># Production: Use grpc+tls\nclient = FlightSQLClient(\n    \"grpc+tls://analytics.example.com:50051\",\n    auth_provider=auth\n)\n\n# Development only: Plain grpc\nclient = FlightSQLClient(\n    \"grpc://localhost:50051\",\n    auth_provider=auth\n)\n</code></pre> <p>Configure your load balancer or reverse proxy to handle TLS termination.</p>"},{"location":"admin/authentication/#pkce-proof-key-for-code-exchange","title":"PKCE (Proof Key for Code Exchange)","text":"<p>The Python client uses PKCE for all OIDC flows, providing security for public clients (desktop apps, CLIs) that cannot securely store client secrets.</p> <p>How PKCE Works:</p> <ol> <li>Client generates random <code>code_verifier</code></li> <li>Client creates <code>code_challenge</code> (SHA256 hash of verifier)</li> <li>Authorization request includes <code>code_challenge</code></li> <li>Token exchange includes original <code>code_verifier</code></li> <li>Identity provider validates the verifier matches the challenge</li> </ol> <p>This prevents authorization code interception attacks even if the client secret is compromised or unavailable.</p>"},{"location":"admin/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin/authentication/#authentication-failures","title":"Authentication Failures","text":"<p>Symptom: \"Invalid token\" or \"Authentication failed\" errors</p> <p>Solutions:</p> <ol> <li>Check server logs: <code>tail -f /tmp/analytics.log | grep -i auth</code></li> <li>Verify OIDC configuration matches between server and client</li> <li>Ensure Client ID and Issuer URL are correct</li> <li>Check token expiration: <code>cat ~/.micromegas/tokens.json | jq .expires_at</code></li> <li>Clear tokens and re-authenticate: <code>micromegas_logout</code></li> </ol>"},{"location":"admin/authentication/#token-refresh-failures","title":"Token Refresh Failures","text":"<p>Symptom: Browser opens on every CLI invocation</p> <p>Solutions:</p> <ol> <li>Check if refresh token is present: <code>cat ~/.micromegas/tokens.json | jq .refresh_token</code></li> <li>Verify client secret matches (if required by provider)</li> <li>Check token file permissions: <code>ls -la ~/.micromegas/tokens.json</code> (should be 600)</li> <li>Re-authenticate: <code>micromegas_logout</code> then retry</li> </ol>"},{"location":"admin/authentication/#server-configuration-issues","title":"Server Configuration Issues","text":"<p>Symptom: Server fails to start or rejects all authentication</p> <p>Solutions:</p> <ol> <li>Validate OIDC config JSON syntax: <code>echo $MICROMEGAS_OIDC_CONFIG | jq .</code></li> <li>Check server can reach identity provider: <code>curl https://accounts.google.com/.well-known/openid-configuration</code></li> <li>Verify audience matches client ID exactly</li> <li>Check server logs for OIDC discovery errors</li> </ol>"},{"location":"admin/authentication/#multi-provider-issues","title":"Multi-Provider Issues","text":"<p>Symptom: Only one identity provider works</p> <p>Solutions:</p> <ol> <li>Verify all issuers are in the configuration array</li> <li>Check each issuer URL is correct and accessible</li> <li>Ensure audience (client_id) matches for each provider</li> <li>Review server logs for OIDC discovery failures per issuer</li> </ol>"},{"location":"admin/authentication/#migration-from-api-keys-to-oidc","title":"Migration from API Keys to OIDC","text":"<p>To migrate from API keys to OIDC authentication:</p> <ol> <li>Set up OIDC provider (Google, Azure AD, etc.)</li> <li>Configure server with both API keys and OIDC</li> <li>Update clients to use OIDC authentication</li> <li>Test with OIDC while API keys still work (parallel operation)</li> <li>Remove API keys from configuration when migration complete</li> </ol> <p>Example Migration:</p> <pre><code># Step 1: Add OIDC configuration (API keys still work)\nexport MICROMEGAS_API_KEYS='[{\"name\": \"service1\", \"key\": \"old-key\"}]'\nexport MICROMEGAS_OIDC_CONFIG='{\n  \"issuers\": [{\n    \"issuer\": \"https://accounts.google.com\",\n    \"audience\": \"new-client-id.apps.googleusercontent.com\"\n  }]\n}'\n\n# Step 2: Update clients to use OIDC\n# Test both authentication methods work\n\n# Step 3: Remove API keys when all clients migrated\nunset MICROMEGAS_API_KEYS\n# Only OIDC remains\n</code></pre>"},{"location":"admin/authentication/#best-practices","title":"Best Practices","text":"<ol> <li>Use OIDC for all new deployments - Better security and user management</li> <li>Enable admin privileges sparingly - Only for users who need administrative access</li> <li>Use short token cache TTL in high-security environments (60-300 seconds)</li> <li>Monitor authentication logs - Track failed auth attempts and unusual patterns</li> <li>Rotate client secrets regularly - Update in identity provider and redistribute</li> <li>Use separate OAuth clients for different environments (dev, staging, prod)</li> <li>Document your identity provider setup - Makes onboarding new team members easier</li> </ol>"},{"location":"admin/authentication/#reference","title":"Reference","text":"<ul> <li>OpenID Connect Core Specification</li> <li>OAuth 2.0 RFC 6749</li> <li>PKCE RFC 7636</li> <li>OAuth 2.0 Security Best Practices</li> </ul>"},{"location":"admin/functions-reference/","title":"Administrative Functions Reference","text":"<p>This page provides detailed reference documentation for all administrative functions available in the Micromegas admin module.</p>"},{"location":"admin/functions-reference/#table-functions-udtfs","title":"Table Functions (UDTFs)","text":""},{"location":"admin/functions-reference/#list_view_sets","title":"<code>list_view_sets()</code>","text":"<p>Description: Lists all available view sets with their current schema information.</p> <p>Usage: <pre><code>SELECT * FROM list_view_sets();\n</code></pre></p> <p>Returns: Table with columns:</p> Column Type Description <code>view_set_name</code> String Name of the view set (e.g., \"log_entries\", \"measures\") <code>current_schema_hash</code> Binary Version identifier for current schema (e.g., <code>[4]</code>) <code>schema</code> String Full schema as formatted string <code>has_view_maker</code> Boolean Whether view set supports non-global instances <code>global_instance_available</code> Boolean Whether a global instance exists <p>Example: <pre><code>-- List all view sets with schema versions\nSELECT view_set_name, current_schema_hash, has_view_maker \nFROM list_view_sets()\nORDER BY view_set_name;\n\n-- Find view sets with specific schema version\nSELECT * FROM list_view_sets() \nWHERE current_schema_hash = '[4]';\n</code></pre></p> <p>Performance: Fast operation, queries in-memory view registry.</p>"},{"location":"admin/functions-reference/#list_partitions","title":"<code>list_partitions()</code>","text":"<p>Description: Lists all partitions in the lakehouse with metadata.</p> <p>Usage: <pre><code>SELECT * FROM list_partitions();\n</code></pre></p> <p>Returns: Table with columns including:</p> Column Type Description <code>view_set_name</code> String View set name <code>view_instance_id</code> String Instance ID or 'global' <code>file_schema_hash</code> Binary Schema version when partition was created <code>file_path</code> String Object storage file path <code>file_size</code> Integer File size in bytes <code>begin_insert_time</code> Timestamp Earliest event time in partition <code>end_insert_time</code> Timestamp Latest event time in partition <p>Example: <pre><code>-- List partitions for specific view set\nSELECT file_path, file_size, file_schema_hash \nFROM list_partitions() \nWHERE view_set_name = 'log_entries';\n\n-- Find partitions by schema version\nSELECT view_set_name, COUNT(*) as partition_count\nFROM list_partitions()\nWHERE file_schema_hash = '[3]'\nGROUP BY view_set_name;\n</code></pre></p> <p>Performance: Queries database metadata table, indexed by view_set_name.</p>"},{"location":"admin/functions-reference/#retire_partitionsview_set-view_instance-start_time-end_time","title":"<code>retire_partitions(view_set, view_instance, start_time, end_time)</code>","text":"<p>Description: Retires partitions within a specified time range.</p> <p>Parameters: - <code>view_set</code> (String): Target view set name - <code>view_instance</code> (String): Target view instance ID - <code>start_time</code> (Timestamp): Start of time range (inclusive) - <code>end_time</code> (Timestamp): End of time range (inclusive)</p> <p>Usage: <pre><code>SELECT * FROM retire_partitions('log_entries', 'process-123', '2024-01-01T00:00:00Z', '2024-01-02T00:00:00Z');\n</code></pre></p> <p>Returns: Table with retirement operation results.</p> <p>Safety: Uses database transactions for atomicity. All partitions in time range are retired.</p> <p>Time-Based Retirement</p> <p>This function retires ALL partitions in the specified time range, regardless of schema compatibility. Use with caution.</p>"},{"location":"admin/functions-reference/#scalar-functions-udfs","title":"Scalar Functions (UDFs)","text":""},{"location":"admin/functions-reference/#retire_partition_by_filefile_path","title":"<code>retire_partition_by_file(file_path)</code>","text":"<p>Description: Surgically retires a single partition by exact file path match.</p> <p>Parameters: - <code>file_path</code> (String): Exact file path of partition to retire</p> <p>Usage: <pre><code>SELECT retire_partition_by_file('s3://bucket/data/log_entries/process-123/2024/01/01/file.parquet') as result;\n</code></pre></p> <p>Returns: String message indicating success or failure: - Success: <code>\"SUCCESS: Retired partition &lt;file_path&gt;\"</code> - Failure: <code>\"ERROR: Partition not found: &lt;file_path&gt;\"</code></p> <p>Safety: Surgical precision - only targets the exact specified partition. Cannot accidentally retire other partitions.</p> <p>Example: <pre><code>-- Retire specific partition\nSELECT retire_partition_by_file('s3://bucket/data/log_entries/proc1/partition.parquet');\n\n-- Batch retire multiple partitions\nSELECT file_path, retire_partition_by_file(file_path) as result\nFROM (\n    SELECT file_path FROM list_partitions() \n    WHERE view_set_name = 'log_entries' \n    AND file_schema_hash = '[3]'\n    LIMIT 10\n);\n</code></pre></p> <p>Performance: Single partition operation, very fast with appropriate database indexes.</p>"},{"location":"admin/functions-reference/#python-api-functions","title":"Python API Functions","text":""},{"location":"admin/functions-reference/#micromegasadminlist_incompatible_partitionsclient-view_set_namenone","title":"<code>micromegas.admin.list_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Description: Identifies partitions with schemas incompatible with current schema versions.</p> <p>Parameters: - <code>client</code> (FlightSQLClient): Connected Micromegas client - <code>view_set_name</code> (str, optional): Filter to specific view set</p> <p>Returns: pandas DataFrame with columns:</p> Column Type Description <code>view_set_name</code> str View set name <code>view_instance_id</code> str Instance ID <code>incompatible_schema_hash</code> str Old schema version in partition <code>current_schema_hash</code> str Current schema version <code>partition_count</code> int Number of incompatible partitions <code>total_size_bytes</code> int Total size of incompatible partitions <code>file_paths</code> list Array of file paths for each partition <p>Example: <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# List all incompatible partitions\nincompatible = micromegas.admin.list_incompatible_partitions(client)\nprint(f\"Found {incompatible['partition_count'].sum()} incompatible partitions\")\n\n# List for specific view set\nlog_incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nfor _, row in log_incompatible.iterrows():\n    print(f\"{row['view_set_name']}: {row['partition_count']} partitions, {row['total_size_bytes']} bytes\")\n</code></pre></p> <p>Implementation: Uses SQL JOIN between <code>list_partitions()</code> and <code>list_view_sets()</code> with server-side aggregation.</p> <p>Performance: Efficient server-side processing, minimal network overhead.</p>"},{"location":"admin/functions-reference/#micromegasadminretire_incompatible_partitionsclient-view_set_namenone","title":"<code>micromegas.admin.retire_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Description: Safely retires partitions with incompatible schemas using surgical file-path-based retirement.</p> <p>Parameters: - <code>client</code> (FlightSQLClient): Connected Micromegas client - <code>view_set_name</code> (str, optional): Filter to specific view set</p> <p>Returns: pandas DataFrame with columns:</p> Column Type Description <code>view_set_name</code> str View set processed <code>view_instance_id</code> str Instance ID processed <code>partitions_retired</code> int Count of successfully retired partitions <code>storage_freed_bytes</code> int Total bytes freed from storage <code>retirement_messages</code> list Detailed messages for each retirement attempt <p>Example: <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# Preview what would be retired\npreview = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Will retire {preview['partition_count'].sum()} partitions\")\n\n# Retire incompatible partitions\nresult = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')\nfor _, row in result.iterrows():\n    print(f\"Retired {row['partitions_retired']} partitions from {row['view_set_name']}\")\n    print(f\"Freed {row['storage_freed_bytes']} bytes\")\n</code></pre></p> <p>Safety Features: - Uses <code>retire_partition_by_file()</code> for surgical precision - Cannot accidentally retire compatible partitions - Comprehensive error handling with detailed messages - Continues processing even if individual partitions fail - Full transaction safety and rollback protection</p> <p>Implementation:  1. Calls <code>list_incompatible_partitions()</code> to identify targets 2. For each incompatible partition, calls <code>retire_partition_by_file()</code>  3. Aggregates results and provides summary statistics 4. Includes detailed operation logs for auditing</p> <p>Performance: Processes partitions individually for safety, efficient for typical partition counts.</p>"},{"location":"admin/functions-reference/#complex-query-examples","title":"Complex Query Examples","text":""},{"location":"admin/functions-reference/#find-schema-migration-candidates","title":"Find Schema Migration Candidates","text":"<pre><code>-- Identify view sets with the most incompatible partitions\nSELECT \n    vs.view_set_name,\n    vs.current_schema_hash,\n    COUNT(DISTINCT p.file_schema_hash) as schema_versions_count,\n    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN 1 ELSE 0 END) as incompatible_count,\n    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN p.file_size ELSE 0 END) as incompatible_size_bytes\nFROM list_view_sets() vs\nLEFT JOIN list_partitions() p ON vs.view_set_name = p.view_set_name\nGROUP BY vs.view_set_name, vs.current_schema_hash\nHAVING incompatible_count &gt; 0\nORDER BY incompatible_size_bytes DESC;\n</code></pre>"},{"location":"admin/functions-reference/#analyze-partition-age-distribution","title":"Analyze Partition Age Distribution","text":"<pre><code>-- Find old incompatible partitions that are candidates for retirement\nSELECT \n    p.view_set_name,\n    p.file_schema_hash as old_schema,\n    vs.current_schema_hash,\n    COUNT(*) as partition_count,\n    MIN(p.end_insert_time) as oldest_partition,\n    MAX(p.end_insert_time) as newest_partition,\n    SUM(p.file_size) as total_size_bytes\nFROM list_partitions() p\nJOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name\nWHERE p.file_schema_hash != vs.current_schema_hash\n    AND p.end_insert_time &lt; NOW() - INTERVAL '30 days'\nGROUP BY p.view_set_name, p.file_schema_hash, vs.current_schema_hash\nORDER BY oldest_partition ASC;\n</code></pre>"},{"location":"admin/functions-reference/#storage-impact-analysis","title":"Storage Impact Analysis","text":"<pre><code>-- Calculate storage savings from retiring incompatible partitions\nWITH incompatible_summary AS (\n    SELECT \n        p.view_set_name,\n        COUNT(*) as incompatible_partitions,\n        SUM(p.file_size) as incompatible_size_bytes\n    FROM list_partitions() p\n    JOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name\n    WHERE p.file_schema_hash != vs.current_schema_hash\n    GROUP BY p.view_set_name\n),\ntotal_summary AS (\n    SELECT \n        view_set_name,\n        COUNT(*) as total_partitions,\n        SUM(file_size) as total_size_bytes\n    FROM list_partitions()\n    GROUP BY view_set_name\n)\nSELECT \n    t.view_set_name,\n    COALESCE(i.incompatible_partitions, 0) as incompatible_partitions,\n    t.total_partitions,\n    ROUND(100.0 * COALESCE(i.incompatible_partitions, 0) / t.total_partitions, 2) as incompatible_percentage,\n    COALESCE(i.incompatible_size_bytes, 0) as incompatible_size_bytes,\n    t.total_size_bytes,\n    ROUND(100.0 * COALESCE(i.incompatible_size_bytes, 0) / t.total_size_bytes, 2) as size_percentage\nFROM total_summary t\nLEFT JOIN incompatible_summary i ON t.view_set_name = i.view_set_name\nORDER BY size_percentage DESC;\n</code></pre>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Micromegas is built on a modern lakehouse architecture designed for high-performance observability data collection and analytics.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<pre><code>graph TD\n    subgraph \"Application Layer\"\n        App1[Your Application]\n        App2[Another Service]\n        App3[Third Service]\n    end\n\n    subgraph \"Micromegas Tracing\"\n        Lib1[micromegas-tracing]\n        Lib2[micromegas-tracing]\n        Lib3[micromegas-tracing]\n        Sink1[telemetry-sink]\n        Sink2[telemetry-sink]\n        Sink3[telemetry-sink]\n    end\n\n    subgraph \"Ingestion Layer\"\n        Ingestion[telemetry-ingestion-srv&lt;br/&gt;:9000 HTTP]\n    end\n\n    subgraph \"Storage Layer\"\n        PG[(PostgreSQL&lt;br/&gt;Metadata &amp; Schema)]\n        S3[(Object Storage&lt;br/&gt;S3/GCS/Local&lt;br/&gt;Raw Payloads)]\n    end\n\n    subgraph \"Maintenance\"\n        Admin[telemetry-admin&lt;br/&gt;crond]\n    end\n\n    subgraph \"Analytics Layer\"\n        DataFusion[DataFusion Engine]\n        Parquet[(Parquet Files&lt;br/&gt;Columnar Views)]\n        FlightSQL[flight-sql-srv&lt;br/&gt;:50051 FlightSQL]\n        WebApp[analytics-web-srv&lt;br/&gt;:8000 HTTP]\n    end\n\n    subgraph \"Client Layer\"\n        PyClient[Python Client]\n        Grafana[Grafana Plugin]\n        Custom[Custom Clients]\n        Browser[Web Browser&lt;br/&gt;Analytics UI]\n    end\n\n    App1 --&gt; Lib1\n    App2 --&gt; Lib2 \n    App3 --&gt; Lib3\n    Lib1 --&gt; Sink1\n    Lib2 --&gt; Sink2\n    Lib3 --&gt; Sink3\n\n    Sink1 --&gt; Ingestion\n    Sink2 --&gt; Ingestion\n    Sink3 --&gt; Ingestion\n\n    Ingestion --&gt; PG\n    Ingestion --&gt; S3\n\n    PG --&gt; DataFusion\n    S3 --&gt; DataFusion\n    DataFusion --&gt; Parquet\n    DataFusion --&gt; FlightSQL\n\n    Admin --&gt; PG\n    Admin --&gt; S3\n    Admin --&gt; Parquet\n\n    FlightSQL --&gt; PyClient\n    FlightSQL --&gt; Grafana\n    FlightSQL --&gt; Custom\n    FlightSQL --&gt; WebApp\n    WebApp --&gt; Browser\n\n    classDef app fill:#e8f5e8\n    classDef tracing fill:#fff3e0\n    classDef service fill:#f3e5f5\n    classDef storage fill:#e1f5fe\n    classDef client fill:#fce4ec\n\n    class App1,App2,App3 app\n    class Lib1,Lib2,Lib3,Sink1,Sink2,Sink3 tracing\n    class Ingestion,FlightSQL,DataFusion,Admin,WebApp service\n    class PG,S3,Parquet storage\n    class PyClient,Grafana,Custom,Browser client</code></pre>"},{"location":"architecture/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"architecture/#data-collection","title":"Data Collection","text":"<ul> <li>Tracing Library: Ultra-low overhead (20ns per event) instrumentation embedded in applications</li> <li>Telemetry Sink: Batches events and handles transmission to ingestion service</li> <li>Ingestion Service: HTTP endpoint for receiving telemetry data from sinks</li> </ul>"},{"location":"architecture/#data-storage","title":"Data Storage","text":"<ul> <li>PostgreSQL: Stores metadata, process information, and stream definitions</li> <li>Object Storage: Stores raw telemetry payloads in efficient binary format (S3, GCS, or local files)</li> <li>Lakehouse: Materialized Parquet views created on-demand for fast analytics</li> </ul>"},{"location":"architecture/#analytics-engine","title":"Analytics Engine","text":"<ul> <li>DataFusion: SQL query engine with vectorized execution optimized for Parquet (columnar format)</li> <li>FlightSQL: High-performance query protocol using Apache Arrow for data transfer</li> <li>HTTP Gateway: REST API gateway for accessing FlightSQL analytics service</li> <li>Analytics Web App: Web interface for exploring data, generating Perfetto traces, and monitoring processes</li> <li>Maintenance Daemon: Background processing for view materialization and data lifecycle</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<pre><code>flowchart TD\n    App[Application Code] --&gt; Lib[Micromegas Tracing]\n    Lib --&gt; Sink[Telemetry Sink]\n    Sink --&gt; HTTP[HTTP Ingestion Service]\n\n    HTTP --&gt; PG[(PostgreSQL&lt;br/&gt;Metadata)]\n    HTTP --&gt; S3[(Object Storage&lt;br/&gt;Raw Payloads)]\n\n    PG --&gt; Analytics[Analytics Engine]\n    S3 --&gt; Analytics\n    Analytics --&gt; Parquet[(Parquet Files&lt;br/&gt;Lakehouse)]\n    Analytics --&gt; Client[SQL Client]\n\n    Client --&gt; Dashboard[Dashboards &amp; Analysis]\n\n    classDef storage fill:#e1f5fe\n    classDef compute fill:#f3e5f5\n    classDef client fill:#e8f5e8\n\n    class PG,S3,Parquet storage\n    class HTTP,Analytics compute\n    class App,Client,Dashboard client</code></pre>"},{"location":"architecture/#data-flow-steps","title":"Data Flow Steps","text":"<ol> <li>Instrumentation: Applications emit telemetry events using the Micromegas tracing library</li> <li>Collection: Events are batched and sent to the ingestion service via HTTP</li> <li>Storage: Metadata stored in PostgreSQL, raw payloads stored in object storage</li> <li>Materialization: Views created on-demand from raw data using DataFusion</li> <li>Query: SQL interface provides analytics capabilities through FlightSQL</li> </ol>"},{"location":"architecture/#lakehouse-architecture","title":"Lakehouse Architecture","text":"<pre><code>flowchart TD\n    subgraph \"Data Lake Layer\"\n        Events[Live Events&lt;br/&gt;Logs, Metrics, Spans]\n        Binary[(Binary Blocks&lt;br/&gt;LZ4 Compressed&lt;br/&gt;Custom Format)]\n    end\n\n    subgraph \"Processing Layer\"\n        JIT[JIT ETL Engine]\n        Live[Live ETL&lt;br/&gt;Maintenance Daemon]\n    end\n\n    subgraph \"Lakehouse Layer\"\n        Parquet[(Parquet Files&lt;br/&gt;Columnar Format&lt;br/&gt;Optimized for Analytics)]\n        Views[Materialized Views&lt;br/&gt;Global &amp; Process-Scoped]\n    end\n\n    subgraph \"Query Layer\"\n        DataFusion[DataFusion SQL Engine]\n        Client[Query Clients]\n    end\n\n    Events --&gt; Binary\n    Binary --&gt; JIT\n    Binary --&gt; Live\n\n    JIT --&gt; Parquet\n    Live --&gt; Views\n    Views --&gt; Parquet\n\n    Parquet --&gt; DataFusion\n    DataFusion --&gt; Client\n\n    JIT -.-&gt;|\"On-demand&lt;br/&gt;Process-scoped\"| Views\n    Live -.-&gt;|\"Continuous&lt;br/&gt;Global views\"| Views\n\n    classDef datalake fill:#ffebee\n    classDef process fill:#e8f5e8\n    classDef lakehouse fill:#e3f2fd\n    classDef query fill:#f3e5f5\n\n    class Events,Binary datalake\n    class JIT,Live process\n    class Parquet,Views lakehouse\n    class DataFusion,Client query</code></pre>"},{"location":"architecture/#data-transformation-flow","title":"Data Transformation Flow","text":""},{"location":"architecture/#1-data-lake-ingestion","title":"1. Data Lake Ingestion","text":"<ul> <li>Events collected from applications in real-time</li> <li>Stored as compressed binary blocks in object storage</li> <li>Custom binary format optimized for high-throughput writes</li> </ul>"},{"location":"architecture/#2-dual-processing-strategies","title":"2. Dual Processing Strategies","text":"<p>Live ETL (Maintenance Daemon): - Processes recent data continuously (every second/minute/hour) - Creates global materialized views for cross-process analytics - Optimized for dashboards and real-time monitoring</p> <p>JIT ETL (On-Demand): - Triggered when querying process-specific data - Fetches relevant blocks, decompresses, and converts to Parquet - Optimized for deep-dive analysis and debugging</p>"},{"location":"architecture/#3-lakehouse-analytics-optimization","title":"3. Lakehouse Analytics Optimization","text":"<ul> <li>Parquet columnar format enables efficient scanning and filtering</li> <li>Dictionary compression reduces storage and improves query performance  </li> <li>Predicate pushdown leverages Parquet metadata for fast data pruning</li> </ul>"},{"location":"architecture/#analytics-web-application","title":"Analytics Web Application","text":"<p>The analytics web app provides a modern web interface for exploring telemetry data. It consists of:</p> <ul> <li>Backend: Rust-based web server (<code>analytics-web-srv</code>) using Axum framework</li> <li>Frontend: Next.js React application with TypeScript  </li> <li>Integration: Direct FlightSQL connection to analytics service</li> </ul>"},{"location":"architecture/#key-features","title":"Key Features","text":"<ul> <li>Process Explorer: View active processes with real-time metadata</li> <li>Log Viewer: Stream log entries with level filtering and color coding</li> <li>Trace Generation: Generate and download Perfetto traces from process data</li> <li>Process Statistics: Detailed process metrics and monitoring</li> </ul> <p>Development Stage</p> <p>The Analytics Web Application is in early development and only suitable for local testing. Not recommended for production use.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ul> <li>High-frequency collection: Support for 100k+ events/second per process</li> <li>Cost-efficient storage: Cheap object storage for raw data with on-demand processing</li> <li>Dual ETL strategy: Live processing for recent data, JIT for historical analysis</li> <li>Unified observability: Logs, metrics, and traces in single queryable format</li> <li>Tail sampling friendly: Store everything cheaply, process selectively</li> </ul>"},{"location":"cost-comparisons/cost-modeling/","title":"Observability Solutions: A Cost Modeling","text":"<p>Disclaimer: This document provides a high-level, qualitative comparison of the Micromegas cost model against common pricing models found in other observability solutions. This document was authored by Gemini, a large language model from Google. Direct cost comparisons are complex and depend heavily on specific usage patterns, data volumes, and negotiated enterprise pricing. This guide is intended to highlight different cost philosophies rather than provide a quantitative analysis.</p>"},{"location":"cost-comparisons/cost-modeling/#common-pricing-models-in-commercial-observability-platforms","title":"Common Pricing Models in Commercial Observability Platforms","text":"<p>Most commercial observability solutions use one or a combination of the following pricing models:</p> <ol> <li> <p>Per-GB Ingested: This is one of the most common models. You are charged based on the volume of log, metric, and trace data you send to the platform each month.</p> <ul> <li>Pros: Simple to understand initially.</li> <li>Cons: Can lead to unpredictable costs. Often encourages teams to sample aggressively or drop data to control costs, potentially losing valuable insights. Costs can scale quickly with application growth or during incidents (when you need data the most).</li> </ul> </li> <li> <p>Per-Host / Per-Node: You are charged a flat rate for each server, container, or agent you monitor.</p> <ul> <li>Pros: Predictable monthly costs.</li> <li>Cons: Can be expensive for highly dynamic or containerized environments (e.g., Kubernetes) where the number of nodes fluctuates. This model is particularly impractical for widely distributed applications (e.g., client-side instrumentation on desktop or mobile) where the node count can be massive and unpredictable. It may not accurately reflect the actual data volume or value derived from each node.</li> </ul> </li> <li> <p>Per-User: You are charged based on the number of users who have access to the platform.</p> <ul> <li>Pros: Predictable and easy to manage for small teams.</li> <li>Cons: Can become a bottleneck, discouraging widespread access to observability data across an organization. It doesn't scale well as more engineers, SREs, and product managers need access.</li> </ul> </li> <li> <p>Feature-Based Tiers: Platforms often bundle features into different tiers (e.g., Basic, Pro, Enterprise). Higher tiers unlock advanced features like longer data retention, more sophisticated analytics, or higher user counts.</p> <ul> <li>Pros: Allows you to pay for only the features you need.</li> <li>Cons: You may be forced into a much more expensive tier to get a single critical feature. The cost jump between tiers can be substantial.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/cost-modeling/#the-micromegas-cost-model-direct-infrastructure-cost","title":"The Micromegas Cost Model: Direct Infrastructure Cost","text":"<p>Micromegas takes a fundamentally different approach. Instead of abstracting away the infrastructure, it is designed to be deployed and run on your own cloud account (e.g., AWS, GCP, Azure).</p> <p>Your cost is the direct cost of the underlying cloud services you consume.</p> <p>As detailed in the <code>README.md</code>, these costs are primarily:</p> <ul> <li>Blob Storage (e.g., S3, GCS): For storing raw telemetry data and materialized views. This is typically the largest portion of the cost.</li> <li>Compute (e.g., EC2, Kubernetes, Fargate): For running the ingestion, analytics, and daemon services.</li> <li>Database (e.g., PostgreSQL, Aurora): For storing metadata.</li> <li>Networking (e.g., Load Balancers, Data Transfer): For routing traffic and moving data.</li> </ul>"},{"location":"cost-comparisons/cost-modeling/#comparison-of-philosophies","title":"Comparison of Philosophies","text":"Aspect Commercial SaaS Platforms Micromegas Cost Basis Abstracted (per-GB, per-host, per-user) Concrete (direct cloud infrastructure spend) Transparency Opaque. The vendor's margin is built into the price. Fully transparent. You see every dollar spent on your cloud bill. Control Limited. You control the data you send, but not the underlying infrastructure or its cost efficiency. Full control. You can fine-tune every component, choose instance types, and optimize storage tiers. Scalability Scales automatically, but costs can become unpredictable and grow non-linearly. Cost scales more directly and predictably with resource consumption. You manage the scaling. Data Ownership Your data is in a third-party system. Your data never leaves your cloud account, enhancing security and data governance. Cost Management Relies on sampling, filtering, and dropping data before ingestion. Relies on on-demand processing (tail sampling). Keep all raw data in cheap storage and only pay to process what you need, when you need it. Management Overhead Low. The vendor manages the platform. Higher. You are responsible for deploying, managing, and scaling the Micromegas services."},{"location":"cost-comparisons/cost-modeling/#when-to-consider-the-micromegas-model","title":"When to Consider the Micromegas Model","text":"<p>The Micromegas cost model is particularly advantageous if:</p> <ul> <li>Cost at scale is a major concern. For large data volumes, paying direct infrastructure costs is almost always cheaper than paying the margins built into a commercial SaaS product.</li> <li>You want maximum control and transparency. You can make granular decisions about cost vs. performance for every component.</li> <li>Data governance and security are paramount. Keeping all telemetry data within your own cloud environment is a significant security benefit.</li> <li>You have the operational maturity to manage a distributed system.</li> </ul> <p>In summary, while commercial platforms offer convenience and managed simplicity, Micromegas provides a path to a more transparent, controllable, and potentially much lower cost structure at scale, by aligning your observability costs directly with your infrastructure spend.</p>"},{"location":"cost-comparisons/cost-modeling/#detailed-comparisons","title":"Detailed Comparisons","text":"<p>For a more in-depth, hypothetical dollar-for-dollar comparison, refer to the following documents:</p> <ul> <li>Micromegas vs. Splunk</li> <li>Micromegas vs. Grafana Cloud Stack</li> <li>Micromegas vs. Datadog</li> <li>Micromegas vs. Elastic Observability</li> <li>Micromegas vs. New Relic</li> <li>Micromegas vs. Dynatrace</li> </ul>"},{"location":"cost-comparisons/datadog/","title":"Cost Comparison: Micromegas vs. Datadog","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Datadog SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Datadog's pricing is complex and modular. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/datadog/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Infrastructure: 20 hosts/nodes to monitor.</li> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days for logs and traces (requires extended retention plans).</li> </ul> </li> <li> <p>Datadog Pricing Assumption:</p> <ul> <li>We will use the publicly available pricing for the Datadog Pro and Enterprise plans as of mid-2025, as different products are required. Datadog's pricing is highly modular, with costs varying based on specific product usage and consumption. (see references 1, 2)</li> <li>Datadog's pricing is highly modular. We will estimate the cost by combining the necessary components.</li> <li>Assumption on Datadog Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the billable units for Datadog.<ul> <li>Average log entry size for Datadog (after processing): 500 bytes. This is a common approximation for a typical, well-structured log entry after processing, including the message, timestamp, and various attributes/tags. Datadog's API supports log entries up to 1MB, implying that typical entries are significantly smaller. (see reference 3)</li> <li>Average trace event size for Datadog (after processing): 1 KB. This is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links.</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/datadog/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Datadog APM, Grafana Tempo, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/datadog/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/datadog/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#2-estimated-monthly-cost-datadog-logs-metrics-only","title":"2. Estimated Monthly Cost: Datadog (Logs &amp; Metrics Only)","text":"<p>The Datadog cost is calculated by summing the costs for its individual products based on the defined workload and assumed data sizes, including the cost of extended retention.</p> <ul> <li> <p>Infrastructure Monitoring:</p> <ul> <li><code>20 hosts * ~$23/host/month (Pro Plan)</code></li> <li>Estimated Cost: ~$460 / month</li> </ul> </li> <li> <p>Log Management:</p> <ul> <li>Ingestion Volume: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Ingestion Cost: <code>1,500 GB/month * ~$0.10/GB = ~$150 / month</code></li> <li>Retention Cost (90 days): Datadog charges for log retention beyond 15 days. Assuming <code>~$2.50 per million log events</code> for 60 extra days, this is a rough estimate.</li> <li><code>3,000 million log events/month * ~$2.50/million = ~$7,500 / month</code></li> <li>Estimated Cost (including retention): ~$7,650 / month</li> </ul> </li> <li> <p>Subtotal (Platform): ~$8,110 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Datadog is a feature-rich but complex platform that requires significant internal expertise to manage effectively. This cost is not zero but is considered part of the value of the SaaS subscription for this comparison.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$8,110 / month</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Datadog (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$8,110 / month Total Estimated Cost ~$3,500 / month ~$8,110 / month"},{"location":"cost-comparisons/datadog/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the dramatic cost difference for high-volume telemetry between a self-hosted, compact solution and a feature-rich SaaS platform.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events, the estimated TCO for Micromegas is dramatically lower than for Datadog. This difference is primarily driven by the much more compact data representation and storage efficiency of Micromegas, especially for logs and traces, which directly translates to lower infrastructure costs.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>Datadog (Federated but Integrated): Datadog's platform consists of distinct products for logs, metrics, and traces. While highly integrated for user experience, the underlying data is stored in specialized systems, leading to separate billing for each.</li> <li>Micromegas (Unified): Micromegas uses a single, unified storage and query layer for all telemetry types. This can offer more powerful and fundamental data correlation capabilities and significantly better storage efficiency.</li> </ul> </li> <li> <p>Cost Complexity: Datadog's pricing model is famously complex and modular. While this offers flexibility, it can also lead to unpredictable and extremely high costs as you enable more features or as usage patterns change, especially for high-volume data. Micromegas's cost is simpler to understand, as it maps directly to your cloud bill.</p> </li> <li> <p>Control &amp; Data Ownership: This remains a primary differentiator. Micromegas offers full data ownership and control within your own cloud environment, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: The cost dynamics are heavily skewed towards Micromegas at extreme scale due to its superior data compactness. In a real-world scenario, to manage these costs, a Datadog deployment handling such high volumes would rely heavily on aggressive sampling of logs and traces, potentially sacrificing data completeness for cost efficiency.</p> </li> </ul>"},{"location":"cost-comparisons/datadog/#references","title":"References","text":"<ol> <li>Datadog Pricing: A Comprehensive Guide - Middleware</li> <li>Datadog Pricing Main Caveats Explained (Updated for 2025) - SigNoz</li> <li>Datadog API Reference - Log Submission</li> </ol>"},{"location":"cost-comparisons/dynatrace/","title":"Cost Comparison: Micromegas vs. Dynatrace","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Dynatrace SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Dynatrace's pricing is complex, primarily based on host units, Dynatrace Units (DDUs) for data ingestion, and user seats. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/dynatrace/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Infrastructure: 20 hosts/nodes to monitor.</li> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> <li>Users: 5 active users.</li> </ul> </li> <li> <p>Dynatrace Pricing Assumption:</p> <ul> <li>Dynatrace's pricing is primarily based on Host Units (for infrastructure and APM monitoring) and Dynatrace Units (DDUs) for data ingestion (logs, custom metrics, traces).</li> <li>We will use publicly available pricing estimates for their standard plans as of mid-2025.</li> <li>Host Unit Cost: We assume ~$69 per Host Unit per month (for a typical cloud instance).</li> <li>DDU Cost: We assume ~$0.001 per DDU (with DDUs consumed by logs, custom metrics, and traces).</li> <li>Assumption on Dynatrace Data Size (DDUs): To enable a dollar-for-dollar comparison based on events, we must estimate the DDU consumption for these events in Dynatrace. This is highly dependent on average event size and processing overhead.<ul> <li>Average log entry DDU consumption: ~0.0005 DDUs per log event (equivalent to ~0.5 KB of raw log data).</li> <li>Average metric data point DDU consumption: ~0.000001 DDUs per metric data point (for custom metrics).</li> <li>Average trace event DDU consumption: ~0.0005 DDUs per trace event (for spans).</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/dynatrace/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Dynatrace, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB, spans, or DDUs, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per unit, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/dynatrace/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/dynatrace/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/dynatrace/#2-estimated-monthly-cost-dynatrace-logs-metrics-only","title":"2. Estimated Monthly Cost: Dynatrace (Logs &amp; Metrics Only)","text":"<p>The Dynatrace cost is calculated by summing the costs for Host Units and DDU consumption for logs and metrics.</p> <ul> <li> <p>Host Units:</p> <ul> <li><code>20 hosts * ~$69/host unit/month</code></li> <li>Subtotal (Host Units): ~$1,380 / month</li> </ul> </li> <li> <p>Logs (DDUs):</p> <ul> <li><code>3 billion log entries/month * 0.0005 DDUs/log event = 1,500,000 DDUs/month</code></li> <li><code>1,500,000 DDUs * ~$0.001/DDU = ~$1,500 / month</code></li> <li>Subtotal (Logs DDUs): ~$1,500 / month</li> </ul> </li> <li> <p>Metrics (DDUs - for custom metrics beyond standard host metrics):</p> <ul> <li><code>~92 billion metric data points/month * 0.000001 DDUs/metric = ~92,000 DDUs/month</code></li> <li><code>~92,000 DDUs * ~$0.001/DDU = ~$92 / month</code></li> <li>Subtotal (Metrics DDUs): ~$92 / month</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$2,972 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Dynatrace is a highly automated SaaS platform, but it still requires internal expertise for configuration, custom dashboards, and leveraging its advanced AI capabilities. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost: ~$2,972 / month</p> </li> </ul>"},{"location":"cost-comparisons/dynatrace/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Dynatrace (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$2,972 / month Total Estimated Cost ~$3,500 / month ~$2,972 / month"},{"location":"cost-comparisons/dynatrace/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the impact of host-based and DDU-based pricing models on overall costs.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events (excluding traces), the estimated TCO for Micromegas is lower than for Dynatrace. The primary drivers are the Host Unit and DDU consumption costs in Dynatrace, which can become substantial for large infrastructures and high data volumes.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>Dynatrace (AI-Powered Observability): Dynatrace is known for its highly automated, AI-driven approach to observability, offering deep insights into application performance and dependencies with minimal manual configuration. It aims to provide answers, not just data.</li> <li>Micromegas (Unified &amp; Self-Hosted): Micromegas provides a unified data model for all telemetry types within your own cloud environment, offering greater control and cost efficiency for high-volume data, particularly when deep, unsampled data is required.</li> </ul> </li> <li> <p>Pricing Model: Dynatrace's pricing combines Host Units (for infrastructure and APM) with DDUs (for data ingestion). This model can be predictable for infrastructure but can lead to high costs for very high volumes of logs, custom metrics, or traces. Micromegas's cost is directly tied to your cloud infrastructure spend.</p> </li> <li> <p>Operational Burden: Dynatrace, as a SaaS, has a lower operational burden for managing the platform itself due to its high automation. However, leveraging its full capabilities and integrating it into complex environments still requires internal expertise.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership and control within your own cloud account, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: For very high data volumes, especially logs and traces, the DDU consumption in Dynatrace can become very high, potentially making a self-hosted solution like Micromegas more cost-effective. The Host Unit pricing also means costs scale with the size of your monitored infrastructure.</p> </li> </ul>"},{"location":"cost-comparisons/elastic/","title":"Cost Comparison: Micromegas vs. Elastic Observability","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the Elastic Observability solution hosted on Elastic Cloud. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Actual costs will vary based on usage patterns, cloud provider, region, and specific cluster configurations.</p>"},{"location":"cost-comparisons/elastic/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events (equivalent to 82.5 billion spans)</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months)</li> </ul> </li> <li> <p>Elastic Cloud Pricing Assumption:</p> <ul> <li>Elastic Cloud uses resource-based pricing (RAM, Storage). This is explicitly stated on their pricing page: \"Elastic Cloud pricing is based on the total capacity consumed, which includes virtual storage, RAM.\" (see reference 1)</li> <li>We need to estimate the cluster size required to handle the event volume and retention.</li> <li>Assumption on Elastic Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the storage consumed by these events in Elastic. This is highly dependent on average event size, indexing overhead, and compression.<ul> <li>Average log entry size in Elastic (after indexing/overhead): 500 bytes. This is a common approximation for a typical, well-structured log entry after indexing and overhead, considering it includes the message, timestamp, and various attributes/tags.</li> <li>Average metric data point size in Elastic: 100 bytes. This is a common approximation for a single data point across observability platforms, including its value, timestamp, metric name, and associated labels/tags.</li> <li>Average trace span size in Elastic: 1 KB. This is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links.</li> </ul> </li> <li>Calculated Storage Needed for Elastic:<ul> <li>Logs: 9 billion * 500 bytes = 4.5 TB</li> <li>Metrics: 275 billion * 100 bytes = 27.5 TB</li> <li>Traces: 82.5 billion * 1 KB = 82.5 TB</li> <li>Total Raw Storage Needed: ~114.5 TB</li> </ul> </li> <li>Elastic Cloud Cluster Sizing: To handle ~114.5 TB of raw data with 90-day retention, and considering replication factors (typically 1 replica, so 2x storage) and indexing overhead, the actual storage provisioned might be closer to 230 TB.</li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/elastic/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/elastic/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#2-estimated-monthly-cost-elastic-cloud","title":"2. Estimated Monthly Cost: Elastic Cloud","text":"<p>The Elastic Cloud cost is calculated based on the resources required for a cluster capable of handling the specified event volume and retention.</p> <ul> <li> <p>Cluster Configuration: To handle ~38 TB of raw data per month (which translates to ~76 TB provisioned storage with replication and overhead) and the associated query load, a substantial cluster is required.</p> <ul> <li>We will estimate a cluster with 76 TB of storage and corresponding compute (e.g., 2,400 GB RAM).</li> </ul> </li> <li> <p>Estimated Monthly Cost:</p> <ul> <li>Based on public pricing for I/O Optimized instances:<ul> <li>RAM cost: <code>2,400 GB * ~$0.50/GB/month = ~$1,200 / month</code></li> <li>Storage cost: <code>76,000 GB * ~$0.10/GB/month = ~$7,600 / month</code></li> </ul> </li> <li>Subtotal (Platform): ~$8,800 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Elastic Cloud is a managed service, but it still requires significant expertise to manage data schemas (index templates), build visualizations in Kibana, and optimize queries. This cost is considered part of the value of the SaaS subscription for this comparison.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost: ~$8,800 / month</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Elastic Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$8,800 / month Total Estimated Cost ~$3,500 / month ~$8,800 / month"},{"location":"cost-comparisons/elastic/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the significant impact of data compactness on overall cost, especially for high-volume telemetry.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events, the estimated TCO for Micromegas is significantly lower than for Elastic Cloud. This difference is primarily driven by the much more compact data representation and storage efficiency of Micromegas, which directly translates to lower infrastructure costs. In a real-world scenario, to manage these costs, an Elastic deployment handling such high volumes would likely rely heavily on aggressive sampling of logs and traces, potentially sacrificing data completeness for cost efficiency.</p> </li> <li> <p>Architectural Philosophy:</p> <ul> <li>Elastic (Search Index-Centric): The Elastic Stack was built around the Lucene search index. It is exceptionally powerful for log search and text analysis. Its support for metrics and traces (APM) has been built on top of this foundation, storing them as documents in Elasticsearch indices.</li> <li>Micromegas (Unified Telemetry Model): Micromegas was designed from the ground up with a unified data model for logs, metrics, and traces. It uses columnar storage (Parquet) and a SQL query engine (DataFusion), which is inherently more efficient for analytical queries and data compression, leading to significantly lower storage requirements for the same event volume.</li> </ul> </li> <li> <p>Query Language: This is a major differentiator.</p> <ul> <li>Elastic: Uses KQL (Kibana Query Language) and Lucene query syntax, which are powerful for text search but can be a learning curve for teams primarily familiar with SQL.</li> <li>Micromegas: Uses SQL, the standard language for data analysis. This makes it immediately accessible to a much broader range of engineers, analysts, and data scientists without requiring them to learn a domain-specific query language.</li> </ul> </li> <li> <p>Operational Burden: Elastic Cloud has a lower operational burden, as Elastic manages the cluster's uptime, security, and patching. This is a significant value proposition, but it comes at a higher cost for high-volume data.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership within your own cloud account, offering a higher degree of control and simplifying data governance.</p> </li> <li> <p>Cost Model: The cost models are fundamentally different. Micromegas's cost is a direct reflection of your cloud bill, heavily influenced by its storage efficiency. Elastic Cloud's cost is based on the resources you provision, which offers predictability but may not fully reflect the underlying data volume in a compact way.</p> </li> </ul>"},{"location":"cost-comparisons/elastic/#references","title":"References","text":"<ol> <li>Official Elastic Cloud pricing - compare serverless and hosted</li> </ol>"},{"location":"cost-comparisons/grafana/","title":"vs. Grafana Cloud","text":""},{"location":"cost-comparisons/grafana/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points (equivalent to 1,000,000 active series for pricing)</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months)</li> <li>Users: 5 active users.</li> </ul> </li> <li> <p>Grafana Cloud Pricing Assumption:</p> <ul> <li>We will use the publicly available pricing for the Grafana Cloud Pro plan as of mid-2025.</li> <li>Pricing is calculated based on logs ingested (GB), metric series, traces ingested (GB), and users.</li> <li>Assumption on Grafana Cloud Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the storage consumed by these events in Grafana Cloud. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for Loki (after processing): 500 bytes</li> <li>Average trace event size for Tempo (after processing): 1 KB</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/grafana/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month, as seen in previous calculations). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/grafana/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/grafana/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/grafana/#2-estimated-monthly-cost-grafana-cloud-pro-logs-metrics-only","title":"2. Estimated Monthly Cost: Grafana Cloud Pro (Logs &amp; Metrics Only)","text":"<p>The Grafana Cloud cost for logs and metrics is calculated by summing the costs for its individual components based on the defined workload and assumed data sizes, including the cost of extended retention.</p> <ul> <li> <p>Logs (Loki):</p> <ul> <li>Ingestion Volume: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Ingestion Cost: <code>1,500 GB/month * ~$0.50/GB = ~$750 / month</code></li> <li>Retention Cost (90 days): Storing 1,500 GB for an additional 60 days is estimated to cost <code>~$0.30/GB</code>. <code>1,500 GB * $0.30/GB * 2 months</code> = <code>~$900 / month</code></li> <li>Subtotal (Logs): ~$1,650 / month</li> </ul> </li> <li> <p>Metrics (Mimir):</p> <ul> <li><code>1,000,000 active series</code> (retention is typically longer for metrics and included)</li> <li>Subtotal (Metrics): ~$1,000 / month</li> </ul> </li> <li> <p>Users:</p> <ul> <li><code>5 users</code></li> <li>Subtotal (Users): ~$100 / month</li> </ul> </li> <li> <p>Subtotal (Platform): ~$2,750 / month</p> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Grafana Cloud is a managed service, but it still requires internal expertise to build dashboards, run searches, and manage data onboarding. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$2,750 / month</p> </li> </ul> <p>Therefore, for this comparison, we will focus on the costs of logs and metrics, acknowledging that the trace handling philosophies and associated costs are fundamentally different and not directly comparable on a per-event basis without considering sampling strategies.</p>"},{"location":"cost-comparisons/grafana/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Grafana Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$2,750 / month Total Estimated Cost ~$3,500 / month ~$2,750 / month"},{"location":"cost-comparisons/newrelic/","title":"Cost Comparison: Micromegas vs. New Relic","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and the New Relic SaaS platform. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. New Relic's pricing is based on data ingested and user seats. Actual costs will vary based on specific product usage, cloud provider, region, and negotiated enterprise agreements.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/newrelic/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> <li>Users: 5 Full Users (access to all data and features).</li> </ul> </li> <li> <p>New Relic Pricing Assumption:</p> <ul> <li>New Relic's pricing is primarily based on data ingested (GB) and user seats.</li> <li>We will use publicly available pricing estimates for their standard plans as of mid-2025.</li> <li>Data Ingest Cost: We assume an average cost of ~$0.30 per GB of ingested data per month.</li> <li>User Seat Cost: We assume ~$99 per Full User per month.</li> <li>Assumption on New Relic Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the ingested GB for these events in New Relic. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for New Relic (after processing): 500 bytes</li> <li>Average metric data point size for New Relic: 100 bytes</li> <li>Average trace event size for New Relic: 1 KB</li> </ul> </li> </ul> </li> <li> <p>Micromegas TCO Assumption:</p> <ul> <li>The Total Cost of Ownership (TCO) for a self-hosted solution must include both direct infrastructure spend and the cost of personnel to manage the system.</li> <li>Infrastructure Costs: Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload (which results in ~8.5 TB of storage for Micromegas) is ~$1,000 / month.</li> <li>Operational Personnel Costs: We assume managing the self-hosted solution requires 20% of one full-time DevOps/SRE engineer's time. At a fully-loaded annual salary of $150,000, this equates to ~$2,500 / month.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/newrelic/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like New Relic APM, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/newrelic/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/newrelic/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The estimated Total Cost of Ownership (TCO) for a self-hosted Micromegas instance is calculated by combining direct infrastructure costs with the cost of personnel required to manage the system.</p> <ul> <li>Infrastructure Costs: ~$1,000 / month<ul> <li>(Includes blob storage, compute, database, and load balancer based on the example deployment, handling the specified event volume with ~8.5 TB of storage)</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs: ~$2,500 / month</p> <ul> <li>(Assumes 20% of a DevOps/SRE engineer's time)</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/newrelic/#2-estimated-monthly-cost-new-relic-logs-metrics-only","title":"2. Estimated Monthly Cost: New Relic (Logs &amp; Metrics Only)","text":"<p>The New Relic cost is calculated by summing the costs for data ingested and user seats.</p> <ul> <li> <p>Calculated Ingested GB for New Relic (Logs &amp; Metrics):</p> <ul> <li>Logs: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Metrics: <code>~92 billion metric data points/month * 100 bytes/point = ~9,200 GB/month</code></li> <li>Total Ingested GB (Logs &amp; Metrics): <code>1,500 + 9,200 = 10,700 GB/month</code></li> </ul> </li> <li> <p>Data Ingest Cost:</p> <ul> <li><code>10,700 GB/month * $0.30/GB</code></li> <li>Subtotal (Data Ingest): ~$3,210 / month</li> </ul> </li> <li> <p>User Seats:</p> <ul> <li><code>5 Full Users * ~$99/user/month</code></li> <li>Subtotal (Users): ~$495 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>New Relic is a managed SaaS, but it still requires internal expertise to configure agents, build dashboards, and optimize queries. This cost is considered part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$3,705 / month</p> </li> </ul>"},{"location":"cost-comparisons/newrelic/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) New Relic (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$3,705 / month Total Estimated Cost ~$3,500 / month ~$3,705 / month"},{"location":"cost-comparisons/newrelic/#qualitative-differences","title":"Qualitative Differences","text":"<p>This comparison highlights the impact of data volume and user-based pricing on overall costs.</p> <ul> <li> <p>Total Cost of Ownership (TCO): For the same volume of events (excluding traces), the estimated TCO for Micromegas is lower than for New Relic. The primary driver is the cost of data ingestion in New Relic, which can become substantial for high volumes.</p> </li> <li> <p>Platform Philosophy:</p> <ul> <li>New Relic (Integrated SaaS): New Relic offers a broad, integrated platform covering APM, infrastructure, logs, and more. It aims to provide a single pane of glass for observability, with a strong focus on application performance.</li> <li>Micromegas (Unified &amp; Self-Hosted): Micromegas provides a unified data model for all telemetry types within your own cloud environment, offering greater control and cost efficiency for high-volume data.</li> </ul> </li> <li> <p>Pricing Model: New Relic's pricing is a combination of data ingested and user seats. While data ingest is common, the per-user pricing can become a significant factor for larger teams. Micromegas's cost is directly tied to your cloud infrastructure spend.</p> </li> <li> <p>Operational Burden: New Relic, as a SaaS, has a lower operational burden for managing the platform itself. However, configuring agents and optimizing data ingestion still requires internal effort.</p> </li> <li> <p>Control &amp; Data Ownership: Micromegas provides full data ownership and control within your own cloud account, which is a critical requirement for many organizations.</p> </li> <li> <p>Cost at Extreme Scale: For very high data volumes, the cost of data ingestion in New Relic can become very high, potentially making a self-hosted solution like Micromegas more cost-effective. The user-based pricing also means costs scale with team size, regardless of data volume.</p> </li> </ul>"},{"location":"cost-comparisons/splunk/","title":"Cost Comparison: Micromegas vs. Splunk","text":"<p>Author: Gemini, a large language model from Google.</p> <p>Disclaimer: This document presents a hypothetical, dollar-for-dollar cost comparison between a self-hosted Micromegas deployment and a Splunk Cloud subscription. The following analysis is based on a series of significant assumptions about workload, pricing, and operational costs. These are estimates, not quotes. Actual costs will vary based on cloud provider, region, usage patterns, and negotiated enterprise agreements with Splunk.</p> <p>For a broader overview of observability cost models, see the Cost Modeling document.</p>"},{"location":"cost-comparisons/splunk/#core-assumptions-for-this-comparison","title":"Core Assumptions for this Comparison","text":"<ol> <li> <p>Workload Definition (based on Micromegas Example Deployment):</p> <ul> <li>Total Events (over 90-day retention):<ul> <li>Logs: 9 billion log entries</li> <li>Metrics: 275 billion metric data points</li> <li>Traces: 165 billion trace events</li> </ul> </li> <li>Monthly Ingestion Rate (for pricing comparison):<ul> <li>Logs: 3 billion log entries / month (9 billion / 3 months)</li> <li>Metrics: ~92 billion metric data points / month (275 billion / 3 months)</li> <li>Traces: 55 billion trace events / month (165 billion / 3 months)</li> </ul> </li> <li>Retention: 90 days (3 months).</li> </ul> </li> <li> <p>Splunk Pricing Assumption:</p> <ul> <li>Splunk's pricing is complex and not fully public. For this analysis, we assume an ingest-based pricing model for Splunk Cloud. This is supported by Splunk's official pricing page, which states: \"Pay based on the amount of data you bring into the Splunk Platform.\" (see reference 1)</li> <li>Based on publicly available industry analysis, which indicates significant volume discounts, we will use an estimated cost of $2.25 per GB of ingested data per month for this high-volume workload. This is a critical assumption, as actual costs can vary significantly based on negotiated enterprise rates. This estimate is a conservative adjustment to low-volume pricing examples (such as the one cited in reference 2) to account for expected discounts at scale. (see references 1, 2)</li> <li>Assumption on Splunk Data Size: To enable a dollar-for-dollar comparison based on events, we must estimate the ingested GB for these events in Splunk. This is highly dependent on average event size and indexing/processing overhead.<ul> <li>Average log entry size for Splunk (after indexing/overhead): 500 bytes</li> <li>Average metric data point size for Splunk: 100 bytes. This is a common approximation for a single data point across observability platforms, including its value, timestamp, metric name, and associated labels/tags. For example, Datadog's API documentation suggests that a metric data point, including its timestamp (8 bytes), value (8 bytes), metric name (approx. 20 bytes), and typical labels/tags (approx. 50 bytes of overhead per data point for unique identification), sums up to around 100 bytes when considering additional overhead. (see reference 3)</li> <li>Average trace event size for Splunk: 1 KB. While Splunk does not provide an exact average, this is a common industry approximation for a typical span (a trace event), considering it includes various attributes like operation name, start/end times, attributes, events, and links. Splunk APM has a maximum span size limit of 64KB, implying that typical spans are significantly smaller.</li> </ul> </li> </ul> </li> <li> <p>Micromegas Operational Cost Assumption:</p> <ul> <li>Self-hosting requires engineering time for setup, maintenance, and upgrades. This is a real cost.</li> <li>We assume this requires 20% of one full-time DevOps/SRE engineer's time.</li> <li>We assume a fully-loaded annual salary of $150,000 for this engineer, which translates to a monthly cost of $12,500.</li> </ul> </li> </ol>"},{"location":"cost-comparisons/splunk/#the-challenge-of-traces-why-direct-comparison-is-impractical","title":"The Challenge of Traces: Why Direct Comparison is Impractical","text":"<p>Micromegas is designed to ingest and store a very high volume of raw trace events (165 billion total, or 55 billion per month in our example) and process them on-demand. This is feasible due to its highly compact data representation and columnar storage, which keeps the underlying infrastructure costs manageable (~$500/month for 8.5 TB of total data, including traces).</p> <p>Commercial SaaS tracing solutions like Splunk APM, Grafana Tempo, Datadog APM, or Elastic APM are typically priced based on ingested GB or spans, and their underlying architectures are optimized for real-time analysis and high-cardinality indexing. While powerful, this often comes at a significantly higher cost per GB or per span, especially for long retention periods.</p> <p>For the 165 billion trace events (equivalent to 165 TB of raw data at 1KB/event) with 90-day retention, the estimated cost in a typical SaaS tracing solution would be prohibitively expensive (e.g., hundreds of thousands of dollars per month). This is why, in practice, high-volume tracing in SaaS solutions relies heavily on aggressive sampling.</p> <ul> <li>SaaS Tracing Reality: To manage costs, users of SaaS tracing solutions often implement head-based or tail-based sampling, meaning only a small fraction (e.g., 1-10%) of traces are actually ingested and retained. This sacrifices data completeness for cost control.</li> <li>Micromegas Tracing Philosophy: Micromegas is designed to ingest and retain a significantly larger volume of raw trace data compared to typical SaaS solutions. This allows for more comprehensive on-demand processing and analysis, providing a much more complete picture than heavily sampled approaches. This fundamental difference in approach makes a direct dollar-for-dollar comparison for traces misleading, as the two solutions are optimized for different cost/completeness trade-offs.</li> </ul>"},{"location":"cost-comparisons/splunk/#analysis-for-hypothetical-workload","title":"Analysis for Hypothetical Workload","text":""},{"location":"cost-comparisons/splunk/#1-estimated-monthly-cost-micromegas-self-hosted","title":"1. Estimated Monthly Cost: Micromegas (Self-Hosted)","text":"<p>The Micromegas cost is broken down into direct infrastructure spend and the operational personnel cost.</p> <ul> <li> <p>Infrastructure Costs:</p> <ul> <li>Based on the \"Example Deployment\" in the main <code>README.md</code>, the estimated monthly cloud spend for this workload is ~$1,000 / month.<ul> <li>(Includes blob storage, compute, database, and load balancer)</li> </ul> </li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>Based on the assumption of 20% of an engineer's time.</li> <li><code>$12,500/month * 0.20</code></li> <li>Subtotal (Personnel): ~$2,500 / month</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (TCO): ~$3,500 / month</p> </li> </ul>"},{"location":"cost-comparisons/splunk/#2-estimated-monthly-cost-splunk-cloud-logs-metrics-only","title":"2. Estimated Monthly Cost: Splunk Cloud (Logs &amp; Metrics Only)","text":"<p>The Splunk Cloud cost for logs and metrics is calculated based on the assumed ingest-based pricing model and the estimated ingested GB from the event volume.</p> <ul> <li> <p>Calculated Ingested GB for Splunk (Logs &amp; Metrics):</p> <ul> <li>Logs: <code>3 billion log entries/month * 500 bytes/entry = 1,500 GB/month</code></li> <li>Metrics: <code>~92 billion metric data points/month * 100 bytes/point = ~9,200 GB/month</code></li> <li>Total Ingested GB (Logs &amp; Metrics): <code>1,500 + 9,200 = 10,700 GB/month</code></li> </ul> </li> <li> <p>Ingestion Cost:</p> <ul> <li><code>10,700 GB/month * $2.25/GB</code></li> <li>Subtotal (Ingestion): ~$24,075 / month</li> </ul> </li> <li> <p>Operational &amp; Personnel Costs:</p> <ul> <li>While Splunk is a managed SaaS, it still requires internal expertise to build dashboards, run searches, and manage data onboarding. This cost is highly variable but generally lower than managing a full self-hosted solution. For this comparison, we will consider it part of the subscription's value.</li> </ul> </li> <li> <p>Total Estimated Monthly Cost (Logs &amp; Metrics): ~$24,075 / month</p> </li> </ul>"},{"location":"cost-comparisons/splunk/#dollar-for-dollar-comparison-summary","title":"Dollar-for-Dollar Comparison Summary","text":"Category Micromegas (Self-Hosted) Splunk Cloud (SaaS) Infrastructure Cost ~$1,000 / month (Included in subscription) Personnel / Ops Cost ~$2,500 / month (Included in subscription) Licensing / Subscription $0 ~$24,075 / month Total Estimated Cost ~$3,500 / month ~$24,075 / month"},{"location":"cost-comparisons/splunk/#qualitative-differences","title":"Qualitative Differences","text":"<p>Beyond the direct cost estimates, the two solutions represent different philosophies:</p> <ul> <li>Total Cost of Ownership (TCO): For the specified workload, the estimated TCO for Micromegas is significantly lower than Splunk Cloud. The primary driver is paying direct infrastructure costs versus a bundled SaaS price that includes the vendor's margin.</li> <li>Operational Burden: Micromegas carries a higher operational burden. You are responsible for the uptime, scaling, and maintenance of the system. Splunk, as a SaaS, handles this for you.</li> <li>Control &amp; Transparency: With Micromegas, you have full control over the infrastructure and complete transparency into the cost of every component. You can fine-tune instance types and storage classes to optimize costs. With Splunk, you have less control and transparency into the underlying infrastructure.</li> <li>Data Ownership &amp; Security: The Micromegas model means all telemetry data remains within your own cloud environment, which can be a major advantage for security and data governance.</li> <li>Scalability: Both solutions are designed to scale. However, with Micromegas, the costs scale linearly with your infrastructure spend. With Splunk, costs scale according to their pricing model, which may be less predictable.</li> </ul>"},{"location":"cost-comparisons/splunk/#references","title":"References","text":"<ol> <li>Splunk Pricing - Splunk</li> <li>Guide to Splunk Pricing and Costs in 2025 - Uptrace</li> <li>Datadog API Reference - Metric Submission</li> </ol>"},{"location":"development/build/","title":"Build Guide","text":"<p>This guide covers building Micromegas from source and setting up a development environment.</p>"},{"location":"development/build/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust - Latest stable version</li> <li>Python 3.8+</li> <li>Docker - For running PostgreSQL</li> <li>Git</li> </ul>"},{"location":"development/build/#rust-development","title":"Rust Development","text":""},{"location":"development/build/#clone-and-build","title":"Clone and Build","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas/rust\n\n# Build all components\ncargo build\n\n# Build with optimizations\ncargo build --release\n\n# Build specific component\ncargo build -p telemetry-ingestion-srv\n</code></pre>"},{"location":"development/build/#testing","title":"Testing","text":"<pre><code># Run all tests\ncargo test\n\n# Run tests with output\ncargo test -- --nocapture\n\n# Run specific test\ncargo test -p micromegas-tracing\n</code></pre>"},{"location":"development/build/#format-and-lint","title":"Format and Lint","text":"<pre><code># Format code (required before commits)\ncargo fmt\n\n# Run linter\ncargo clippy --workspace -- -D warnings\n\n# Run full CI pipeline\npython3 ../build/rust_ci.py\n</code></pre>"},{"location":"development/build/#advanced-builds","title":"Advanced Builds","text":"<pre><code># Clean build\ncargo clean &amp;&amp; cargo build\n\n# Release with debug symbols for profiling\ncargo build --profile release-debug\n\n# Profiling build\ncargo build --profile profiling\n\n# Cross-platform build\nrustup target add x86_64-pc-windows-gnu\ncargo build --target x86_64-pc-windows-gnu\n</code></pre>"},{"location":"development/build/#python-development","title":"Python Development","text":"<pre><code>cd python/micromegas\n\n# Install dependencies\npoetry install\n\n# Run tests\npytest\n\n# Format code (required before commits)\nblack .\n</code></pre>"},{"location":"development/build/#documentation","title":"Documentation","text":"<pre><code># Install dependencies\npip install -r mkdocs/docs-requirements.txt\n\n# Start development server\ncd mkdocs\nmkdocs serve\n\n# Build static site\nmkdocs build\n</code></pre>"},{"location":"development/build/#next-steps","title":"Next Steps","text":"<ul> <li>Contributing Guide - How to contribute to the project</li> <li>Getting Started - Set up a development instance</li> <li>Architecture Overview - Understand the system design</li> </ul>"},{"location":"grafana/","title":"Grafana Datasource Plugin","text":"<p>The Micromegas Grafana datasource plugin enables you to query your telemetry data directly in Grafana dashboards using SQL queries via Apache Arrow FlightSQL protocol.</p> <p>Plugin Origin</p> <p>This plugin was forked from InfluxDB's FlightSQL Grafana plugin and adapted for Micromegas while maintaining FlightSQL protocol compatibility.</p>"},{"location":"grafana/#overview","title":"Overview","text":"<p>The plugin provides:</p> <ul> <li>SQL Query Builder: Visual interface for building queries with dropdowns for tables and columns</li> <li>Raw SQL Mode: Full SQL support for complex queries</li> <li>Multiple Authentication Methods: Choose between API keys or OAuth 2.0 client credentials</li> <li>Time-Series Visualization: Native support for Grafana time-series panels</li> <li>Table Support: Display query results in Grafana tables</li> </ul>"},{"location":"grafana/#quick-start","title":"Quick Start","text":""},{"location":"grafana/#prerequisites","title":"Prerequisites","text":"<ul> <li>Grafana 9.0 or later</li> <li>Micromegas analytics server (flight-sql-srv) running</li> <li>Authentication credentials (API key or OAuth 2.0 client credentials)</li> </ul>"},{"location":"grafana/#installation","title":"Installation","text":"<ol> <li>Download the latest plugin release</li> <li>Extract to your Grafana plugins directory</li> <li>Restart Grafana</li> <li>Add Micromegas as a data source</li> </ol> <p>For detailed installation instructions, see the Installation Guide.</p>"},{"location":"grafana/#adding-a-data-source","title":"Adding a Data Source","text":"<ol> <li>Navigate to Configuration \u2192 Data Sources in Grafana</li> <li>Click Add data source</li> <li>Search for and select Micromegas</li> <li>Configure connection settings:</li> <li>Host: Your flight-sql-srv address (e.g., <code>localhost:50051</code>)</li> <li>Authentication Method: Choose API Key or OAuth 2.0</li> <li>TLS/SSL: Enable if your server uses TLS</li> <li>Click Save &amp; Test</li> </ol> <p>For detailed configuration, see the Configuration Guide.</p>"},{"location":"grafana/#key-features","title":"Key Features","text":""},{"location":"grafana/#query-builder","title":"Query Builder","text":"<p>The visual query builder helps you construct SQL queries:</p> <ul> <li>Select tables from dropdown</li> <li>Choose columns with autocomplete</li> <li>Add WHERE clauses with + button</li> <li>Switch to raw SQL for advanced queries</li> </ul>"},{"location":"grafana/#authentication-options","title":"Authentication Options","text":"<p>Choose the authentication method that fits your environment:</p> <ul> <li>API Keys: Simple, direct authentication (recommended for quick setup)</li> <li>OAuth 2.0 Client Credentials: Enterprise-grade authentication with identity provider integration</li> </ul> <p>See the Authentication Guide for setup instructions.</p>"},{"location":"grafana/#flightsql-protocol","title":"FlightSQL Protocol","text":"<p>The plugin uses Apache Arrow FlightSQL protocol for efficient data transfer:</p> <ul> <li>High-performance binary protocol</li> <li>Efficient columnar data format</li> <li>Streaming support for large result sets</li> <li>Compatible with any FlightSQL server</li> </ul>"},{"location":"grafana/#documentation","title":"Documentation","text":"<ul> <li>Installation - Install and set up the plugin</li> <li>Configuration - Configure connection settings</li> <li>Authentication - Set up API keys or OAuth 2.0</li> <li>Usage - Query builder and SQL examples</li> </ul>"},{"location":"grafana/#development","title":"Development","text":"<p>For development instructions, see the grafana/DEVELOPMENT.md file in the repository.</p>"},{"location":"grafana/#support","title":"Support","text":"<p>For issues or questions:</p> <ul> <li>GitHub Issues: madesroches/micromegas/issues</li> <li>Add label: <code>grafana-plugin</code></li> </ul>"},{"location":"grafana/#related-documentation","title":"Related Documentation","text":"<ul> <li>Query Guide - SQL query examples and patterns</li> <li>Schema Reference - Database schema documentation</li> <li>Authentication - Server-side authentication setup</li> </ul>"},{"location":"grafana/authentication/","title":"Authentication","text":"<p>The Micromegas Grafana plugin supports two authentication methods.</p>"},{"location":"grafana/authentication/#authentication-methods","title":"Authentication Methods","text":""},{"location":"grafana/authentication/#api-keys","title":"API Keys","text":"<p>Simple authentication using a static API key.</p> <ul> <li>Best for: Development, small deployments, quick start</li> </ul>"},{"location":"grafana/authentication/#oauth-20-client-credentials","title":"OAuth 2.0 Client Credentials","text":"<p>Enterprise authentication via identity provider (Google, Auth0, Azure AD, Okta).</p> <ul> <li>Best for: Production, enterprise deployments</li> </ul>"},{"location":"grafana/authentication/#api-key-authentication","title":"API Key Authentication","text":""},{"location":"grafana/authentication/#quick-setup","title":"Quick Setup","text":"<ol> <li> <p>Generate API Key:    <pre><code>openssl rand -base64 512\n</code></pre></p> </li> <li> <p>Configure server (see Admin Guide):    <pre><code>export MICROMEGAS_API_KEYS='[\n  {\"name\": \"grafana-prod\", \"key\": \"YOUR_GENERATED_KEY_HERE\"}\n]'\n</code></pre></p> </li> <li> <p>Configure Grafana datasource:</p> </li> <li>Auth Method: API Key</li> <li>API Key: Paste your generated key</li> <li>Save &amp; Test</li> </ol>"},{"location":"grafana/authentication/#oauth-20-client-credentials_1","title":"OAuth 2.0 Client Credentials","text":""},{"location":"grafana/authentication/#quick-setup_1","title":"Quick Setup","text":"<ol> <li>Create service account in your identity provider:</li> <li>Google: Service account with JSON key</li> <li>Auth0: Machine-to-Machine application</li> <li>Azure AD: App registration with client secret</li> <li> <p>Okta: Service app</p> </li> <li> <p>Configure server with OIDC settings (see Admin Guide)</p> </li> <li> <p>Configure Grafana datasource:</p> </li> <li>Auth Method: OAuth 2.0 Client Credentials</li> <li>OIDC Issuer: Your provider URL</li> <li>Client ID: From step 1</li> <li>Client Secret: From step 1</li> <li>Audience: (Auth0/Azure AD only)</li> <li>Enable User Attribution: On (default) or Off</li> <li>Save &amp; Test</li> </ol>"},{"location":"grafana/authentication/#privacy-settings","title":"Privacy Settings","text":"<p>Enable User Attribution controls whether user information is sent with queries:</p> <ul> <li>Enabled (default): Grafana username and email are logged on the server for audit purposes</li> <li>Disabled: Only the service account identity is logged</li> </ul> <p>User attribution provides an audit trail showing which Grafana user ran which queries. This is separate from authentication (the service account authenticates the connection).</p>"},{"location":"grafana/authentication/#provider-urls","title":"Provider URLs","text":"Provider Issuer URL Google <code>https://accounts.google.com</code> Auth0 <code>https://YOUR-TENANT.auth0.com</code> Azure AD <code>https://login.microsoftonline.com/TENANT-ID/v2.0</code> Okta <code>https://YOUR-DOMAIN.okta.com</code>"},{"location":"grafana/authentication/#example-auth0","title":"Example: Auth0","text":"<p>Create a Machine-to-Machine application in Auth0:</p> <ol> <li>Go to Applications \u2192 Create Application</li> <li>Choose \"Machine to Machine Applications\"</li> <li>Select your API or create a new API identifier</li> <li>Copy the Client ID and Client Secret</li> </ol> <p>Grafana Configuration: <pre><code>Auth Method: OAuth 2.0 Client Credentials\nOIDC Issuer: https://YOUR-TENANT.auth0.com\nClient ID: (from Auth0 application)\nClient Secret: (from Auth0 application)\nAudience: https://your-api-identifier (your API identifier from Auth0)\n</code></pre></p>"},{"location":"grafana/authentication/#example-google-cloud","title":"Example: Google Cloud","text":"<pre><code># Create service account\ngcloud iam service-accounts create grafana-prod \\\n  --display-name=\"Grafana Micromegas Datasource\"\n\n# Create key\ngcloud iam service-accounts keys create credentials.json \\\n  --iam-account=grafana-prod@PROJECT.iam.gserviceaccount.com\n</code></pre> <p>Grafana Configuration: <pre><code>Auth Method: OAuth 2.0 Client Credentials\nOIDC Issuer: https://accounts.google.com\nClient ID: grafana-prod@PROJECT.iam.gserviceaccount.com\nClient Secret: (from credentials.json)\n</code></pre></p>"},{"location":"grafana/authentication/#testing","title":"Testing","text":"<p>Click Save &amp; Test to verify connection and authentication.</p>"},{"location":"grafana/authentication/#next-steps","title":"Next Steps","text":"<ul> <li>Configure queries</li> <li>Server authentication setup</li> </ul>"},{"location":"grafana/configuration/","title":"Configuration","text":"<p>This guide covers configuring the Micromegas datasource in Grafana.</p>"},{"location":"grafana/configuration/#adding-a-data-source","title":"Adding a Data Source","text":"<ol> <li>Navigate to Configuration \u2192 Data Sources in Grafana</li> <li>Click Add data source</li> <li>Search for and select Micromegas</li> <li>Configure the settings (see sections below)</li> <li>Click Save &amp; Test</li> </ol>"},{"location":"grafana/configuration/#connection-settings","title":"Connection Settings","text":""},{"location":"grafana/configuration/#host-configuration","title":"Host Configuration","text":"<p>Host: The address of your FlightSQL server</p> <ul> <li>Format: <code>hostname:port</code></li> <li>Example: <code>localhost:50051</code></li> <li>Production: <code>analytics.example.com:50051</code></li> </ul> <p>Default Port</p> <p>The default FlightSQL port for Micromegas is 50051.</p>"},{"location":"grafana/configuration/#tlsssl-settings","title":"TLS/SSL Settings","text":"<p>Require TLS/SSL: Enable if your server uses TLS encryption</p> <ul> <li>Development: Usually disabled (localhost connections)</li> <li>Production: Strongly recommended for security</li> </ul> <p>Security Recommendation</p> <p>Always enable TLS/SSL for production deployments to encrypt data in transit.</p>"},{"location":"grafana/configuration/#skip-tls-verification","title":"Skip TLS Verification","text":"<p>Skip TLS Verify: Only for self-signed certificates</p> <p>Use Only for Development</p> <p>Never skip TLS verification in production. Use valid certificates instead.</p>"},{"location":"grafana/configuration/#authentication","title":"Authentication","text":"<p>Choose between two authentication methods:</p> <ul> <li>API Key: Simple authentication with a single credential</li> <li>OAuth 2.0 Client Credentials: Enterprise authentication with identity provider</li> </ul> <p>See the Authentication Guide for detailed setup instructions.</p>"},{"location":"grafana/configuration/#metadata","title":"Metadata","text":"<p>Metadata: Optional key-value pairs sent to the FlightSQL server</p> <p>Common use cases: - Environment identifiers (<code>env: production</code>) - Tenant identifiers (<code>tenant: acme-corp</code>) - Custom headers required by your server</p> <p>Format: Key-value pairs</p> <pre><code>key1: value1\nkey2: value2\n</code></pre> <p>Query Performance Settings</p> <p>Query timeout and caching are configured at the Grafana dashboard or panel level, not in the datasource settings. See the Usage Guide for query optimization tips.</p>"},{"location":"grafana/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"grafana/configuration/#development-setup","title":"Development Setup","text":"<pre><code>Host: localhost:50051\nTLS/SSL: Disabled\nAuth Method: API Key\nAPI Key: dev-key-12345\n</code></pre>"},{"location":"grafana/configuration/#production-setup","title":"Production Setup","text":"<pre><code>Host: analytics.example.com:50051\nTLS/SSL: Enabled\nAuth Method: OAuth 2.0 Client Credentials\nOIDC Issuer: https://accounts.google.com\nClient ID: grafana-prod@project.iam.gserviceaccount.com\nClient Secret: ********\n</code></pre>"},{"location":"grafana/configuration/#with-metadata","title":"With Metadata","text":"<pre><code>Host: analytics.example.com:50051\nTLS/SSL: Enabled\nAuth Method: API Key\nAPI Key: prod-key-67890\nMetadata:\n  environment: production\n  region: us-east-1\n</code></pre>"},{"location":"grafana/configuration/#testing-configuration","title":"Testing Configuration","text":"<p>After configuration, click Save &amp; Test to verify:</p> <p>\u2705 Success: \"Data source is working\"</p> <ul> <li>Connection successful</li> <li>Authentication valid</li> <li>Server responding</li> </ul> <p>\u274c Error: Check error message for details:</p> <ul> <li>Connection errors \u2192 Verify host and port</li> <li>Authentication errors \u2192 Check credentials</li> <li>TLS errors \u2192 Verify TLS settings</li> </ul>"},{"location":"grafana/configuration/#updating-configuration","title":"Updating Configuration","text":"<p>To update an existing data source:</p> <ol> <li>Navigate to Configuration \u2192 Data Sources</li> <li>Select your Micromegas data source</li> <li>Update settings</li> <li>Click Save &amp; Test</li> </ol> <p>Credential Updates</p> <p>When updating credentials, Grafana may require you to re-enter secure fields (API keys, client secrets).</p>"},{"location":"grafana/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Set up authentication</li> <li>Start querying data</li> </ul>"},{"location":"grafana/installation/","title":"Installation","text":"<p>This guide covers installing the Micromegas Grafana datasource plugin.</p>"},{"location":"grafana/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Grafana: Version 9.0 or later</li> <li>Micromegas Analytics Server: flight-sql-srv running and accessible</li> <li>Authentication Credentials: API key or OAuth 2.0 client credentials</li> </ul>"},{"location":"grafana/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"grafana/installation/#option-1-from-release-recommended","title":"Option 1: From Release (Recommended)","text":"<ol> <li> <p>Download the latest plugin release from GitHub:    <pre><code>wget https://github.com/madesroches/micromegas/releases/download/grafana-vX.Y.Z/micromegas-datasource-X.Y.Z.zip\n</code></pre></p> </li> <li> <p>Extract to your Grafana plugins directory:    <pre><code># Default plugin directory\nunzip micromegas-datasource-X.Y.Z.zip -d /var/lib/grafana/plugins/\n\n# Or custom plugin directory\nunzip micromegas-datasource-X.Y.Z.zip -d /path/to/grafana/plugins/\n</code></pre></p> </li> <li> <p>Set proper permissions:    <pre><code>chown -R grafana:grafana /var/lib/grafana/plugins/micromegas-datasource\n</code></pre></p> </li> <li> <p>Restart Grafana:    <pre><code># Systemd\nsudo systemctl restart grafana-server\n\n# Docker\ndocker restart grafana\n</code></pre></p> </li> <li> <p>Verify installation:</p> </li> <li>Navigate to Configuration \u2192 Plugins in Grafana</li> <li>Search for \"Micromegas\"</li> <li>Plugin should appear in the list</li> </ol>"},{"location":"grafana/installation/#option-2-build-from-source","title":"Option 2: Build from Source","text":"<p>Development Setup</p> <p>This method is recommended for development or if you need to customize the plugin.</p> <ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas/grafana\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>npm install\n# or\nyarn install\n</code></pre></p> </li> <li> <p>Build the plugin:    <pre><code># Production build\nnpm run build\n\n# Development build with watch mode\nnpm run dev\n</code></pre></p> </li> <li> <p>Build backend binaries (Go):    <pre><code>mage -v build\n</code></pre></p> </li> <li> <p>Link plugin to Grafana:    <pre><code># Create symlink in Grafana plugins directory\nln -s $(pwd) /var/lib/grafana/plugins/micromegas-datasource\n</code></pre></p> </li> <li> <p>Restart Grafana (see commands above)</p> </li> </ol> <p>For detailed development instructions, see DEVELOPMENT.md.</p>"},{"location":"grafana/installation/#option-3-docker-with-plugin","title":"Option 3: Docker with Plugin","text":"<p>If running Grafana in Docker, mount the plugin directory:</p> <pre><code># docker-compose.yml\nversion: '3'\nservices:\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./micromegas-datasource:/var/lib/grafana/plugins/micromegas-datasource\n    environment:\n      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=micromegas-datasource\n</code></pre> <p>Unsigned Plugin</p> <p>During development, you may need to allow unsigned plugins with: <pre><code>GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=micromegas-datasource\n</code></pre></p>"},{"location":"grafana/installation/#verify-installation","title":"Verify Installation","text":"<ol> <li>Open Grafana (default: http://localhost:3000)</li> <li>Navigate to Configuration \u2192 Plugins</li> <li>Search for \"Micromegas\"</li> <li>Plugin should be listed as installed</li> </ol>"},{"location":"grafana/installation/#next-steps","title":"Next Steps","text":"<p>Once installed:</p> <ol> <li>Configure the data source</li> <li>Set up authentication</li> <li>Start querying data</li> </ol>"},{"location":"grafana/usage/","title":"Usage Guide","text":"<p>This guide covers using the Micromegas Grafana plugin to query and visualize your telemetry data.</p>"},{"location":"grafana/usage/#quick-start","title":"Quick Start","text":"<p>The Micromegas plugin makes querying simple with automatic filtering and limiting:</p> <ol> <li>Build your query using the Query Builder or write SQL</li> <li>Enable Time Filter checkbox (enabled by default) to automatically filter results to the dashboard time range</li> <li>Enable Auto Limit checkbox (enabled by default) to automatically limit results to the panel's display capacity</li> <li>Run query - results are automatically filtered and limited!</li> </ol> <p>No need to write <code>$__timeFilter()</code> macros or <code>LIMIT</code> clauses in your SQL - the checkboxes handle it automatically.</p>"},{"location":"grafana/usage/#query-builder","title":"Query Builder","text":"<p>The visual query builder helps you construct SQL queries without writing SQL manually.</p>"},{"location":"grafana/usage/#building-a-query","title":"Building a Query","text":"<ol> <li>Select Table:</li> <li>Click the table dropdown</li> <li>Choose from available tables (e.g., <code>log_entries</code>, <code>measures</code>, <code>thread_spans</code>)</li> <li> <p>Tables auto-populate from your schema</p> </li> <li> <p>Select Columns:</p> </li> <li>Click + to add columns</li> <li>Choose from available columns for the selected table</li> <li>Use <code>*</code> to select all columns</li> <li> <p>Type custom column names or expressions</p> </li> <li> <p>Add WHERE Clauses:</p> </li> <li>Click + next to WHERE to add conditions</li> <li>Enter conditions like <code>level = 2</code></li> <li> <p>Multiple conditions combined with AND</p> </li> <li> <p>Preview Query:</p> </li> <li>The SQL query is shown at the bottom</li> <li> <p>Click Edit SQL to switch to raw SQL mode</p> </li> <li> <p>Run Query:</p> </li> <li>Click Run query to execute</li> <li>Results appear in the panel</li> </ol>"},{"location":"grafana/usage/#time-filter-checkbox","title":"Time Filter Checkbox","text":"<p>The Time Filter checkbox (enabled by default) automatically applies the Grafana dashboard time range to your queries. When enabled:</p> <ul> <li>The backend receives the time range from the dashboard time picker</li> <li>No need to manually add time filters in your SQL</li> <li>Works with both Query Builder and Raw SQL modes</li> <li>Time range updates automatically when you change the dashboard time picker</li> </ul> <p>Example: To query error logs within the dashboard's selected time range:</p> <ol> <li>Select table: <code>log_entries</code></li> <li>Select columns: <code>time</code>, <code>msg</code>, <code>level</code>, <code>exe</code></li> <li>Add WHERE clause: <code>level = 2</code></li> <li>Ensure Time Filter checkbox is checked (default)</li> <li>Run query</li> </ol> <p>The backend automatically filters results to the dashboard time range - no SQL time filter needed!</p> <p>Note: To disable automatic time filtering (e.g., to query all historical data), uncheck the Time Filter checkbox.</p>"},{"location":"grafana/usage/#auto-limit-checkbox","title":"Auto Limit Checkbox","text":"<p>The Auto Limit checkbox (enabled by default) automatically limits query results to match the panel's display capacity. When enabled:</p> <ul> <li>The backend receives the optimal number of data points based on panel width</li> <li>Prevents overwhelming Grafana with excessive data</li> <li>No need to manually add LIMIT clauses in most cases</li> <li>Limits are dynamically adjusted when panel is resized</li> </ul> <p>Example: A panel that is 1000 pixels wide might have <code>maxDataPoints</code> of 1000, automatically limiting results to 1000 rows.</p> <p>When to disable:</p> <ul> <li>When you need specific result counts (e.g., \"show all errors\")</li> <li>When using aggregation queries that already return limited results</li> <li>When you want to add explicit LIMIT clauses in your SQL</li> </ul> <p>Note: The Auto Limit applies to the number of rows returned, while the Time Filter applies to the time range. They work together - Time Filter narrows the time window, and Auto Limit caps the number of results.</p>"},{"location":"grafana/usage/#raw-sql-mode","title":"Raw SQL Mode","text":"<p>For advanced queries, switch to raw SQL mode:</p> <ol> <li>Click Edit SQL button</li> <li>Write your SQL query</li> <li>Ensure Time Filter checkbox is checked (default)</li> <li>Click Run query</li> </ol> <p>The dashboard time range is automatically applied - no need for time filter macros in your SQL!</p>"},{"location":"grafana/usage/#sql-examples","title":"SQL Examples","text":""},{"location":"grafana/usage/#time-series-query","title":"Time-Series Query","text":"<pre><code>SELECT\n  time_bucket('1 minute', time) AS time,\n  process_name,\n  COUNT(*) as event_count\nFROM log_entries\nWHERE level = 2\nGROUP BY 1, 2\nORDER BY 1\n</code></pre> <p>Time range is automatically applied when Time Filter is checked.</p>"},{"location":"grafana/usage/#filtering-by-process","title":"Filtering by Process","text":"<pre><code>SELECT\n  time,\n  msg,\n  level\nFROM log_entries\nWHERE exe = 'api-server'\nORDER BY time DESC\nLIMIT 100\n</code></pre>"},{"location":"grafana/usage/#aggregating-metrics","title":"Aggregating Metrics","text":"<pre><code>SELECT\n  time_bucket('5 minutes', time) AS time,\n  metric_name,\n  AVG(value) as avg_value,\n  MAX(value) as max_value\nFROM measures\nWHERE metric_name LIKE 'cpu.%'\nGROUP BY 1, 2\nORDER BY 1\n</code></pre>"},{"location":"grafana/usage/#grafana-variables","title":"Grafana Variables","text":"<p>Use Grafana variables for dynamic queries.</p>"},{"location":"grafana/usage/#dashboard-variables","title":"Dashboard Variables","text":"<p>Create variables in Dashboard Settings \u2192 Variables:</p> <p>Variable: <code>process</code> <pre><code>SELECT DISTINCT exe FROM processes\n</code></pre></p> <p>Use in query: <pre><code>SELECT time, msg\nFROM log_entries\nWHERE exe = '$process'\n</code></pre></p> <p>Variable: <code>level</code> <pre><code>-- Custom values\n1 : Fatal\n2 : Error\n3 : Warn\n4 : Info\n5 : Debug\n</code></pre></p> <p>Use in query: <pre><code>SELECT time, msg, level\nFROM log_entries\nWHERE level = $level\n</code></pre></p>"},{"location":"grafana/usage/#multi-select-variables","title":"Multi-Select Variables","text":"<p>Enable multi-select in variable settings:</p> <p>Variable: <code>processes</code> (multi-select enabled) <pre><code>SELECT DISTINCT exe FROM processes\n</code></pre></p> <p>Use with IN clause: <pre><code>SELECT time, msg\nFROM log_entries\nWHERE exe IN ($processes)\n</code></pre></p> <p>Time filtering is automatically applied when the Time Filter checkbox is enabled.</p>"},{"location":"grafana/usage/#query-performance-tips","title":"Query Performance Tips","text":""},{"location":"grafana/usage/#use-time-filters","title":"Use Time Filters","text":"<p>Always enable the Time Filter checkbox to limit data scanned:</p> <ul> <li>\u2705 Good: Time Filter checkbox enabled (default)</li> <li>Automatically limits query to dashboard time range</li> <li> <p>Reduces data scanned and improves performance</p> </li> <li> <p>\u274c Bad: Time Filter checkbox disabled</p> </li> <li>Scans entire table regardless of dashboard time range</li> <li>Can be slow for large datasets</li> </ul>"},{"location":"grafana/usage/#limit-result-size","title":"Limit Result Size","text":"<p>The Auto Limit checkbox (enabled by default) automatically limits results based on panel width. For custom limits, you can:</p> <p>Option 1: Use Auto Limit (Recommended) - Enable Auto Limit checkbox - Let Grafana automatically determine optimal limit - No SQL changes needed</p> <p>Option 2: Explicit LIMIT clause - Disable Auto Limit checkbox - Add your own LIMIT to the SQL:</p> <pre><code>SELECT * FROM log_entries\nORDER BY time DESC\nLIMIT 1000  -- Custom limit\n</code></pre> <p>Best Practice: Keep Auto Limit enabled for most queries. Only use explicit LIMIT when you need a specific number of results regardless of panel size.</p>"},{"location":"grafana/usage/#use-pre-aggregated-views","title":"Use Pre-Aggregated Views","text":"<p>For best performance, use pre-aggregated materialized views instead of aggregating raw data:</p> <pre><code>-- \u2705 Best: Query pre-aggregated view\n-- Fast - minimal data scanning\nSELECT\n  time_bin as time,\n  SUM(CASE WHEN level &lt;= 2 THEN count ELSE 0 END) as error_count\nFROM log_stats\nGROUP BY time_bin\nORDER BY time_bin\n\n-- \u26a0\ufe0f Acceptable: Aggregate raw data\n-- Slow - scans all matching rows\nSELECT\n  time_bucket('1 minute', time) AS time,\n  COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2\nGROUP BY 1\nORDER BY 1\n\n-- \u274c Bad: Raw data without aggregation\n-- Very slow - scans and transfers large result set\nSELECT time, msg FROM log_entries WHERE level &lt;= 2\n</code></pre> <p>Available Pre-Aggregated Views:</p> <ul> <li><code>log_stats</code> - Pre-aggregated log counts by minute, process, level, and target</li> <li>Much faster than aggregating <code>log_entries</code></li> <li>Updated automatically as new data arrives</li> <li>Daily partitioned for efficient storage</li> </ul> <p>Why Pre-Aggregated Views Are Faster:</p> <p>The bottleneck in queries is data scanning, not data transfer. Aggregating raw data requires scanning millions of rows from object storage, even when the final result is small. Pre-aggregated views store the computed results, so queries scan far fewer rows.</p> <p>Example: Counting errors over 24 hours</p> <ul> <li><code>log_entries</code>: Scan 100 million log rows \u2192 aggregate \u2192 return 1,440 data points (1 per minute)</li> <li><code>log_stats</code>: Scan 1,440 pre-aggregated rows \u2192 return 1,440 data points</li> </ul> <p>Both return the same amount of data, but <code>log_stats</code> is 100,000x faster because it scans 100,000x less data.</p> <p>Recommendation: Use <code>log_stats</code> for log volume analysis and trend monitoring. For other frequently-used aggregation queries, ask your administrator to create custom materialized views. See Admin Guide - Materialized Views for setup details.</p>"},{"location":"grafana/usage/#advanced-manual-time-filter-macros","title":"Advanced: Manual Time Filter Macros","text":"<p>For advanced use cases where you need explicit control over time filtering in your SQL, you can disable the Time Filter checkbox and use macros directly in your queries.</p>"},{"location":"grafana/usage/#when-to-use-macros-instead-of-the-checkbox","title":"When to Use Macros Instead of the Checkbox","text":"<ul> <li>Complex time logic: When you need multiple different time ranges in a single query</li> <li>Explicit SQL requirements: When you need the time filter visible in the SQL for documentation</li> <li>Custom time ranges: When you need time ranges different from the dashboard picker</li> </ul>"},{"location":"grafana/usage/#available-time-macros","title":"Available Time Macros","text":"<p><code>$__timeFilter(columnName)</code> - Adds a time range condition: <pre><code>SELECT time, msg\nFROM log_entries\nWHERE $__timeFilter(time)\n</code></pre></p> <p>Expands to: <pre><code>WHERE time &gt;= '2025-10-30 10:00:00' AND time &lt;= '2025-10-30 11:00:00'\n</code></pre></p> <p><code>$__timeFrom()</code> - Start of time range: <pre><code>SELECT COUNT(*) FROM log_entries\nWHERE time &gt;= $__timeFrom()\n</code></pre></p> <p><code>$__timeTo()</code> - End of time range: <pre><code>SELECT COUNT(*) FROM log_entries\nWHERE time &lt;= $__timeTo()\n</code></pre></p>"},{"location":"grafana/usage/#checkbox-vs-macros-comparison","title":"Checkbox vs. Macros Comparison","text":"Feature Time Filter Checkbox Manual Macros Ease of use \u2705 Simple, automatic \u274c Requires SQL knowledge Default behavior \u2705 Enabled by default \u274c Must write manually SQL visibility \u274c Hidden in metadata \u2705 Visible in SQL Multiple time ranges \u274c Single range \u2705 Multiple ranges possible Recommended for Most users Advanced users"},{"location":"grafana/usage/#next-steps","title":"Next Steps","text":"<ul> <li>Schema Reference - Available tables and columns</li> <li>Query Patterns - More query examples</li> <li>Functions Reference - SQL functions</li> </ul>"},{"location":"query-guide/","title":"Query Guide Overview","text":"<p>Micromegas provides a powerful SQL interface for querying observability data including logs, metrics, spans, and traces. Micromegas SQL is an extension of Apache DataFusion SQL - you can use all standard DataFusion SQL features plus Micromegas-specific functions and views optimized for observability workloads.</p>"},{"location":"query-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"query-guide/#sql-engine","title":"SQL Engine","text":"<p>Micromegas uses Apache DataFusion as its SQL engine, which means you get:</p> <ul> <li>Full SQL standard compliance</li> <li>Advanced query optimization</li> <li>Vectorized execution engine</li> <li>Columnar data processing with Apache Arrow</li> </ul>"},{"location":"query-guide/#data-architecture","title":"Data Architecture","text":"<ul> <li>Raw data stored in object storage (S3/GCS) in Parquet format</li> <li>Metadata stored in PostgreSQL for fast lookups</li> <li>Views provide logical organization of telemetry data</li> <li>On-demand ETL processes data only when queried</li> </ul>"},{"location":"query-guide/#available-interfaces","title":"Available Interfaces","text":""},{"location":"query-guide/#python-api","title":"Python API","text":"<p>The primary interface for querying Micromegas data programmatically. All queries return pandas DataFrames, making it easy to work with results using the pandas ecosystem:</p> <pre><code>import micromegas\nclient = micromegas.connect()\ndf = client.query(\"SELECT * FROM log_entries LIMIT 10;\")\n</code></pre>"},{"location":"query-guide/#grafana-plugin","title":"Grafana Plugin","text":"<p>Use the same SQL capabilities in Grafana dashboards through the Micromegas Grafana plugin.</p>"},{"location":"query-guide/#data-views","title":"Data Views","text":"<p>Micromegas organizes telemetry data into several queryable views:</p> View Description <code>processes</code> Process metadata and system information <code>streams</code> Data stream information within processes <code>log_entries</code> Application log messages with levels and context <code>measures</code> Numeric metrics and performance measurements <code>thread_spans</code> Synchronous execution spans and timing <code>async_events</code> Asynchronous event lifecycle tracking"},{"location":"query-guide/#query-capabilities","title":"Query Capabilities","text":""},{"location":"query-guide/#standard-sql-features","title":"Standard SQL Features","text":"<ul> <li>SELECT, FROM, WHERE, ORDER BY, GROUP BY</li> <li>JOINs between views</li> <li>Aggregation functions (COUNT, SUM, AVG, etc.)</li> <li>Window functions and CTEs</li> <li>Complex filtering and sorting</li> </ul>"},{"location":"query-guide/#observability-extensions","title":"Observability Extensions","text":"<ul> <li>Time-range filtering for performance</li> <li>Process-scoped view instances</li> <li>Histogram generation functions</li> <li>Log level filtering and analysis</li> <li>Span relationship queries</li> </ul>"},{"location":"query-guide/#performance-features","title":"Performance Features","text":"<ul> <li>Query streaming for large datasets</li> <li>Predicate pushdown to storage layer</li> <li>Automatic view materialization</li> <li>Memory-efficient processing</li> </ul>"},{"location":"query-guide/#getting-started","title":"Getting Started","text":"<ol> <li>Quick Start - Basic queries to get you started</li> <li>Python API - Complete API reference and examples</li> <li>Schema Reference - Detailed view and field documentation</li> <li>Functions Reference - Available SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Async Performance Analysis - Comprehensive async operation analysis with depth tracking</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Advanced Features - View materialization and custom views</li> </ol>"},{"location":"query-guide/#best-practices","title":"Best Practices","text":""},{"location":"query-guide/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<p>For performance and memory efficiency, always specify time ranges in your queries:</p> <pre><code># Good - uses time range\ndf = client.query(sql, begin_time, end_time)\n\n# Avoid - queries all data\ndf = client.query(sql)  # Can be slow and memory-intensive\n</code></pre>"},{"location":"query-guide/#start-simple","title":"Start Simple","text":"<p>Begin with basic queries and add complexity incrementally:</p> <pre><code>-- Start with this\nSELECT * FROM log_entries LIMIT 10;\n\n-- Then add filtering\nSELECT * FROM log_entries WHERE level &lt;= 3 LIMIT 10;\n\n-- Then add time range\nSELECT * FROM log_entries\nWHERE level &lt;= 3 AND time &gt;= NOW() - INTERVAL '1 hour'\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/#use-process-scoped-views","title":"Use Process-Scoped Views","text":"<p>For better performance when analyzing specific processes:</p> <pre><code>-- Instead of filtering the global view\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n\n-- Use a process-scoped view instance\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre> <p>Ready to start querying? Head to the Quick Start guide!</p>"},{"location":"query-guide/advanced-features/","title":"Advanced Features","text":"<p>Advanced Micromegas features including view materialization, custom views, and system administration.</p>"},{"location":"query-guide/advanced-features/#view-materialization","title":"View Materialization","text":"<p>Micromegas uses a lakehouse architecture with on-demand view materialization for optimal performance.</p>"},{"location":"query-guide/advanced-features/#jit-view-processing","title":"JIT View Processing","text":"<ul> <li>Raw data stored in object storage (S3/GCS)</li> <li>Views materialized on-demand when queried</li> <li>Automatic caching for frequently accessed data</li> </ul>"},{"location":"query-guide/advanced-features/#global-views-vs-view-instances","title":"Global Views vs View Instances","text":"<p>Micromegas provides two ways to access telemetry data:</p>"},{"location":"query-guide/advanced-features/#global-views-implicit","title":"Global Views (Implicit)","text":"<p>When you query views directly by name, you're using global views that span all processes:</p> <pre><code>-- Global view - queries data from ALL processes\nSELECT * FROM log_entries WHERE level &lt;= 2;\nSELECT * FROM measures WHERE name = 'cpu_usage';\n</code></pre> <p>Global views are convenient for: - Exploring data across the entire system - Cross-process analysis and correlation - Getting started without knowing specific process IDs</p>"},{"location":"query-guide/advanced-features/#view-instances-explicit","title":"View Instances (Explicit)","text":"<p>Use the <code>view_instance()</code> function to create process-scoped views for better performance:</p> <pre><code>-- View instance - queries data from ONE specific process\nSELECT * FROM view_instance('log_entries', 'my_process_123') WHERE level &lt;= 2;\nSELECT * FROM view_instance('measures', 'my_process_123') WHERE name = 'cpu_usage';\n</code></pre> <p>View instances are optimal for: - Analyzing specific processes or streams - Better query performance (fewer partitions to scan) - Production systems with large amounts of data</p> <p>Performance Impact: - Global views: May scan many partitions across all processes - View instances: Only scan partitions for the specified process/stream</p>"},{"location":"query-guide/advanced-features/#architecture-benefits","title":"Architecture Benefits","text":""},{"location":"query-guide/advanced-features/#datalake-lakehouse-query","title":"Datalake \u2192 Lakehouse \u2192 Query","text":"<ul> <li>Datalake (S3): Custom binary format, cheap storage, fast writes</li> <li>Lakehouse (Parquet): Columnar format, fast analytics, industry standard</li> <li>Query Engine (DataFusion): SQL engine optimized for analytical workloads</li> </ul>"},{"location":"query-guide/advanced-features/#tail-sampling-support","title":"Tail Sampling Support","text":"<ul> <li>Heavy data streams remain unprocessed until queried</li> <li>Cheap to store in S3, cheap to delete unused data</li> <li>Use low-frequency streams (logs, metrics) to decide sampling of high-frequency streams (spans)</li> </ul>"},{"location":"query-guide/async-performance-analysis/","title":"Async Performance Analysis Guide","text":"<p>This guide provides comprehensive patterns and examples for analyzing asynchronous operation performance using the <code>async_events</code> view with depth tracking.</p>"},{"location":"query-guide/async-performance-analysis/#understanding-async-event-depth","title":"Understanding Async Event Depth","text":"<p>The <code>depth</code> field in <code>async_events</code> represents the nesting level in the async call hierarchy:</p> <ul> <li>Depth 0: Top-level async operations (entry points)</li> <li>Depth 1: First-level nested async operations</li> <li>Depth 2+: Deeper nested async operations</li> </ul> <p>This enables hierarchical performance analysis similar to synchronous call stack profiling.</p>"},{"location":"query-guide/async-performance-analysis/#core-analysis-patterns","title":"Core Analysis Patterns","text":""},{"location":"query-guide/async-performance-analysis/#1-top-level-performance-overview","title":"1. Top-Level Performance Overview","text":"<p>Start with top-level operations (depth = 0) to identify primary performance bottlenecks:</p> <pre><code>-- Top-level async operations with performance metrics\nSELECT\n    name,\n    COUNT(*) as operation_count,\n    AVG(duration_ms) as avg_duration,\n    MIN(duration_ms) as min_duration,\n    MAX(duration_ms) as max_duration,\n    STDDEV(duration_ms) as duration_stddev\nFROM (\n    SELECT\n        begin_events.name,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id')\n         WHERE event_type = 'begin' AND depth = 0) begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id')\n         WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY name\nORDER BY avg_duration DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#2-depth-based-performance-comparison","title":"2. Depth-Based Performance Comparison","text":"<p>Compare performance characteristics across different call depths:</p> <pre><code>-- Performance metrics by async call depth\nSELECT\n    depth,\n    COUNT(*) as span_count,\n    AVG(duration_ms) as avg_duration,\n    PERCENTILE(duration_ms, 0.5) as median_duration,\n    PERCENTILE(duration_ms, 0.95) as p95_duration,\n    PERCENTILE(duration_ms, 0.99) as p99_duration\nFROM (\n    SELECT\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth\nORDER BY depth;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#3-parent-child-performance-analysis","title":"3. Parent-Child Performance Analysis","text":"<p>Analyze how async operations delegate work to nested operations:</p> <pre><code>-- Parent-child async operation performance relationships\nSELECT\n    parent.name as parent_operation,\n    parent.depth as parent_depth,\n    child.name as child_operation,\n    child.depth as child_depth,\n    COUNT(*) as relationship_count,\n    AVG(parent_duration_ms) as avg_parent_duration,\n    AVG(child_duration_ms) as avg_child_duration,\n    AVG(parent_duration_ms) - AVG(child_duration_ms) as avg_overhead_ms\nFROM (\n    SELECT\n        p.name, p.depth, p.span_id,\n        c.name as child_name, c.depth as child_depth, c.span_id as child_span_id,\n        CAST((p_end.time - p_begin.time) AS BIGINT) / 1000000 as parent_duration_ms,\n        CAST((c_end.time - c_begin.time) AS BIGINT) / 1000000 as child_duration_ms\n    FROM view_instance('async_events', 'process_id') p\n    JOIN view_instance('async_events', 'process_id') c ON p.span_id = c.parent_span_id\n    JOIN view_instance('async_events', 'process_id') p_begin ON p.span_id = p_begin.span_id AND p_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'process_id') p_end ON p.span_id = p_end.span_id AND p_end.event_type = 'end'\n    JOIN view_instance('async_events', 'process_id') c_begin ON c.span_id = c_begin.span_id AND c_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'process_id') c_end ON c.span_id = c_end.span_id AND c_end.event_type = 'end'\n    WHERE p.event_type = 'begin' AND c.event_type = 'begin'\n) as relationships(name, depth, span_id, child_name, child_depth, child_span_id, parent_duration_ms, child_duration_ms)\nGROUP BY parent_operation, parent_depth, child_operation, child_depth\nHAVING COUNT(*) &gt; 5  -- Focus on significant relationships\nORDER BY relationship_count DESC, avg_overhead_ms DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#advanced-analysis-techniques","title":"Advanced Analysis Techniques","text":""},{"location":"query-guide/async-performance-analysis/#4-async-concurrency-analysis","title":"4. Async Concurrency Analysis","text":"<p>Identify periods of high async concurrency:</p> <pre><code>-- Concurrent async operations over time\nSELECT\n    time_bucket,\n    MAX(concurrent_operations) as peak_concurrency,\n    AVG(concurrent_operations) as avg_concurrency\nFROM (\n    SELECT\n        date_trunc('minute', time) as time_bucket,\n        COUNT(*) as concurrent_operations\n    FROM view_instance('async_events', 'process_id')\n    WHERE event_type = 'begin'\n    GROUP BY date_trunc('minute', time)\n)\nGROUP BY time_bucket\nORDER BY time_bucket;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#5-deep-nesting-detection","title":"5. Deep Nesting Detection","text":"<p>Find problematic deep async call chains:</p> <pre><code>-- Operations with excessive async nesting depth\nSELECT\n    name,\n    depth,\n    COUNT(*) as occurrence_count,\n    AVG(duration_ms) as avg_duration\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nWHERE depth &gt;= 3  -- Focus on deep nesting\nGROUP BY name, depth\nORDER BY depth DESC, occurrence_count DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#6-async-operation-hotspots","title":"6. Async Operation Hotspots","text":"<p>Identify the most frequently called async operations by depth:</p> <pre><code>-- Async operation frequency by depth level\nSELECT\n    depth,\n    name,\n    COUNT(*) as call_count,\n    AVG(duration_ms) as avg_duration,\n    COUNT(*) * AVG(duration_ms) as total_time_spent\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth, name\nORDER BY total_time_spent DESC;\n</code></pre>"},{"location":"query-guide/async-performance-analysis/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"query-guide/async-performance-analysis/#focus-areas-based-on-depth-analysis","title":"Focus Areas Based on Depth Analysis","text":"<ol> <li>Depth 0 Optimization: Target top-level operations for maximum impact</li> <li>High-Frequency Operations: Optimize operations with high call counts</li> <li>Deep Nesting Reduction: Flatten async call hierarchies where possible</li> <li>Concurrency Tuning: Balance async concurrency with resource usage</li> </ol>"},{"location":"query-guide/async-performance-analysis/#query-performance-tips","title":"Query Performance Tips","text":"<ol> <li>Always use time ranges through Python API parameters</li> <li>Filter by depth early to reduce data processing</li> <li>Use process-scoped views (<code>view_instance</code>) for efficiency</li> <li>Combine with other views (logs, measures) for context</li> </ol>"},{"location":"query-guide/async-performance-analysis/#example-comprehensive-async-performance-dashboard","title":"Example: Comprehensive Async Performance Dashboard","text":"<pre><code>-- Multi-dimensional async performance summary\nWITH async_durations AS (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        begin_events.time as start_time,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'process_id') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n),\ndepth_summary AS (\n    SELECT\n        depth,\n        COUNT(*) as operation_count,\n        AVG(duration_ms) as avg_duration,\n        PERCENTILE(duration_ms, 0.95) as p95_duration\n    FROM async_durations\n    GROUP BY depth\n),\ntop_operations AS (\n    SELECT\n        name,\n        COUNT(*) as call_count,\n        AVG(duration_ms) as avg_duration\n    FROM async_durations\n    WHERE depth = 0  -- Top-level only\n    GROUP BY name\n    ORDER BY avg_duration DESC\n    LIMIT 5\n)\nSELECT\n    'Depth Summary' as analysis_type,\n    CAST(depth AS VARCHAR) as name,\n    operation_count as count,\n    avg_duration,\n    p95_duration as p95\nFROM depth_summary\nUNION ALL\nSELECT\n    'Top Operations' as analysis_type,\n    name,\n    call_count as count,\n    avg_duration,\n    NULL as p95\nFROM top_operations\nORDER BY analysis_type, avg_duration DESC;\n</code></pre> <p>This comprehensive approach enables effective async performance analysis and optimization based on call hierarchy depth information.</p>"},{"location":"query-guide/functions-reference/","title":"Functions Reference","text":"<p>This page provides a complete reference to all SQL functions available in Micromegas queries, including both standard DataFusion functions and Micromegas-specific extensions.</p>"},{"location":"query-guide/functions-reference/#micromegas-extensions","title":"Micromegas Extensions","text":""},{"location":"query-guide/functions-reference/#table-functions","title":"Table Functions","text":"<p>Table functions return tables that can be used in FROM clauses.</p>"},{"location":"query-guide/functions-reference/#view_instanceview_name-identifier","title":"<code>view_instance(view_name, identifier)</code>","text":"<p>Creates a process or stream-scoped view instance for better performance.</p> <p>Syntax: <pre><code>view_instance(view_name, identifier)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_name</code> (<code>Utf8</code>): Name of the view ('log_entries', 'measures', 'thread_spans', 'async_events')</p> </li> <li> <p><code>identifier</code> (<code>Utf8</code>): Process ID (for most views) or Stream ID (for thread_spans)</p> </li> </ul> <p>Returns: Schema depends on the view type (see Schema Reference)</p> <p>Examples: <pre><code>-- Get logs for a specific process\nSELECT time, level, msg\nFROM view_instance('log_entries', 'my_process_123')\nWHERE level &lt;= 3;\n\n-- Get spans for a specific stream\nSELECT name, duration\nFROM view_instance('thread_spans', 'stream_456')\nWHERE duration &gt; 1000000;  -- &gt; 1ms\n</code></pre></p>"},{"location":"query-guide/functions-reference/#list_partitions","title":"<code>list_partitions()</code> \ud83d\udd27","text":"<p>Administrative Function - Lists available data partitions in the lakehouse.</p> <p>Syntax: <pre><code>SELECT * FROM list_partitions()\n</code></pre></p> <p>Returns:</p> Column Type Description view_set_name Utf8 Name of the view set view_instance_id Utf8 Instance identifier begin_insert_time Timestamp(Nanosecond) Partition start time end_insert_time Timestamp(Nanosecond) Partition end time min_event_time Timestamp(Nanosecond) Earliest event time max_event_time Timestamp(Nanosecond) Latest event time updated Timestamp(Nanosecond) Last update time file_path Utf8 Partition file path file_size Int64 File size in bytes file_schema_hash Binary Hash of the file schema source_data_hash Binary Hash of the source data <p>Example: <pre><code>-- View partition information\nSELECT view_set_name, view_instance_id, file_size\nFROM list_partitions()\nORDER BY updated DESC;\n</code></pre></p> <p>\u2139\ufe0f Administrative Use: This function provides system-level partition metadata primarily useful for administrators monitoring lakehouse storage and partition management. Regular users querying data typically don't need this information.</p>"},{"location":"query-guide/functions-reference/#retire_partitionsview_set_name-view_instance_id-begin_insert_time-end_insert_time","title":"<code>retire_partitions(view_set_name, view_instance_id, begin_insert_time, end_insert_time)</code> \ud83d\udd27","text":"<p>Administrative Function - Retires (removes) data partitions from the lakehouse for a specified time range. Returns a log stream of the operation.</p> <p>Syntax: <pre><code>SELECT * FROM retire_partitions(view_set_name, view_instance_id, begin_insert_time, end_insert_time)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_set_name</code> (<code>Utf8</code>): Name of the view set</p> </li> <li> <p><code>view_instance_id</code> (<code>Utf8</code>): Instance identifier</p> </li> <li> <p><code>begin_insert_time</code> (<code>Timestamp(Nanosecond)</code>): Start time for partition retirement</p> </li> <li> <p><code>end_insert_time</code> (<code>Timestamp(Nanosecond)</code>): End time for partition retirement</p> </li> </ul> <p>Returns: Log stream table with operation progress and messages</p> <p>Example: <pre><code>-- Retire old partitions for a specific view\nSELECT * FROM retire_partitions(\n    'log_entries', \n    'global',\n    NOW() - INTERVAL '30 days',\n    NOW() - INTERVAL '7 days'\n);\n</code></pre></p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This function permanently removes data partitions from the lakehouse, making the contained data inaccessible. Use only for data retention management and with extreme caution in production environments. Ensure proper backups exist before retiring partitions.</p>"},{"location":"query-guide/functions-reference/#materialize_partitionsview_name-begin_insert_time-end_insert_time-partition_delta_seconds","title":"<code>materialize_partitions(view_name, begin_insert_time, end_insert_time, partition_delta_seconds)</code> \ud83d\udd27","text":"<p>Administrative Function - Materializes data partitions for a view over a specified time range. Returns a log stream of the operation.</p> <p>Syntax: <pre><code>SELECT * FROM materialize_partitions(view_name, begin_insert_time, end_insert_time, partition_delta_seconds)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>view_name</code> (<code>Utf8</code>): Name of the view to materialize</p> </li> <li> <p><code>begin_insert_time</code> (<code>Timestamp(Nanosecond)</code>): Start time for materialization</p> </li> <li> <p><code>end_insert_time</code> (<code>Timestamp(Nanosecond)</code>): End time for materialization</p> </li> <li> <p><code>partition_delta_seconds</code> (<code>Int64</code>): Partition time delta in seconds</p> </li> </ul> <p>Returns: Log stream table with operation progress and messages</p> <p>Example: <pre><code>-- Materialize partitions for CPU usage view\nSELECT * FROM materialize_partitions(\n    'cpu_usage_per_process_per_minute',\n    NOW() - INTERVAL '1 day',\n    NOW(),\n    3600  -- 1 hour partitions\n);\n</code></pre></p> <p>\u26a0\ufe0f Administrative Use Only: This function is intended for system administrators and data engineers managing the lakehouse infrastructure. Regular users querying data should not need to call this function. It triggers background processing to create materialized partitions and can impact system performance.</p>"},{"location":"query-guide/functions-reference/#list_view_sets","title":"<code>list_view_sets()</code> \ud83d\udd27","text":"<p>Administrative Function - Lists all available view sets with their current schema information. Useful for schema discovery and management.</p> <p>Syntax: <pre><code>SELECT * FROM list_view_sets()\n</code></pre></p> <p>Returns:</p> Column Type Description view_set_name Utf8 Name of the view set (e.g., 'log_entries', 'measures') current_schema_hash Binary Current schema version identifier schema Utf8 Full schema as formatted string has_view_maker Boolean Whether view set supports process-specific instances global_instance_available Boolean Whether a global instance exists <p>Example: <pre><code>-- View all available view sets and their schemas\nSELECT view_set_name, current_schema_hash, has_view_maker\nFROM list_view_sets()\nORDER BY view_set_name;\n\n-- Check schema for specific view set\nSELECT schema\nFROM list_view_sets()\nWHERE view_set_name = 'log_entries';\n</code></pre></p> <p>\u2139\ufe0f Administrative Use: This function provides schema discovery for administrators managing view compatibility and schema evolution. It shows the current schema versions and capabilities of each view set in the lakehouse.</p>"},{"location":"query-guide/functions-reference/#retire_partition_by_filefile_path","title":"<code>retire_partition_by_file(file_path)</code> \ud83d\udd27","text":"<p>Administrative Function - Retires a single partition by its exact file path. Provides targeted partition removal for schema evolution and maintenance.</p> <p>Syntax: <pre><code>SELECT retire_partition_by_file(file_path) as result\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>file_path</code> (<code>Utf8</code>): Exact file path of the partition to retire</li> </ul> <p>Returns: <code>Utf8</code> - Result message indicating success or failure</p> <p>Example: <pre><code>-- Retire a specific partition\nSELECT retire_partition_by_file('/lakehouse/log_entries/process-123/2024/01/01/partition.parquet') as result;\n\n-- Retire multiple partitions (use with list_partitions())\nSELECT retire_partition_by_file(file_path) as result\nFROM list_partitions()\nWHERE view_set_name = 'log_entries' \n  AND file_schema_hash != '[4]'  -- Retire old schema versions\nLIMIT 10;\n</code></pre></p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This function permanently removes a single data partition from the lakehouse, making the contained data inaccessible. Unlike <code>retire_partitions()</code> which operates on time ranges, this function targets exact file paths for precise partition management. Ensure proper backups exist before retiring partitions.</p> <p>\u2705 Safety Note: This function only affects the specified partition file. It cannot accidentally retire other partitions, making it safer than time-range-based retirement for schema evolution tasks.</p>"},{"location":"query-guide/functions-reference/#scalar-functions","title":"Scalar Functions","text":""},{"location":"query-guide/functions-reference/#jsonjsonb-functions","title":"JSON/JSONB Functions","text":"<p>Micromegas provides functions for working with JSON data stored in binary JSONB format for efficient storage and querying.</p>"},{"location":"query-guide/functions-reference/#jsonb_parsejson_string","title":"<code>jsonb_parse(json_string)</code>","text":"<p>Parses a JSON string into binary JSONB format.</p> <p>Syntax: <pre><code>jsonb_parse(json_string)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>json_string</code> (<code>Utf8</code>): JSON string to parse</li> </ul> <p>Returns: <code>Binary</code> - Parsed JSONB data</p> <p>Example: <pre><code>-- Parse JSON string into JSONB\nSELECT jsonb_parse('{\"name\": \"web_server\", \"port\": 8080}') as parsed_json\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_getjsonb-key","title":"<code>jsonb_get(jsonb, key)</code>","text":"<p>Extracts a value from a JSONB object by key name.</p> <p>Syntax: <pre><code>jsonb_get(jsonb, key)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>jsonb</code> (<code>Binary</code>): JSONB object</p> </li> <li> <p><code>key</code> (<code>Utf8</code>): Key name to extract</p> </li> </ul> <p>Returns: <code>Binary</code> - JSONB value or NULL if key not found</p> <p>Example: <pre><code>-- Extract name field from JSON data\nSELECT jsonb_get(jsonb_parse('{\"name\": \"web_server\", \"port\": 8080}'), 'name') as name_value\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_format_jsonjsonb","title":"<code>jsonb_format_json(jsonb)</code>","text":"<p>Converts a JSONB value back to a human-readable JSON string.</p> <p>Syntax: <pre><code>jsonb_format_json(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>jsonb</code> (Multiple formats supported): JSONB value in any of these formats:</p> </li> <li> <p><code>Dictionary&lt;Int32, Binary&gt;</code> - Dictionary-encoded JSONB (default)</p> </li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> <p>Returns: <code>Utf8</code> - JSON string representation</p> <p>Examples: <pre><code>-- Format JSONB back to JSON string\nSELECT jsonb_format_json(jsonb_parse('{\"name\": \"web_server\"}')) as json_string\nFROM processes;\n\n-- Works directly with dictionary-encoded properties\nSELECT jsonb_format_json(properties_to_jsonb(properties)) as json_props\nFROM log_entries;\n\n-- Format property values as JSON\nSELECT jsonb_format_json(properties) as json_string\nFROM processes\nWHERE properties IS NOT NULL;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_stringjsonb","title":"<code>jsonb_as_string(jsonb)</code>","text":"<p>Casts a JSONB value to a string.</p> <p>Syntax: <pre><code>jsonb_as_string(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Utf8</code> - String value or NULL if not a string</p> <p>Example: <pre><code>-- Extract string value from JSONB\nSELECT jsonb_as_string(jsonb_get(jsonb_parse('{\"service\": \"web_server\"}'), 'service')) as service_name\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_f64jsonb","title":"<code>jsonb_as_f64(jsonb)</code>","text":"<p>Casts a JSONB value to a 64-bit float.</p> <p>Syntax: <pre><code>jsonb_as_f64(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Float64</code> - Numeric value or NULL if not a number</p> <p>Example: <pre><code>-- Extract numeric value from JSONB\nSELECT jsonb_as_f64(jsonb_get(jsonb_parse('{\"cpu_usage\": 75.5}'), 'cpu_usage')) as cpu_usage\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#jsonb_as_i64jsonb","title":"<code>jsonb_as_i64(jsonb)</code>","text":"<p>Casts a JSONB value to a 64-bit integer.</p> <p>Syntax: <pre><code>jsonb_as_i64(jsonb)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>jsonb</code> (<code>Binary</code>): JSONB value to convert</li> </ul> <p>Returns: <code>Int64</code> - Integer value or NULL if not an integer</p> <p>Example: <pre><code>-- Extract integer value from JSONB\nSELECT jsonb_as_i64(jsonb_get(jsonb_parse('{\"port\": 8080}'), 'port')) as port_number\nFROM processes;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#data-access-functions","title":"Data Access Functions","text":""},{"location":"query-guide/functions-reference/#get_payloadprocess_id-stream_id-block_id","title":"<code>get_payload(process_id, stream_id, block_id)</code>","text":"<p>Retrieves the raw binary payload of a telemetry block from data lake storage.</p> <p>Syntax: <pre><code>get_payload(process_id, stream_id, block_id)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>process_id</code> (<code>Utf8</code>): Process identifier</p> </li> <li> <p><code>stream_id</code> (<code>Utf8</code>): Stream identifier</p> </li> <li> <p><code>block_id</code> (<code>Utf8</code>): Block identifier</p> </li> </ul> <p>Returns: <code>Binary</code> - Raw block payload data</p> <p>Example: <pre><code>-- Get raw payload data for specific blocks\nSELECT process_id, stream_id, block_id, get_payload(process_id, stream_id, block_id) as payload\nFROM blocks\nWHERE insert_time &gt;= NOW() - INTERVAL '1 hour'\nLIMIT 10;\n</code></pre></p> <p>Note: This is an async function that fetches data from object storage. Use sparingly in queries as it can impact performance.</p>"},{"location":"query-guide/functions-reference/#property-functions","title":"Property Functions","text":"<p>Micromegas provides specialized functions for working with property data, including efficient dictionary encoding for memory optimization.</p>"},{"location":"query-guide/functions-reference/#property_getproperties-key","title":"<code>property_get(properties, key)</code>","text":"<p>Extracts a value from a properties map with automatic format detection and optimized performance for JSONB data.</p> <p>Syntax: <pre><code>property_get(properties, key)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties data in any of these formats:</p> <ul> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format (default, optimized)</li> <li><code>List&lt;Struct&lt;key, value&gt;&gt;</code> - Legacy format (automatic conversion)</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded legacy</li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> </li> <li> <p><code>key</code> (<code>Utf8</code>): Property key to extract</p> </li> </ul> <p>Returns: <code>Dictionary&lt;Int32, Utf8&gt;</code> - Property value or NULL if not found</p> <p>Performance: Optimized for the new JSONB format. Legacy formats are automatically converted for backward compatibility.</p> <p>Examples: <pre><code>-- Get thread name from process properties (works with all formats)\nSELECT time, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n\n-- Filter by custom property\nSELECT time, name, value\nFROM measures\nWHERE property_get(properties, 'source') = 'system_monitor';\n\n-- Direct JSONB property access (post-migration default)\nSELECT time, msg, property_get(properties, 'service') as service\nFROM log_entries\nWHERE property_get(properties, 'env') = 'production';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#properties_lengthproperties","title":"<code>properties_length(properties)</code>","text":"<p>Returns the number of properties in a properties map with support for multiple storage formats.</p> <p>Syntax: <pre><code>properties_length(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties data in any of these formats:</p> <ul> <li><code>List&lt;Struct&lt;key, value&gt;&gt;</code> - Legacy format</li> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format (optimized)</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded legacy</li> <li><code>Binary</code> - Non-dictionary JSONB</li> </ul> </li> </ul> <p>Returns: <code>Int32</code> - Number of properties</p> <p>Examples: <pre><code>-- Works with regular properties\nSELECT properties_length(properties) as prop_count\nFROM measures;\n\n-- Works with dictionary-encoded properties\nSELECT properties_length(properties_to_dict(properties)) as prop_count\nFROM measures;\n\n-- JSONB property counting\nSELECT properties_length(properties_to_jsonb(properties)) as prop_count\nFROM measures;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#properties_to_dictproperties","title":"<code>properties_to_dict(properties)</code>","text":"<p>Converts a properties list to a dictionary-encoded array for memory efficiency.</p> <p>Syntax: <pre><code>properties_to_dict(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>properties</code> (<code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code>): Properties list to encode</li> </ul> <p>Returns: <code>Dictionary&lt;Int32, List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;&gt;</code> - Dictionary-encoded properties</p> <p>Examples: <pre><code>-- Convert properties to dictionary encoding for memory efficiency\nSELECT properties_to_dict(properties) as dict_props\nFROM measures;\n\n-- Use with other functions via properties_to_array\nSELECT array_length(properties_to_array(properties_to_dict(properties))) as prop_count\nFROM measures;\n</code></pre></p> <p>Note: Dictionary encoding can reduce memory usage by 50-80% for datasets with repeated property patterns.</p>"},{"location":"query-guide/functions-reference/#properties_to_jsonbproperties","title":"<code>properties_to_jsonb(properties)</code>","text":"<p>Converts a properties list to binary JSONB format with dictionary encoding for efficient storage and querying.</p> <p>Syntax: <pre><code>properties_to_jsonb(properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>properties</code> (Multiple formats supported): Properties in any of these formats:</p> <ul> <li><code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code> - Regular properties list</li> <li><code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code> - Dictionary-encoded properties</li> <li><code>Binary</code> - Non-dictionary JSONB</li> <li><code>Dictionary&lt;Int32, Binary&gt;</code> - JSONB format</li> </ul> </li> </ul> <p>Returns: <code>Dictionary&lt;Int32, Binary&gt;</code> - Dictionary-encoded JSONB object containing the properties as key-value pairs</p> <p>Examples: <pre><code>-- Convert properties to JSONB format\nSELECT properties_to_jsonb(properties) as jsonb_props\nFROM log_entries;\n\n-- Use with other JSONB functions\nSELECT jsonb_get(properties_to_jsonb(properties), 'hostname') as hostname\nFROM log_entries;\n\n-- Convert dictionary-encoded properties to JSONB\nSELECT properties_to_jsonb(properties_to_dict(properties)) as jsonb_props\nFROM measures;\n</code></pre></p> <p>Note: This function returns <code>Dictionary&lt;Int32, Binary&gt;</code> format for optimal memory usage with Arrow's built-in dictionary encoding.</p>"},{"location":"query-guide/functions-reference/#properties_to_arraydict_properties","title":"<code>properties_to_array(dict_properties)</code>","text":"<p>Converts dictionary-encoded properties back to a regular array for compatibility with standard functions.</p> <p>Syntax: <pre><code>properties_to_array(dict_properties)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>dict_properties</code> (<code>Dictionary&lt;Int32, List&lt;Struct&gt;&gt;</code>): Dictionary-encoded properties</li> </ul> <p>Returns: <code>List&lt;Struct&lt;key: Utf8, value: Utf8&gt;&gt;</code> - Regular properties array</p> <p>Examples: <pre><code>-- Convert dictionary-encoded properties back to array\nSELECT properties_to_array(properties_to_dict(properties)) as props\nFROM measures;\n\n-- Use with array functions\nSELECT array_length(properties_to_array(properties_to_dict(properties))) as count\nFROM measures;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#histogram-functions","title":"Histogram Functions","text":"<p>Micromegas provides a comprehensive set of functions for creating and analyzing histograms, enabling efficient statistical analysis of large datasets.</p>"},{"location":"query-guide/functions-reference/#make_histogramstart-end-bins-values","title":"<code>make_histogram(start, end, bins, values)</code>","text":"<p>Creates histogram data from numeric values with specified range and bin count.</p> <p>Syntax: <pre><code>make_histogram(start, end, bins, values)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>start</code> (<code>Float64</code>): Histogram minimum value</p> </li> <li> <p><code>end</code> (<code>Float64</code>): Histogram maximum value</p> </li> <li> <p><code>bins</code> (<code>Int64</code>): Number of histogram bins</p> </li> <li> <p><code>values</code> (<code>Float64</code>): Column of numeric values to histogram</p> </li> </ul> <p>Returns: Histogram structure with buckets and counts</p> <p>Example: <pre><code>-- Create histogram of response times (0-50ms, 20 bins)\nSELECT make_histogram(0.0, 50.0, 20, CAST(duration AS FLOAT64) / 1000000.0) as duration_histogram\nFROM view_instance('thread_spans', 'web_server_123')\nWHERE name = 'handle_request';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#sum_histogramshistogram_column","title":"<code>sum_histograms(histogram_column)</code>","text":"<p>Aggregates multiple histograms by summing their bins.</p> <p>Syntax: <pre><code>sum_histograms(histogram_column)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram_column</code> (Histogram): Column containing histogram values</li> </ul> <p>Returns: Combined histogram with summed bins</p> <p>Example: <pre><code>-- Combine histograms across processes\nSELECT sum_histograms(duration_histogram) as combined_histogram\nFROM cpu_usage_per_process_per_minute\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#quantile_from_histogramhistogram-quantile","title":"<code>quantile_from_histogram(histogram, quantile)</code>","text":"<p>Estimates a quantile value from a histogram.</p> <p>Syntax: <pre><code>quantile_from_histogram(histogram, quantile)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <p><code>histogram</code> (Histogram): Histogram to analyze</p> </li> <li> <p><code>quantile</code> (<code>Float64</code>): Quantile to estimate (0.0 to 1.0)</p> </li> </ul> <p>Returns: <code>Float64</code> - Estimated quantile value</p> <p>Examples: <pre><code>-- Get median (50th percentile) response time\nSELECT quantile_from_histogram(duration_histogram, 0.5) as median_duration\nFROM performance_histograms;\n\n-- Get 95th percentile response time\nSELECT quantile_from_histogram(duration_histogram, 0.95) as p95_duration\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#variance_from_histogramhistogram","title":"<code>variance_from_histogram(histogram)</code>","text":"<p>Calculates variance from histogram data.</p> <p>Syntax: <pre><code>variance_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>Float64</code> - Variance of the histogram data</p> <p>Example: <pre><code>-- Calculate response time variance\nSELECT variance_from_histogram(duration_histogram) as duration_variance\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#count_from_histogramhistogram","title":"<code>count_from_histogram(histogram)</code>","text":"<p>Extracts the total count of values from a histogram.</p> <p>Syntax: <pre><code>count_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>UInt64</code> - Total number of values in the histogram</p> <p>Example: <pre><code>-- Get total sample count from histogram\nSELECT count_from_histogram(duration_histogram) as total_samples\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#sum_from_histogramhistogram","title":"<code>sum_from_histogram(histogram)</code>","text":"<p>Extracts the sum of all values from a histogram.</p> <p>Syntax: <pre><code>sum_from_histogram(histogram)\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>histogram</code> (Histogram): Histogram to analyze</li> </ul> <p>Returns: <code>Float64</code> - Sum of all values in the histogram</p> <p>Example: <pre><code>-- Get total duration from histogram\nSELECT sum_from_histogram(duration_histogram) as total_duration\nFROM performance_histograms;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#standard-sql-functions","title":"Standard SQL Functions","text":"<p>Micromegas supports all standard DataFusion SQL functions including math, string, date/time, conditional, and array functions. For a complete list with examples, see the DataFusion Scalar Functions documentation.</p>"},{"location":"query-guide/functions-reference/#advanced-query-patterns","title":"Advanced Query Patterns","text":""},{"location":"query-guide/functions-reference/#histogram-analysis","title":"Histogram Analysis","text":"<pre><code>-- Create performance histogram (0-100ms, 10 bins)\nSELECT make_histogram(0.0, 100.0, 10, duration / 1000000.0) as response_time_ms_histogram\nFROM view_instance('thread_spans', 'web_server')\nWHERE name = 'handle_request'\n  AND duration &gt; 1000000;  -- &gt; 1ms\n</code></pre> <pre><code>-- Analyze histogram statistics\nSELECT \n    quantile_from_histogram(response_time_histogram, 0.5) as median_ms,\n    quantile_from_histogram(response_time_histogram, 0.95) as p95_ms,\n    quantile_from_histogram(response_time_histogram, 0.99) as p99_ms,\n    variance_from_histogram(response_time_histogram) as variance,\n    count_from_histogram(response_time_histogram) as sample_count,\n    sum_from_histogram(response_time_histogram) as total_time_ms\nFROM performance_histograms\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour';\n</code></pre> <pre><code>-- Aggregate histograms across multiple processes\nSELECT \n    time_bin,\n    sum_histograms(cpu_usage_histo) as combined_cpu_histogram,\n    quantile_from_histogram(sum_histograms(cpu_usage_histo), 0.95) as p95_cpu\nFROM cpu_usage_per_process_per_minute\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY time_bin\nORDER BY time_bin;\n</code></pre>"},{"location":"query-guide/functions-reference/#property-extraction-and-filtering","title":"Property Extraction and Filtering","text":"<pre><code>-- Find logs with specific thread names\nSELECT time, level, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') LIKE '%worker%'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#high-performance-jsonb-property-access","title":"High-Performance JSONB Property Access","text":"<pre><code>-- Convert properties to JSONB for better performance\nSELECT\n    time,\n    msg,\n    property_get(properties_to_jsonb(properties), 'service') as service,\n    property_get(properties_to_jsonb(properties), 'version') as version\nFROM log_entries\nWHERE property_get(properties_to_jsonb(properties), 'env') = 'production'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n</code></pre> <pre><code>-- Efficient property filtering with JSONB\nWITH jsonb_logs AS (\n    SELECT\n        time,\n        level,\n        msg,\n        properties_to_jsonb(properties) as jsonb_props\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '1 day'\n)\nSELECT\n    time,\n    level,\n    msg,\n    property_get(jsonb_props, 'service') as service,\n    property_get(jsonb_props, 'request_id') as request_id\nFROM jsonb_logs\nWHERE property_get(jsonb_props, 'error_code') IS NOT NULL\nORDER BY time DESC;\n</code></pre> <pre><code>-- Property aggregation with optimal performance\nSELECT\n    property_get(properties_to_jsonb(properties), 'service') as service,\n    property_get(properties_to_jsonb(properties), 'env') as environment,\n    COUNT(*) as event_count,\n    COUNT(CASE WHEN level &lt;= 2 THEN 1 END) as error_count\nFROM log_entries\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n  AND property_get(properties_to_jsonb(properties), 'service') IS NOT NULL\nGROUP BY service, environment\nORDER BY error_count DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#json-data-processing","title":"JSON Data Processing","text":"<pre><code>-- Parse and extract configuration from JSON logs\nSELECT \n    time,\n    msg,\n    jsonb_as_string(jsonb_get(jsonb_parse(msg), 'service')) as service_name,\n    jsonb_as_i64(jsonb_get(jsonb_parse(msg), 'port')) as port,\n    jsonb_as_f64(jsonb_get(jsonb_parse(msg), 'cpu_limit')) as cpu_limit\nFROM log_entries\nWHERE msg LIKE '%{%'  -- Contains JSON\n  AND jsonb_parse(msg) IS NOT NULL\nORDER BY time DESC;\n</code></pre> <pre><code>-- Aggregate metrics from JSON payloads\nSELECT \n    jsonb_as_string(jsonb_get(jsonb_parse(msg), 'service')) as service,\n    COUNT(*) as event_count,\n    AVG(jsonb_as_f64(jsonb_get(jsonb_parse(msg), 'response_time'))) as avg_response_ms\nFROM log_entries\nWHERE msg LIKE '%response_time%'\n  AND jsonb_parse(msg) IS NOT NULL\nGROUP BY service\nORDER BY avg_response_ms DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#time-based-aggregation","title":"Time-based Aggregation","text":"<pre><code>-- Hourly error counts\nSELECT \n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/functions-reference/#performance-trace-analysis","title":"Performance Trace Analysis","text":"<pre><code>-- Top 10 slowest functions with statistics\nSELECT \n    name,\n    COUNT(*) as call_count,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    STDDEV(duration) / 1000000.0 as stddev_ms\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 100000  -- &gt; 0.1ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/functions-reference/#datafusion-reference","title":"DataFusion Reference","text":"<p>Micromegas supports all standard DataFusion SQL syntax, functions, and operators. For complete documentation including functions, operators, data types, and SQL syntax, see the Apache DataFusion SQL Reference.</p>"},{"location":"query-guide/functions-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Schema Reference - Complete view and field reference</li> </ul>"},{"location":"query-guide/performance/","title":"Performance Guide","text":"<p>Guidelines for writing efficient Micromegas SQL queries and avoiding common performance pitfalls.</p>"},{"location":"query-guide/performance/#critical-performance-rules","title":"Critical Performance Rules","text":""},{"location":"query-guide/performance/#1-always-use-time-ranges-via-python-api","title":"1. Always Use Time Ranges (via Python API)","text":"<p>\u26a1 Performance Tip: Always specify time ranges through the Python API parameters, not in SQL WHERE clauses.</p> <p>\u274c Inefficient - SQL time filter: <pre><code># Analytics server scans ALL partitions, then filters in SQL\nsql = \"\"\"\n    SELECT COUNT(*) FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '1 hour';\n\"\"\"\nresult = client.query(sql)  # No time range parameters!\n</code></pre></p> <p>\u2705 Efficient - API time range: <pre><code>import datetime\n\n# Analytics server eliminates irrelevant partitions BEFORE query execution\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\nsql = \"SELECT COUNT(*) FROM log_entries;\"\nresult = client.query(sql, begin, end)  # \u2b50 Time range in API\n</code></pre></p> <p>Why API time ranges are faster:</p> <ul> <li>Partition Elimination: Analytics server removes entire partitions from consideration before SQL execution</li> <li>Metadata Optimization: Uses partition metadata to skip irrelevant data files  </li> <li>Memory Efficiency: Only loads relevant data into query engine memory</li> <li>Network Efficiency: Transfers only relevant data over FlightSQL</li> </ul> <p>Performance Impact:</p> <ul> <li>API time range: Query considers only 1-2 partitions</li> <li>SQL time filter: Query scans all partitions, then filters millions of rows</li> </ul>"},{"location":"query-guide/performance/#2-use-process-scoped-views","title":"2. Use Process-Scoped Views","text":"<p>\u274c Less Efficient: <pre><code>-- Scans all data then filters\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n</code></pre></p> <p>\u2705 More Efficient: <pre><code>-- Uses optimized process partition\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre></p>"},{"location":"query-guide/performance/#query-optimization","title":"Query Optimization","text":""},{"location":"query-guide/performance/#predicate-pushdown","title":"Predicate Pushdown","text":"<p>Micromegas automatically pushes filters down to the storage layer when possible:</p> <pre><code>-- These filters are pushed to Parquet reader for efficiency\nWHERE time &gt;= NOW() - INTERVAL '1 day'\n  AND level &lt;= 3\n  AND process_id = 'my_process'\n</code></pre>"},{"location":"query-guide/performance/#memory-considerations","title":"Memory Considerations","text":"<p>Use LIMIT for exploration: <pre><code>-- Good for testing queries\nSELECT * FROM log_entries LIMIT 1000;\n</code></pre></p> <p>Use streaming for large results: <pre><code># Python API for large datasets\nfor batch in client.query_stream(sql, begin, end):\n    process_batch(batch.to_pandas())\n</code></pre></p>"},{"location":"query-guide/python-api-advanced/","title":"Python API Advanced Guide","text":"<p>This guide covers advanced usage patterns, performance optimization techniques, and specialized features for power users of the Micromegas Python client.</p> <p>Prerequisites</p> <p>Before reading this guide, familiarize yourself with the Python API Reference for basic usage patterns.</p>"},{"location":"query-guide/python-api-advanced/#advanced-connection-patterns","title":"Advanced Connection Patterns","text":""},{"location":"query-guide/python-api-advanced/#authentication-and-headers","title":"Authentication and Headers","text":"<p>Configure custom authentication headers for enterprise deployments:</p> <pre><code>from micromegas.flightsql.client import FlightSQLClient\n\n# Token-based authentication\nclient = FlightSQLClient(\n    \"grpc+tls://analytics.company.com:50051\",\n    headers={\n        \"authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\",\n        \"x-tenant-id\": \"production\",\n        \"x-request-id\": \"trace-12345\"\n    }\n)\n\n# API key authentication  \nclient = FlightSQLClient(\n    \"grpc+tls://micromegas.example.com:50051\",\n    headers={\n        \"x-api-key\": \"mm_prod_1234567890abcdef\",\n        \"x-client-version\": \"python-1.0.0\"\n    }\n)\n</code></pre>"},{"location":"query-guide/python-api-advanced/#schema-discovery-and-query-validation","title":"Schema Discovery and Query Validation","text":""},{"location":"query-guide/python-api-advanced/#advanced-schema-introspection","title":"Advanced Schema Introspection","text":"<pre><code>def analyze_query_schema(client, sql):\n    \"\"\"Analyze query result schema without execution.\"\"\"\n    stmt = client.prepare_statement(sql)\n\n    analysis = {\n        'column_count': len(stmt.dataset_schema),\n        'columns': [],\n        'estimated_row_size': 0\n    }\n\n    for field in stmt.dataset_schema:\n        col_info = {\n            'name': field.name,\n            'type': str(field.type),\n            'nullable': field.nullable\n        }\n\n        # Estimate column size for memory planning\n        if 'string' in str(field.type):\n            estimated_size = 50  # Average string length\n        elif 'int64' in str(field.type):\n            estimated_size = 8\n        elif 'int32' in str(field.type):\n            estimated_size = 4\n        elif 'timestamp' in str(field.type):\n            estimated_size = 8\n        else:\n            estimated_size = 16  # Default estimate\n\n        col_info['estimated_bytes'] = estimated_size\n        analysis['columns'].append(col_info)\n        analysis['estimated_row_size'] += estimated_size\n\n    return analysis\n\n# Usage\nsql = \"SELECT time, process_id, level, msg FROM log_entries\"\nschema_info = analyze_query_schema(client, sql)\n\nprint(f\"Query will return {schema_info['column_count']} columns\")\nprint(f\"Estimated row size: {schema_info['estimated_row_size']} bytes\")\n\nfor col in schema_info['columns']:\n    print(f\"  {col['name']}: {col['type']} ({col['estimated_bytes']} bytes)\")\n</code></pre>"},{"location":"query-guide/python-api-advanced/#query-validation-pipeline","title":"Query Validation Pipeline","text":"<pre><code>def validate_query(client, sql, max_columns=50, max_estimated_size=1000):\n    \"\"\"Validate query before execution.\"\"\"\n    try:\n        analysis = analyze_query_schema(client, sql)\n    except Exception as e:\n        return {\n            'valid': False,\n            'error': f\"Schema analysis failed: {e}\",\n            'recommendations': [\"Check SQL syntax and table names\"]\n        }\n\n    recommendations = []\n\n    if analysis['column_count'] &gt; max_columns:\n        recommendations.append(f\"Query returns {analysis['column_count']} columns, consider selecting specific columns\")\n\n    if analysis['estimated_row_size'] &gt; max_estimated_size:\n        recommendations.append(f\"Large estimated row size ({analysis['estimated_row_size']} bytes), consider using query_stream()\")\n\n    # Check for potentially expensive operations\n    sql_upper = sql.upper()\n    if 'ORDER BY' in sql_upper and 'LIMIT' not in sql_upper:\n        recommendations.append(\"ORDER BY without LIMIT may be expensive, consider adding LIMIT\")\n\n    if not any(param in sql_upper for param in ['BEGIN', 'END', 'WHERE TIME']):\n        recommendations.append(\"No time filtering detected, consider adding time range parameters\")\n\n    return {\n        'valid': True,\n        'analysis': analysis,\n        'recommendations': recommendations\n    }\n\n# Usage\nsql = \"\"\"\nSELECT time, process_id, level, target, msg, properties \nFROM log_entries \nORDER BY time DESC\n\"\"\"\n\nvalidation = validate_query(client, sql)\nif validation['valid']:\n    print(\"Query is valid\")\n    for rec in validation['recommendations']:\n        print(f\"\u26a0\ufe0f  {rec}\")\nelse:\n    print(f\"\u274c Query validation failed: {validation['error']}\")\n</code></pre>"},{"location":"query-guide/python-api-advanced/#performance-optimization-patterns","title":"Performance Optimization Patterns","text":""},{"location":"query-guide/python-api-advanced/#intelligent-batching-for-large-datasets","title":"Intelligent Batching for Large Datasets","text":"<pre><code>import time\nfrom datetime import datetime, timedelta, timezone\n\nclass OptimizedQueryExecutor:\n    \"\"\"Execute large queries with intelligent batching and progress tracking.\"\"\"\n\n    def __init__(self, client, batch_size_mb=100):\n        self.client = client\n        self.batch_size_mb = batch_size_mb\n        self.stats = {\n            'batches_processed': 0,\n            'rows_processed': 0,\n            'total_time': 0,\n            'avg_batch_time': 0\n        }\n\n    def execute_large_query(self, sql, begin, end, processor_func=None):\n        \"\"\"Execute query with automatic batching based on result size.\"\"\"\n        start_time = time.time()\n        total_rows = 0\n\n        # First, estimate result size\n        count_sql = f\"SELECT COUNT(*) as row_count FROM ({sql}) as subquery\"\n        try:\n            count_df = self.client.query(count_sql, begin, end)\n            estimated_rows = count_df['row_count'].iloc[0]\n            print(f\"\ud83d\udcca Estimated {estimated_rows:,} rows\")\n        except:\n            estimated_rows = None\n            print(\"\u26a0\ufe0f  Could not estimate row count\")\n\n        # Execute with streaming for large results\n        batch_count = 0\n        for batch in self.client.query_stream(sql, begin, end):\n            batch_start = time.time()\n            df = batch.to_pandas()\n            batch_rows = len(df)\n\n            # Process batch\n            if processor_func:\n                processor_func(df, batch_count)\n\n            # Update statistics\n            batch_count += 1\n            total_rows += batch_rows\n            batch_time = time.time() - batch_start\n\n            self.stats['batches_processed'] = batch_count\n            self.stats['rows_processed'] = total_rows\n            self.stats['avg_batch_time'] = (self.stats['avg_batch_time'] * (batch_count - 1) + batch_time) / batch_count\n\n            # Progress reporting\n            if estimated_rows:\n                progress = (total_rows / estimated_rows) * 100\n                print(f\"\ud83d\udd04 Batch {batch_count}: {batch_rows:,} rows ({progress:.1f}% complete)\")\n            else:\n                print(f\"\ud83d\udd04 Batch {batch_count}: {batch_rows:,} rows ({total_rows:,} total)\")\n\n        self.stats['total_time'] = time.time() - start_time\n\n        print(f\"\u2705 Completed: {total_rows:,} rows in {self.stats['total_time']:.2f}s\")\n        print(f\"   Average: {total_rows/self.stats['total_time']:.0f} rows/sec\")\n\n        return self.stats\n\n# Usage example\ndef process_error_logs(df, batch_num):\n    \"\"\"Process each batch of error logs.\"\"\"\n    errors = df[df['level'] &lt;= 2]  # Error and critical levels\n    if not errors.empty:\n        print(f\"  Found {len(errors)} errors in batch {batch_num}\")\n        # Could save to file, send alerts, etc.\n\nexecutor = OptimizedQueryExecutor(client)\nend = datetime.now(timezone.utc)\nbegin = end - timedelta(days=7)\n\nstats = executor.execute_large_query(\n    \"SELECT time, level, target, msg FROM log_entries WHERE level &lt;= 3\",\n    begin, end,\n    processor_func=process_error_logs\n)\n</code></pre>"},{"location":"query-guide/python-api-advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Schema Reference - Understand table structures and relationships</li> <li>Functions Reference - Available SQL functions and operators  </li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Query optimization techniques</li> </ul> <p>For complex integration scenarios or custom tooling, consider the patterns in this guide as starting points for your specific use case.</p>"},{"location":"query-guide/python-api/","title":"Python API Reference","text":"<p>The Micromegas Python client provides a simple but powerful interface for querying observability data using SQL. This page covers all client methods, connection options, and advanced features.</p>"},{"location":"query-guide/python-api/#installation","title":"Installation","text":"<p>Install the Micromegas Python client from PyPI:</p> <pre><code>pip install micromegas\n</code></pre>"},{"location":"query-guide/python-api/#basic-usage","title":"Basic Usage","text":""},{"location":"query-guide/python-api/#connection","title":"Connection","text":"<pre><code>import micromegas\n\n# Connect to local Micromegas instance\nclient = micromegas.connect()\n\n# Connect with dictionary encoding preservation (for memory efficiency)\nclient = micromegas.connect(preserve_dictionary=True)\n</code></pre> <p>The <code>connect()</code> function connects to the analytics service at <code>grpc://localhost:50051</code>.</p> <p>Parameters: - <code>preserve_dictionary</code> (bool, optional): Enable dictionary encoding preservation for memory-efficient data transfer. Default: <code>False</code></p>"},{"location":"query-guide/python-api/#simple-queries","title":"Simple Queries","text":"<pre><code>import datetime\n\n# Set up time range\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Execute query with time range\nsql = \"SELECT * FROM log_entries LIMIT 10;\"\ndf = client.query(sql, begin, end)\nprint(df)\n</code></pre>"},{"location":"query-guide/python-api/#client-methods","title":"Client Methods","text":""},{"location":"query-guide/python-api/#querysql-beginnone-endnone","title":"<code>query(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string</li> <li><code>begin</code> (datetime or str, optional): \u26a1 Recommended - Start time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T00:00:00Z\"</code>)</li> <li><code>end</code> (datetime or str, optional): \u26a1 Recommended - End time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T23:59:59Z\"</code>)</li> </ul> <p>Returns:</p> <ul> <li><code>pandas.DataFrame</code>: Query results</li> </ul> <p>Performance Note: Using <code>begin</code> and <code>end</code> parameters instead of SQL time filters allows the analytics server to eliminate entire partitions before query execution, providing significant performance improvements.</p> <p>Example: <pre><code># \u2705 EFFICIENT: API time range enables partition elimination\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\", begin, end)  # \u2b50 Time range in API parameters\n\n# \u274c INEFFICIENT: SQL time filter scans all partitions\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '1 hour'  -- Server scans ALL partitions\n      AND level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\")  # Missing API time parameters!\n\n# \u2705 Using RFC3339 strings for time ranges\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\", \"2024-01-01T00:00:00Z\", \"2024-01-01T23:59:59Z\")  # \u2b50 RFC3339 strings\n\n# \u2705 OK: Query without time range (for metadata queries)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n</code></pre></p>"},{"location":"query-guide/python-api/#query_streamsql-beginnone-endnone","title":"<code>query_stream(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a stream of Apache Arrow RecordBatch objects. Use this for large datasets to avoid memory issues.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string  </li> <li><code>begin</code> (datetime or str, optional): \u26a1 Recommended - Start time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T00:00:00Z\"</code>)</li> <li><code>end</code> (datetime or str, optional): \u26a1 Recommended - End time for partition elimination. Can be a <code>datetime</code> object or RFC3339 string (e.g., <code>\"2024-01-01T23:59:59Z\"</code>)</li> </ul> <p>Returns:</p> <ul> <li>Iterator of <code>pyarrow.RecordBatch</code>: Stream of result batches</li> </ul> <p>Example: <pre><code>import pyarrow as pa\n\n# Stream large dataset\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n    ORDER BY time DESC;\n\"\"\"\n\nfor record_batch in client.query_stream(sql, begin, end):\n    # record_batch is a pyarrow.RecordBatch\n    print(f\"Batch shape: {record_batch.num_rows} x {record_batch.num_columns}\")\n    print(f\"Schema: {record_batch.schema}\")\n\n    # Convert to pandas for analysis\n    df = record_batch.to_pandas()\n\n    # Process this batch\n    error_logs = df[df['level'] &lt;= 3]\n    if not error_logs.empty:\n        print(f\"Found {len(error_logs)} errors in this batch\")\n        # Process errors...\n\n    # Memory is automatically freed after each batch\n</code></pre></p>"},{"location":"query-guide/python-api/#query_arrowsql-beginnone-endnone","title":"<code>query_arrow(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as an Apache Arrow Table. This method preserves dictionary encoding when <code>preserve_dictionary=True</code> is set during connection.</p> <p>Parameters:</p> <ul> <li><code>sql</code> (str): SQL query string</li> <li><code>begin</code> (datetime or str, optional): Start time for partition elimination</li> <li><code>end</code> (datetime or str, optional): End time for partition elimination</li> </ul> <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: Query results as Arrow Table</li> </ul> <p>Example: <pre><code># Connect with dictionary preservation\ndict_client = micromegas.connect(preserve_dictionary=True)\n\n# Get Arrow table with preserved dictionary encoding\ntable = dict_client.query_arrow(\"\"\"\n    SELECT properties_to_dict(properties) as dict_props\n    FROM measures\n\"\"\", begin, end)\n\n# Check if column uses dictionary encoding\nprint(f\"Dictionary encoded: {pa.types.is_dictionary(table.schema.field('dict_props').type)}\")\nprint(f\"Memory usage: {table.nbytes:,} bytes\")\n</code></pre></p>"},{"location":"query-guide/python-api/#dictionary-encoding-for-memory-efficiency","title":"Dictionary Encoding for Memory Efficiency","text":"<p>When working with large datasets containing repeated values (like properties), dictionary encoding can reduce memory usage by 50-80%. Micromegas provides built-in support for dictionary encoding:</p>"},{"location":"query-guide/python-api/#using-dictionary-encoded-properties","title":"Using Dictionary-Encoded Properties","text":"<pre><code># Connect with dictionary preservation enabled\nclient = micromegas.connect(preserve_dictionary=True)\n\n# Use properties_to_dict UDF for dictionary encoding\nsql = \"\"\"\nSELECT \n    time,\n    process_id,\n    properties_to_dict(properties) as dict_props,\n    properties_length(properties_to_dict(properties)) as prop_count\nFROM measures\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n\"\"\"\n\n# Option 1: Get as pandas DataFrame (automatic conversion)\ndf = client.query(sql, begin, end)\nprint(f\"DataFrame shape: {df.shape}\")\nprint(f\"Memory usage: {df.memory_usage(deep=True).sum():,} bytes\")\n\n# Option 2: Get as Arrow Table (preserves dictionary encoding)\ntable = client.query_arrow(sql, begin, end)\nprint(f\"Arrow table memory: {table.nbytes:,} bytes\")\n\n# Dictionary encoding typically uses 50-80% less memory\n</code></pre>"},{"location":"query-guide/python-api/#compatibility-with-standard-functions","title":"Compatibility with Standard Functions","text":"<p>Dictionary-encoded data works seamlessly with Micromegas UDFs:</p> <pre><code>sql = \"\"\"\nSELECT \n    -- Direct property access\n    property_get(properties, 'source') as source,\n\n    -- Length calculation (works with both formats)\n    properties_length(properties) as regular_count,\n    properties_length(properties_to_dict(properties)) as dict_count,\n\n    -- Convert back to array when needed\n    array_length(properties_to_array(properties_to_dict(properties))) as array_count\nFROM measures\n\"\"\"\n\ndf = client.query(sql, begin, end)\n</code></pre>"},{"location":"query-guide/python-api/#working-with-results","title":"Working with Results","text":""},{"location":"query-guide/python-api/#pandas-dataframes","title":"pandas DataFrames","text":"<p>All <code>query()</code> results are pandas DataFrames, giving you access to the full pandas ecosystem:</p> <pre><code># Basic DataFrame operations\nresult = client.query(\"SELECT process_id, exe, start_time FROM processes;\")\n\n# Inspect the data\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Filter and analyze\nrecent = result[result['start_time'] &gt; datetime.datetime.now() - datetime.timedelta(days=1)]\nprint(f\"Recent processes: {len(recent)}\")\n\n# Group and aggregate\nby_exe = result.groupby('exe').size().sort_values(ascending=False)\nprint(\"Processes by executable:\")\nprint(by_exe.head())\n</code></pre>"},{"location":"query-guide/python-api/#pyarrow-recordbatch","title":"pyarrow RecordBatch","text":"<p>Streaming queries return Apache Arrow RecordBatch objects:</p> <pre><code>for batch in client.query_stream(sql, begin, end):\n    # RecordBatch properties\n    print(f\"Rows: {batch.num_rows}\")\n    print(f\"Columns: {batch.num_columns}\")\n    print(f\"Schema: {batch.schema}\")\n\n    # Access individual columns\n    time_column = batch.column('time')\n    level_column = batch.column('level')\n\n    # Convert to pandas (zero-copy operation)\n    df = batch.to_pandas()\n\n    # Convert to other formats\n    table = batch.to_pylist()  # List of dictionaries\n    numpy_dict = batch.to_pydict()  # Dictionary of numpy arrays\n</code></pre>"},{"location":"query-guide/python-api/#connection-configuration","title":"Connection Configuration","text":""},{"location":"query-guide/python-api/#flightsqlclienturi-headersnone","title":"<code>FlightSQLClient(uri, headers=None)</code>","text":"<p>For advanced connection scenarios, use the <code>FlightSQLClient</code> class directly:</p> <pre><code>from micromegas.flightsql.client import FlightSQLClient\n\n# Connect to remote server with authentication\nclient = FlightSQLClient(\n    \"grpc+tls://remote-server:50051\",\n    headers={\"authorization\": \"Bearer your-token\"}\n)\n\n# Connect to local server (equivalent to micromegas.connect())\nclient = FlightSQLClient(\"grpc://localhost:50051\")\n</code></pre> <p>Parameters: - <code>uri</code> (str): FlightSQL server URI. Use <code>grpc://</code> for unencrypted or <code>grpc+tls://</code> for TLS connections - <code>headers</code> (dict, optional): Custom headers for authentication or metadata</p>"},{"location":"query-guide/python-api/#schema-discovery","title":"Schema Discovery","text":""},{"location":"query-guide/python-api/#prepare_statementsql","title":"<code>prepare_statement(sql)</code>","text":"<p>Get query schema information without executing the query:</p> <pre><code># Prepare statement to discover schema\nstmt = client.prepare_statement(\n    \"SELECT time, level, msg FROM log_entries WHERE level &lt;= 3\"\n)\n\n# Inspect the schema\nprint(\"Query result schema:\")\nfor field in stmt.dataset_schema:\n    print(f\"  {field.name}: {field.type}\")\n\n# Output:\n#   time: timestamp[ns]\n#   level: int32  \n#   msg: string\n\n# The query is also available\nprint(f\"Query: {stmt.query}\")\n</code></pre>"},{"location":"query-guide/python-api/#prepared_statement_streamstatement","title":"<code>prepared_statement_stream(statement)</code>","text":"<p>Execute a prepared statement (mainly useful after schema inspection):</p> <pre><code># Execute the prepared statement\nfor batch in client.prepared_statement_stream(stmt):\n    df = batch.to_pandas()\n    print(f\"Received {len(df)} rows\")\n</code></pre> <p>Note: Prepared statements are primarily for schema discovery. Execution offers no performance benefit over <code>query_stream()</code>.</p>"},{"location":"query-guide/python-api/#process-and-stream-discovery","title":"Process and Stream Discovery","text":""},{"location":"query-guide/python-api/#find_processprocess_id","title":"<code>find_process(process_id)</code>","text":"<p>Find detailed information about a specific process:</p> <pre><code># Find process by ID\nprocess_info = client.find_process('550e8400-e29b-41d4-a716-446655440000')\n\nif not process_info.empty:\n    print(f\"Process: {process_info['exe'].iloc[0]}\")\n    print(f\"Started: {process_info['start_time'].iloc[0]}\")\n    print(f\"Computer: {process_info['computer'].iloc[0]}\")\nelse:\n    print(\"Process not found\")\n</code></pre>"},{"location":"query-guide/python-api/#query_streamsbegin-end-limit-process_idnone-tag_filternone","title":"<code>query_streams(begin, end, limit, process_id=None, tag_filter=None)</code>","text":"<p>Query event streams with filtering:</p> <pre><code># Query all streams from the last hour\nend = datetime.datetime.now(datetime.timezone.utc)\nbegin = end - datetime.timedelta(hours=1)\nstreams = client.query_streams(begin, end, limit=100)\n\n# Filter by process\nprocess_streams = client.query_streams(\n    begin, end, \n    limit=50,\n    process_id='550e8400-e29b-41d4-a716-446655440000'\n)\n\n# Filter by stream tag\nlog_streams = client.query_streams(\n    begin, end,\n    limit=20, \n    tag_filter='log'\n)\n\nprint(f\"Found {len(streams)} total streams\")\nprint(f\"Stream types: {streams['stream_type'].value_counts()}\")\n</code></pre>"},{"location":"query-guide/python-api/#query_blocksbegin-end-limit-stream_id","title":"<code>query_blocks(begin, end, limit, stream_id)</code>","text":"<p>Query data blocks within a stream (for low-level inspection):</p> <pre><code># First find a stream\nstreams = client.query_streams(begin, end, limit=1)\nif not streams.empty:\n    stream_id = streams['stream_id'].iloc[0]\n\n    # Query blocks in that stream\n    blocks = client.query_blocks(begin, end, 100, stream_id)\n    print(f\"Found {len(blocks)} blocks\")\n    print(f\"Total events: {blocks['nb_events'].sum()}\")\n    print(f\"Total size: {blocks['payload_size'].sum()} bytes\")\n</code></pre>"},{"location":"query-guide/python-api/#query_spansbegin-end-limit-stream_id","title":"<code>query_spans(begin, end, limit, stream_id)</code>","text":"<p>Query execution spans for performance analysis:</p> <pre><code># Query spans for detailed performance analysis\nspans = client.query_spans(begin, end, 1000, stream_id)\n\n# Find slowest operations\nslow_spans = spans.nlargest(10, 'duration')\nprint(\"Slowest operations:\")\nfor _, span in slow_spans.iterrows():\n    duration_ms = span['duration'] / 1000000  # Convert nanoseconds to milliseconds\n    print(f\"  {span['name']}: {duration_ms:.2f}ms\")\n\n# Analyze span hierarchy\nroot_spans = spans[spans['parent_span_id'].isna()]\nprint(f\"Found {len(root_spans)} root operations\")\n</code></pre>"},{"location":"query-guide/python-api/#data-management","title":"Data Management","text":""},{"location":"query-guide/python-api/#bulk_ingesttable_name-df","title":"<code>bulk_ingest(table_name, df)</code>","text":"<p>Bulk ingest metadata for replication or administrative tasks:</p> <pre><code>import pandas as pd\n\n# Example: Replicate process metadata\nprocesses_df = pd.DataFrame({\n    'process_id': ['550e8400-e29b-41d4-a716-446655440000'],\n    'exe': ['/usr/bin/myapp'],\n    'username': ['user'],\n    'realname': ['User Name'],\n    'computer': ['hostname'],\n    'distro': ['Ubuntu 22.04'],\n    'cpu_brand': ['Intel Core i7'],\n    'tsc_frequency': [2400000000],\n    'start_time': [datetime.datetime.now(datetime.timezone.utc)],\n    'start_ticks': [1234567890],\n    'insert_time': [datetime.datetime.now(datetime.timezone.utc)],\n    'parent_process_id': [''],\n    'properties': [[]]\n})\n\n# Ingest process metadata\nresult = client.bulk_ingest('processes', processes_df)\nif result:\n    print(f\"Ingested {result.record_count} process records\")\n</code></pre> <p>Supported tables: <code>processes</code>, <code>streams</code>, <code>blocks</code>, <code>payloads</code></p> <p>Note: This method is for metadata replication and administrative tasks. Use the telemetry ingestion service HTTP API for normal data ingestion.</p>"},{"location":"query-guide/python-api/#materialize_partitionsview_set_name-begin-end-partition_delta_seconds","title":"<code>materialize_partitions(view_set_name, begin, end, partition_delta_seconds)</code>","text":"<p>Create materialized partitions for performance optimization:</p> <pre><code># Materialize hourly partitions for the last 24 hours\nend = datetime.datetime.now(datetime.timezone.utc)\nbegin = end - datetime.timedelta(days=1)\n\nclient.materialize_partitions(\n    'log_entries',\n    begin,\n    end,\n    3600  # 1-hour partitions\n)\n# Prints progress messages for each materialized partition\n</code></pre>"},{"location":"query-guide/python-api/#retire_partitionsview_set_name-view_instance_id-begin-end","title":"<code>retire_partitions(view_set_name, view_instance_id, begin, end)</code>","text":"<p>Remove materialized partitions to free up storage:</p> <pre><code># Retire old partitions\nclient.retire_partitions(\n    'log_entries',\n    'process-123-456', \n    begin,\n    end\n)\n# Prints status messages as partitions are retired\n</code></pre> <p>Warning: This operation cannot be undone. Retired partitions must be re-materialized if needed.</p>"},{"location":"query-guide/python-api/#administrative-functions","title":"Administrative Functions","text":"<p>The <code>micromegas.admin</code> module provides administrative functions for schema evolution and partition lifecycle management. These functions are intended for system administrators and should be used with caution.</p>"},{"location":"query-guide/python-api/#list_incompatible_partitionsclient-view_set_namenone","title":"<code>list_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Lists partitions with schemas incompatible with current view set schemas. These partitions cannot be queried correctly alongside current partitions and should be retired to enable schema evolution.</p> <pre><code>import micromegas\nimport micromegas.admin\n\n# Connect to analytics service\nclient = micromegas.connect()\n\n# List all incompatible partitions across all view sets\nincompatible = micromegas.admin.list_incompatible_partitions(client)\nprint(f\"Found {len(incompatible)} groups of incompatible partitions\")\nprint(f\"Total incompatible partitions: {incompatible['partition_count'].sum()}\")\nprint(f\"Total size to be freed: {incompatible['total_size_bytes'].sum() / (1024**3):.2f} GB\")\n\n# List incompatible partitions for specific view set\nlog_incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Log entries incompatible partitions: {log_incompatible['partition_count'].sum()}\")\n</code></pre> <p>Returns: - <code>view_set_name</code>: Name of the view set - <code>view_instance_id</code>: Instance ID (e.g., process_id or 'global') - <code>incompatible_schema_hash</code>: The old schema hash in the partition - <code>current_schema_hash</code>: The current schema hash from ViewFactory - <code>partition_count</code>: Number of incompatible partitions with this schema - <code>total_size_bytes</code>: Total size in bytes of all incompatible partitions - <code>file_paths</code>: Array of file paths for each incompatible partition</p>"},{"location":"query-guide/python-api/#retire_incompatible_partitionsclient-view_set_namenone","title":"<code>retire_incompatible_partitions(client, view_set_name=None)</code>","text":"<p>Retires partitions with schemas incompatible with current view set schemas. This enables safe schema evolution by cleaning up old schema versions.</p> <pre><code>import micromegas\nimport micromegas.admin\n\nclient = micromegas.connect()\n\n# Preview what would be retired (recommended first step)\npreview = micromegas.admin.list_incompatible_partitions(client, 'log_entries')\nprint(f\"Would retire {preview['partition_count'].sum()} partitions\")\nprint(f\"Would free {preview['total_size_bytes'].sum() / (1024**3):.2f} GB\")\n\n# Retire incompatible partitions for specific view set\nif input(\"Proceed with retirement? (yes/no): \") == \"yes\":\n    result = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')\n    print(f\"Retired {result['partitions_retired'].sum()} partitions\")\n    print(f\"Failed {result['partitions_failed'].sum()} partitions\")\n\n    # Check for any failures\n    for _, row in result.iterrows():\n        if row['partitions_failed'] &gt; 0:\n            print(f\"Failures in {row['view_set_name']}/{row['view_instance_id']}:\")\n            for msg in row['retirement_messages']:\n                if msg.startswith(\"ERROR:\"):\n                    print(f\"  {msg}\")\n</code></pre> <p>Returns: - <code>view_set_name</code>: View set that was processed - <code>view_instance_id</code>: Instance ID of partitions retired - <code>partitions_retired</code>: Count of partitions successfully retired - <code>partitions_failed</code>: Count of partitions that failed to retire - <code>storage_freed_bytes</code>: Total bytes freed from storage - <code>retirement_messages</code>: Array of detailed messages for each retirement attempt</p> <p>Safety Features: - Uses file-path-based retirement for precision targeting - Cannot accidentally retire compatible partitions - Provides detailed error reporting for failures - Allows view-specific filtering to prevent bulk operations</p> <p>\u26a0\ufe0f DESTRUCTIVE OPERATION: This operation is irreversible. Retired partitions will be permanently deleted from metadata and their data files removed from object storage. Always preview with <code>list_incompatible_partitions()</code> before calling this function.</p>"},{"location":"query-guide/python-api/#time-utilities","title":"Time Utilities","text":""},{"location":"query-guide/python-api/#format_datetimevalue-and-parse_time_deltauser_string","title":"<code>format_datetime(value)</code> and <code>parse_time_delta(user_string)</code>","text":"<p>Utility functions for time handling:</p> <pre><code>from micromegas.time import format_datetime, parse_time_delta\n\n# Format datetime for queries\ndt = datetime.datetime.now(datetime.timezone.utc)\nformatted = format_datetime(dt)\nprint(formatted)  # \"2024-01-01T12:00:00+00:00\"\n\n# Parse human-readable time deltas\none_hour = parse_time_delta('1h')\nthirty_minutes = parse_time_delta('30m') \nseven_days = parse_time_delta('7d')\n\n# Use in calculations\nrecent_time = datetime.datetime.now(datetime.timezone.utc) - parse_time_delta('2h')\n</code></pre> <p>Supported units: <code>m</code> (minutes), <code>h</code> (hours), <code>d</code> (days)</p>"},{"location":"query-guide/python-api/#advanced-features","title":"Advanced Features","text":""},{"location":"query-guide/python-api/#query-streaming-benefits","title":"Query Streaming Benefits","text":"<p>Use <code>query_stream()</code> for large datasets to:</p> <ul> <li>Reduce memory usage: Process data in chunks instead of loading everything</li> <li>Improve responsiveness: Start processing before the query completes</li> <li>Handle large results: Query datasets larger than available RAM</li> </ul> <pre><code># Example: Process week of data in batches\ntotal_errors = 0\ntotal_rows = 0\n\nfor batch in client.query_stream(\"\"\"\n    SELECT level, msg FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n\"\"\", begin, end):\n    df = batch.to_pandas()\n    errors_in_batch = len(df[df['level'] &lt;= 2])\n\n    total_errors += errors_in_batch\n    total_rows += len(df)\n\n    print(f\"Batch: {len(df)} rows, {errors_in_batch} errors\")\n\nprint(f\"Total: {total_rows} rows, {total_errors} errors\")\n</code></pre>"},{"location":"query-guide/python-api/#flightsql-protocol-benefits","title":"FlightSQL Protocol Benefits","text":"<p>Micromegas uses Apache Arrow FlightSQL for optimal performance:</p> <ul> <li>Columnar data transfer: Orders of magnitude faster than JSON</li> <li>Binary protocol: No serialization/deserialization overhead  </li> <li>Native compression: Efficient network utilization</li> <li>Vectorized operations: Optimized for analytical workloads</li> <li>Zero-copy operations: Direct memory mapping from network buffers</li> </ul>"},{"location":"query-guide/python-api/#connection-configuration_1","title":"Connection Configuration","text":"<pre><code># Connect to local server (default)\nclient = micromegas.connect()\n\n# Connect to a custom endpoint using FlightSQLClient directly\nfrom micromegas.flightsql.client import FlightSQLClient\nclient = FlightSQLClient(\"grpc://remote-server:50051\")\n</code></pre>"},{"location":"query-guide/python-api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    df = client.query(\"SELECT * FROM log_entries;\", begin, end)\nexcept Exception as e:\n    print(f\"Query failed: {e}\")\n\n# Check for empty results\nif df.empty:\n    print(\"No data found for this time range\")\nelse:\n    print(f\"Found {len(df)} rows\")\n</code></pre>"},{"location":"query-guide/python-api/#performance-tips","title":"Performance Tips","text":""},{"location":"query-guide/python-api/#use-time-ranges","title":"Use Time Ranges","text":"<p>Always specify time ranges for better performance:</p> <pre><code># \u2705 Good - efficient\ndf = client.query(sql, begin, end)\n\n# \u274c Avoid - can be slow\ndf = client.query(sql)\n</code></pre>"},{"location":"query-guide/python-api/#streaming-for-large-results","title":"Streaming for Large Results","text":"<p>Use streaming for queries that might return large datasets:</p> <pre><code># If you expect &gt; 100MB of results, use streaming\nif expected_result_size_mb &gt; 100:\n    for batch in client.query_stream(sql, begin, end):\n        process_batch(batch.to_pandas())\nelse:\n    df = client.query(sql, begin, end)\n    process_dataframe(df)\n</code></pre>"},{"location":"query-guide/python-api/#limit-result-size","title":"Limit Result Size","text":"<p>Add LIMIT clauses for exploratory queries:</p> <pre><code># Good for exploration\ndf = client.query(\"SELECT * FROM log_entries LIMIT 1000;\", begin, end)\n\n# Then remove limit for production queries\ndf = client.query(\"SELECT * FROM log_entries WHERE level &lt;= 2;\", begin, end)\n</code></pre>"},{"location":"query-guide/python-api/#integration-examples","title":"Integration Examples","text":""},{"location":"query-guide/python-api/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Query data\ndf = client.query(\"\"\"\n    SELECT time, name, value \n    FROM measures \n    WHERE name = 'cpu_usage'\n\"\"\", begin, end)\n\n# Plot time series\nplt.figure(figsize=(12, 6))\nplt.plot(df['time'], df['value'])\nplt.title('CPU Usage Over Time')\nplt.xlabel('Time')\nplt.ylabel('CPU Usage %')\nplt.show()\n</code></pre>"},{"location":"query-guide/python-api/#data-pipeline","title":"Data Pipeline","text":"<pre><code>import pandas as pd\n\ndef extract_metrics(process_id, hours=24):\n    \"\"\"Extract metrics for a specific process.\"\"\"\n    end = datetime.datetime.now(datetime.timezone.utc)\n    begin = end - datetime.timedelta(hours=hours)\n\n    sql = f\"\"\"\n        SELECT time, name, value, unit\n        FROM view_instance('measures', '{process_id}')\n        ORDER BY time;\n    \"\"\"\n\n    return client.query(sql, begin, end)\n\ndef analyze_performance(df):\n    \"\"\"Analyze performance metrics.\"\"\"\n    metrics = {}\n    for name in df['name'].unique():\n        data = df[df['name'] == name]['value']\n        metrics[name] = {\n            'mean': data.mean(),\n            'max': data.max(),\n            'min': data.min(),\n            'std': data.std()\n        }\n    return metrics\n\n# Use in pipeline\nprocess_metrics = extract_metrics('my-service-123')\nperformance_summary = analyze_performance(process_metrics)\nprint(performance_summary)\n</code></pre>"},{"location":"query-guide/python-api/#next-steps","title":"Next Steps","text":"<ul> <li>Python API Advanced - Advanced patterns, performance optimization, and specialized tooling</li> <li>Schema Reference - Understand available views and fields</li> <li>Functions Reference - Learn about SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries</li> </ul>"},{"location":"query-guide/query-patterns/","title":"Query Patterns","text":"<p>Common patterns and examples for querying observability data with Micromegas SQL.</p>"},{"location":"query-guide/query-patterns/#error-tracking-and-debugging","title":"Error Tracking and Debugging","text":""},{"location":"query-guide/query-patterns/#recent-errors","title":"Recent Errors","text":"<pre><code>-- Get all errors from the last hour\nSELECT time, process_id, target, msg\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/query-patterns/#error-trends","title":"Error Trends","text":"<pre><code>-- Hourly error counts for trend analysis\nSELECT\n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/query-patterns/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"query-guide/query-patterns/#slow-operations","title":"Slow Operations","text":"<pre><code>-- Find slowest function calls\nSELECT\n    name,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    COUNT(*) as call_count\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 10000000  -- &gt; 10ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/query-patterns/#resource-usage","title":"Resource Usage","text":"<pre><code>-- CPU and memory trends\nSELECT\n    date_trunc('minute', time) as minute,\n    name,\n    AVG(value) as avg_value,\n    unit\nFROM measures\nWHERE name IN ('cpu_usage', 'memory_usage')\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY minute, name, unit\nORDER BY minute, name;\n</code></pre>"},{"location":"query-guide/query-patterns/#async-performance-analysis","title":"Async Performance Analysis","text":""},{"location":"query-guide/query-patterns/#top-level-async-operations","title":"Top-Level Async Operations","text":"<pre><code>-- Find slowest top-level async operations\nSELECT\n    name,\n    AVG(duration_ms) as avg_duration,\n    MAX(duration_ms) as max_duration,\n    COUNT(*) as operation_count\nFROM (\n    SELECT\n        begin_events.name,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process') WHERE event_type = 'begin' AND depth = 0) begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY name\nORDER BY avg_duration DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/query-patterns/#nested-async-operations","title":"Nested Async Operations","text":"<pre><code>-- Find operations that spawn many async children\nSELECT\n    parent_name,\n    parent_depth,\n    COUNT(*) as child_count,\n    AVG(child_duration_ms) as avg_child_duration\nFROM (\n    SELECT\n        parent.name as parent_name,\n        parent.depth as parent_depth,\n        CAST((child_end.time - child_begin.time) AS BIGINT) / 1000000 as child_duration_ms\n    FROM view_instance('async_events', 'my_process') parent\n    JOIN view_instance('async_events', 'my_process') child_begin\n         ON parent.span_id = child_begin.parent_span_id AND child_begin.event_type = 'begin'\n    JOIN view_instance('async_events', 'my_process') child_end\n         ON child_begin.span_id = child_end.span_id AND child_end.event_type = 'end'\n    WHERE parent.event_type = 'begin'\n)\nGROUP BY parent_name, parent_depth\nHAVING COUNT(*) &gt; 5  -- Operations with many children\nORDER BY child_count DESC;\n</code></pre>"},{"location":"query-guide/query-patterns/#async-operation-timeline","title":"Async Operation Timeline","text":"<pre><code>-- Timeline view of async operations with depth hierarchy\nSELECT\n    time,\n    event_type,\n    name,\n    depth,\n    span_id,\n    parent_span_id,\n    REPEAT('  ', depth) || name as indented_name  -- Visual hierarchy\nFROM view_instance('async_events', 'my_process')\nWHERE time &gt;= NOW() - INTERVAL '10 minutes'\nORDER BY time;\n</code></pre>"},{"location":"query-guide/quick-start/","title":"Quick Start","text":"<p>Get up and running with Micromegas SQL queries in minutes. This guide shows you the essential patterns for querying your observability data.</p>"},{"location":"query-guide/quick-start/#basic-connection","title":"Basic Connection","text":"<p>All Micromegas queries start by connecting to the analytics service:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to Micromegas analytics service\nclient = micromegas.connect()\n</code></pre> <p>The <code>connect()</code> function connects to the analytics service at <code>grpc://localhost:50051</code>.</p>"},{"location":"query-guide/quick-start/#your-first-query","title":"Your First Query","text":"<p>Let's query recent log entries to see what data is available:</p> <pre><code># Set up time range for queries\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 4\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute the query\nlogs = client.query(sql, begin, end)\nprint(logs)\nprint(f\"Result type: {type(logs)}\")  # pandas.DataFrame\n</code></pre> <p>Key points:</p> <ul> <li>\u26a1 Important: Always specify time range via API parameters (<code>begin</code>, <code>end</code>) for best performance</li> <li>Results are returned as pandas DataFrames</li> <li><code>level &lt;= 4</code> filters to show errors and warnings (see log levels)</li> <li>Use API time parameters instead of SQL time filters for partition elimination</li> </ul>"},{"location":"query-guide/quick-start/#understanding-return-types","title":"Understanding Return Types","text":"<p>All queries return pandas DataFrames:</p> <pre><code># Query returns a pandas DataFrame\nresult = client.query(\"SELECT process_id, exe FROM processes LIMIT 5;\")\n\n# Access DataFrame properties\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Use pandas operations\nfiltered = result[result['exe'].str.contains('analytics')]\nprint(filtered.head())\n</code></pre> <p>This makes it easy to work with results using the entire pandas ecosystem for analysis, visualization, and data processing.</p>"},{"location":"query-guide/quick-start/#essential-query-patterns","title":"Essential Query Patterns","text":""},{"location":"query-guide/quick-start/#1-process-information","title":"1. Process Information","text":"<p>Get an overview of processes sending telemetry:</p> <pre><code>processes = client.query(\"\"\"\n    SELECT process_id, exe, computer, start_time\n    FROM processes\n    ORDER BY start_time DESC\n    LIMIT 10;\n\"\"\")\nprint(processes)\n</code></pre>"},{"location":"query-guide/quick-start/#2-recent-log-entries","title":"2. Recent Log Entries","text":"<p>Query logs with error filtering:</p> <pre><code>error_logs = client.query(\"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 3  -- Fatal, Error, Warn\n    ORDER BY time DESC\n    LIMIT 50;\n\"\"\", begin, end)\nprint(error_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#3-performance-metrics","title":"3. Performance Metrics","text":"<p>Query numeric measurements:</p> <pre><code>metrics = client.query(\"\"\"\n    SELECT time, process_id, name, value, unit\n    FROM measures\n    WHERE name LIKE '%cpu%'\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(metrics)\n</code></pre>"},{"location":"query-guide/quick-start/#4-process-specific-data","title":"4. Process-Specific Data","text":"<p>Use view instances for better performance when focusing on specific processes:</p> <pre><code>process_id = \"your_process_id_here\"  # Replace with actual process ID\n\nprocess_logs = client.query(f\"\"\"\n    SELECT time, level, target, msg\n    FROM view_instance('log_entries', '{process_id}')\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(process_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Common filters:</p> <ul> <li><code>level &lt;= 2</code> - Only fatal and error messages</li> <li><code>level &lt;= 3</code> - Fatal, error, and warning messages</li> <li><code>level &lt;= 4</code> - All messages except debug and trace</li> </ul>"},{"location":"query-guide/quick-start/#time-range-best-practices","title":"Time Range Best Practices","text":""},{"location":"query-guide/quick-start/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<pre><code># \u2705 Good - efficient and memory-safe\ndf = client.query(sql, begin_time, end_time)\n\n# \u274c Avoid - can be slow and memory-intensive\ndf = client.query(sql)  # Queries ALL data\n</code></pre>"},{"location":"query-guide/quick-start/#common-time-ranges","title":"Common Time Ranges","text":"<pre><code>now = datetime.datetime.now(datetime.timezone.utc)\n\n# Last hour\nbegin = now - datetime.timedelta(hours=1)\n\n# Last day\nbegin = now - datetime.timedelta(days=1)\n\n# Last week\nbegin = now - datetime.timedelta(weeks=1)\n\n# Custom range\nbegin = datetime.datetime(2024, 1, 1, tzinfo=datetime.timezone.utc)\nend = datetime.datetime(2024, 1, 2, tzinfo=datetime.timezone.utc)\n</code></pre>"},{"location":"query-guide/quick-start/#safe-queries-without-time-ranges","title":"Safe Queries Without Time Ranges","text":"<p>Some queries are safe to run without time ranges because they operate on small metadata tables:</p> <pre><code># Process information (typically small dataset)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n\n# Stream metadata\nstreams = client.query(\"SELECT stream_id, process_id FROM streams LIMIT 10;\")\n\n# Count queries (use with caution on large datasets)\ncount = client.query(\"SELECT COUNT(*) FROM log_entries;\")\n</code></pre> <p>Performance Impact</p> <p>Avoid querying <code>log_entries</code>, <code>measures</code>, <code>thread_spans</code>, or <code>async_events</code> without time ranges on production systems with large datasets.</p>"},{"location":"query-guide/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you can run basic queries:</p> <ol> <li>Explore the Python API - Learn about streaming and advanced features</li> <li>Review the Schema - Understand all available fields and data types</li> <li>Try Query Patterns - Common observability query patterns</li> <li>Optimize Performance - Learn to write efficient queries</li> </ol>"},{"location":"query-guide/quick-start/#quick-reference","title":"Quick Reference","text":""},{"location":"query-guide/quick-start/#essential-views","title":"Essential Views","text":"<ul> <li><code>processes</code> - Process metadata</li> <li><code>log_entries</code> - Application logs</li> <li><code>measures</code> - Numeric metrics</li> <li><code>thread_spans</code> - Execution timing</li> <li><code>async_events</code> - Async operation tracking</li> </ul>"},{"location":"query-guide/quick-start/#key-functions","title":"Key Functions","text":"<ul> <li><code>view_instance('view_name', 'process_id')</code> - Process-scoped views</li> <li><code>property_get(properties, 'key')</code> - Extract property values</li> <li><code>make_histogram(start, end, bins, values)</code> - Create histograms with specified range</li> </ul>"},{"location":"query-guide/quick-start/#time-functions","title":"Time Functions","text":"<ul> <li><code>NOW()</code> - Current timestamp</li> <li><code>INTERVAL '1 hour'</code> - Time duration</li> <li><code>date_trunc('hour', time)</code> - Truncate to time boundary</li> </ul>"},{"location":"query-guide/schema-reference/","title":"Schema Reference","text":"<p>This page provides a complete reference to all views, data types, and field definitions available in Micromegas SQL queries.</p>"},{"location":"query-guide/schema-reference/#views-overview","title":"Views Overview","text":"<p>Micromegas organizes telemetry data into several views that can be queried using SQL:</p> View Description Use Cases <code>processes</code> Process metadata and system information System overview, process tracking <code>streams</code> Data stream information within processes Stream debugging, data flow analysis <code>blocks</code> Core telemetry block metadata Low-level data inspection <code>log_entries</code> Application log messages with levels Error tracking, debugging, monitoring <code>log_stats</code> Aggregated log statistics by process, level, and target Log volume analysis, monitoring trends <code>measures</code> Numeric metrics and performance data Performance monitoring, alerting <code>thread_spans</code> Synchronous execution spans and timing Performance profiling, call tracing <code>async_events</code> Asynchronous event lifecycle tracking Async operation monitoring"},{"location":"query-guide/schema-reference/#core-views","title":"Core Views","text":""},{"location":"query-guide/schema-reference/#processes","title":"<code>processes</code>","text":"<p>Contains metadata about processes that have sent telemetry data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the process <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>realname</code> <code>Dictionary(Int16, Utf8)</code> Real name of the user <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>distro</code> <code>Dictionary(Int16, Utf8)</code> Operating system distribution <code>cpu_brand</code> <code>Dictionary(Int16, Utf8)</code> CPU brand information <code>tsc_frequency</code> <code>UInt64</code> Time stamp counter frequency <code>start_time</code> <code>Timestamp(Nanosecond)</code> Process start time <code>start_ticks</code> <code>UInt64</code> Process start time in ticks <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the process data was first inserted <code>parent_process_id</code> <code>Dictionary(Int16, Utf8)</code> Parent process identifier <code>properties</code> <code>Dictionary(Int32, Binary)</code> Additional process metadata (JSONB format) <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the process data was last updated <code>last_block_end_ticks</code> <code>Int64</code> Tick count when the last block ended <code>last_block_end_time</code> <code>Timestamp(Nanosecond)</code> Timestamp when the last block ended <p>Example Queries: <pre><code>-- Get all processes from the last day\nSELECT process_id, exe, computer, start_time\nFROM processes\nWHERE start_time &gt;= NOW() - INTERVAL '1 day'\nORDER BY start_time DESC;\n\n-- Find processes by executable name\nSELECT process_id, exe, username, computer\nFROM processes\nWHERE exe LIKE '%analytics%';\n</code></pre></p>"},{"location":"query-guide/schema-reference/#streams","title":"<code>streams</code>","text":"<p>Contains information about data streams within processes.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the stream <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Reference to the parent process <code>dependencies_metadata</code> <code>Binary</code> Stream dependency metadata <code>objects_metadata</code> <code>Binary</code> Stream object metadata <code>tags</code> <code>List&lt;Utf8&gt;</code> Stream tags <code>properties</code> <code>Dictionary(Int32, Binary)</code> Stream properties (JSONB format) <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was first inserted <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was last updated <p>Example Queries: <pre><code>-- Get streams for a specific process\nSELECT stream_id, tags, properties\nFROM streams\nWHERE process_id = 'my_process_123';\n\n-- Join streams with process information\nSELECT s.stream_id, s.tags, p.exe, p.computer\nFROM streams s\nJOIN processes p ON s.process_id = p.process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#blocks","title":"<code>blocks</code>","text":"<p>Core table containing telemetry block metadata with joined process and stream information.</p> Field Type Description <code>block_id</code> <code>Utf8</code> Unique identifier for the block <code>stream_id</code> <code>Utf8</code> Stream identifier <code>process_id</code> <code>Utf8</code> Process identifier <code>begin_time</code> <code>Timestamp(Nanosecond)</code> Block start time <code>begin_ticks</code> <code>Int64</code> Block start time in ticks <code>end_time</code> <code>Timestamp(Nanosecond)</code> Block end time <code>end_ticks</code> <code>Int64</code> Block end time in ticks <code>nb_objects</code> <code>Int32</code> Number of objects in block <code>object_offset</code> <code>Int64</code> Offset to objects in storage <code>payload_size</code> <code>Int64</code> Size of block payload <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When block was inserted <p>Joined Stream Fields:</p> Field Type Description <code>streams.dependencies_metadata</code> <code>Binary</code> Stream dependency metadata <code>streams.objects_metadata</code> <code>Binary</code> Stream object metadata <code>streams.tags</code> <code>List&lt;Utf8&gt;</code> Stream tags <code>streams.properties</code> <code>Dictionary(Int32, Binary)</code> Stream properties (JSONB format) <code>streams.insert_time</code> <code>Timestamp(Nanosecond)</code> When stream was inserted <p>Joined Process Fields:</p> Field Type Description <code>processes.start_time</code> <code>Timestamp(Nanosecond)</code> Process start time <code>processes.start_ticks</code> <code>Int64</code> Process start ticks <code>processes.tsc_frequency</code> <code>Int64</code> Time stamp counter frequency <code>processes.exe</code> <code>Utf8</code> Executable name <code>processes.username</code> <code>Utf8</code> User who ran the process <code>processes.realname</code> <code>Utf8</code> Real name of the user <code>processes.computer</code> <code>Utf8</code> Computer/hostname <code>processes.distro</code> <code>Utf8</code> Operating system distribution <code>processes.cpu_brand</code> <code>Utf8</code> CPU brand information <code>processes.insert_time</code> <code>Timestamp(Nanosecond)</code> When process was inserted <code>processes.parent_process_id</code> <code>Utf8</code> Parent process identifier <code>processes.properties</code> <code>Dictionary(Int32, Binary)</code> Process properties (JSONB format) <p>Example Queries: <pre><code>-- Analyze block sizes and object counts\nSELECT\n    process_id,\n    AVG(payload_size) as avg_block_size,\n    AVG(nb_objects) as avg_objects_per_block,\n    COUNT(*) as total_blocks\nFROM blocks\nWHERE insert_time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#observability-data-views","title":"Observability Data Views","text":""},{"location":"query-guide/schema-reference/#log_entries","title":"<code>log_entries</code>","text":"<p>Text-based log entries with levels and structured data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Log entry timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>level</code> <code>Int32</code> Log level (see Log Levels) <code>msg</code> <code>Utf8</code> Log message <code>properties</code> <code>Dictionary(Int32, Binary)</code> Log-specific properties (JSONB format) <code>process_properties</code> <code>Dictionary(Int32, Binary)</code> Process-specific properties (JSONB format)"},{"location":"query-guide/schema-reference/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Example Queries: <pre><code>-- Get recent error and warning logs\nSELECT time, process_id, level, target, msg\nFROM log_entries\nWHERE level &lt;= 3  -- Fatal, Error, Warn\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n\n-- Count logs by level for a specific process\nSELECT level, COUNT(*) as count\nFROM view_instance('log_entries', 'my_process_123')\nWHERE time &gt;= NOW() - INTERVAL '1 day'\nGROUP BY level\nORDER BY level;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#log_stats","title":"<code>log_stats</code>","text":"<p>Materialized view providing aggregated log statistics by process, minute, level, and target. This view is optimized for analyzing log volume trends and patterns over time.</p> Field Type Description <code>time_bin</code> <code>Timestamp(Nanosecond)</code> 1-minute time bucket for aggregation <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>level</code> <code>Int32</code> Log level (see Log Levels) <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target that generated the logs <code>count</code> <code>Int64</code> Number of log entries in this aggregation <p>Key Features: - Pre-aggregated by 1-minute intervals for efficient time-series queries - Materialized for fast query performance - Automatically updated as new log data arrives - Daily partitioning for efficient storage and querying</p> <p>Example Queries: <pre><code>-- Analyze log volume trends over the last hour\nSELECT \n    time_bin,\n    SUM(count) as total_logs,\n    SUM(CASE WHEN level &lt;= 2 THEN count ELSE 0 END) as error_count\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY time_bin\nORDER BY time_bin;\n\n-- Find noisiest modules by log volume\nSELECT \n    target,\n    SUM(count) as total_logs,\n    COUNT(DISTINCT time_bin) as active_minutes\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY target\nORDER BY total_logs DESC\nLIMIT 20;\n\n-- Monitor error rate by process\nSELECT \n    process_id,\n    time_bin,\n    SUM(CASE WHEN level &lt;= 2 THEN count ELSE 0 END) * 100.0 / SUM(count) as error_percentage\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '6 hours'\nGROUP BY process_id, time_bin\nHAVING SUM(count) &gt; 100  -- Filter out low-volume periods\nORDER BY time_bin, error_percentage DESC;\n\n-- Compare log levels distribution\nSELECT \n    level,\n    SUM(count) as total_count,\n    SUM(count) * 100.0 / (SELECT SUM(count) FROM log_stats WHERE time_bin &gt;= NOW() - INTERVAL '1 day') as percentage\nFROM log_stats\nWHERE time_bin &gt;= NOW() - INTERVAL '1 day'\nGROUP BY level\nORDER BY level;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#measures","title":"<code>measures</code>","text":"<p>Numerical measurements and counters.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Measurement timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>name</code> <code>Dictionary(Int16, Utf8)</code> Metric name <code>unit</code> <code>Dictionary(Int16, Utf8)</code> Measurement unit <code>value</code> <code>Float64</code> Metric value <code>properties</code> <code>Dictionary(Int32, Binary)</code> Metric-specific properties (JSONB format) <code>process_properties</code> <code>Dictionary(Int32, Binary)</code> Process-specific properties (JSONB format) <p>Example Queries: <pre><code>-- Get CPU metrics over time\nSELECT time, value, unit\nFROM measures\nWHERE name = 'cpu_usage'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time;\n\n-- Aggregate memory usage by process\nSELECT\n    process_id,\n    AVG(value) as avg_memory,\n    MAX(value) as peak_memory,\n    unit\nFROM measures\nWHERE name LIKE '%memory%'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id, unit;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#thread_spans","title":"<code>thread_spans</code>","text":"<p>Derived view for analyzing span durations and hierarchies. Access via <code>view_instance('thread_spans', stream_id)</code>.</p> Field Type Description <code>id</code> <code>Int64</code> Span identifier <code>parent</code> <code>Int64</code> Parent span identifier <code>depth</code> <code>UInt32</code> Nesting depth in call tree <code>hash</code> <code>UInt32</code> Span hash for deduplication <code>begin</code> <code>Timestamp(Nanosecond)</code> Span start time <code>end</code> <code>Timestamp(Nanosecond)</code> Span end time <code>duration</code> <code>Int64</code> Span duration in nanoseconds <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>line</code> <code>UInt32</code> Line number <p>Example Queries: <pre><code>-- Get slowest functions in a stream\nSELECT name, AVG(duration) as avg_duration_ns, COUNT(*) as call_count\nFROM view_instance('thread_spans', 'stream_123')\nWHERE duration &gt; 1000000  -- &gt; 1ms\nGROUP BY name\nORDER BY avg_duration_ns DESC\nLIMIT 10;\n\n-- Analyze call hierarchy\nSELECT depth, name, duration\nFROM view_instance('thread_spans', 'stream_123')\nWHERE parent = 42  -- specific parent span\nORDER BY begin;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#async_events","title":"<code>async_events</code>","text":"<p>Asynchronous span events for tracking async operations with call hierarchy depth information.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Thread stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>time</code> <code>Timestamp(Nanosecond)</code> Event timestamp <code>event_type</code> <code>Dictionary(Int16, Utf8)</code> \"begin\" or \"end\" <code>span_id</code> <code>Int64</code> Async span identifier <code>parent_span_id</code> <code>Int64</code> Parent span identifier <code>depth</code> <code>UInt32</code> Nesting depth in async call hierarchy <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>line</code> <code>UInt32</code> Line number <p>Example Queries:</p> <pre><code>-- Find top-level async operations (depth = 0) with performance metrics\nSELECT\n    name,\n    depth,\n    AVG(duration_ms) as avg_duration,\n    COUNT(*) as operation_count\nFROM (\n    SELECT\n        begin_events.name,\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL AND begin_events.depth = 0\n)\nGROUP BY name, depth\nORDER BY avg_duration DESC;\n\n-- Compare performance by call depth\nSELECT\n    depth,\n    COUNT(*) as span_count,\n    AVG(duration_ms) as avg_duration,\n    MIN(duration_ms) as min_duration,\n    MAX(duration_ms) as max_duration\nFROM (\n    SELECT\n        begin_events.depth,\n        CAST((end_events.time - begin_events.time) AS BIGINT) / 1000000 as duration_ms\n    FROM\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'begin') begin_events\n    LEFT JOIN\n        (SELECT * FROM view_instance('async_events', 'my_process_123') WHERE event_type = 'end') end_events\n        ON begin_events.span_id = end_events.span_id\n    WHERE end_events.span_id IS NOT NULL\n)\nGROUP BY depth\nORDER BY depth;\n\n-- Find operations that spawn many nested async calls\nSELECT\n    name,\n    depth,\n    COUNT(*) as nested_count\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &gt; 0 AND event_type = 'begin'\nGROUP BY name, depth\nHAVING COUNT(*) &gt; 5  -- Functions that create multiple nested async operations\nORDER BY nested_count DESC, depth DESC;\n\n-- Analyze async call hierarchy and parent-child relationships\nSELECT\n    parent.name as parent_operation,\n    parent.depth as parent_depth,\n    child.name as child_operation,\n    child.depth as child_depth,\n    COUNT(*) as relationship_count\nFROM view_instance('async_events', 'my_process_123') parent\nJOIN view_instance('async_events', 'my_process_123') child\n     ON parent.span_id = child.parent_span_id\nWHERE parent.event_type = 'begin' AND child.event_type = 'begin'\nGROUP BY parent.name, parent.depth, child.name, child.depth\nORDER BY relationship_count DESC;\n\n-- Filter async operations by depth level for focused analysis\n-- Shallow operations only (depth &lt;= 2)\nSELECT name, event_type, time, depth, span_id\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &lt;= 2\nORDER BY time;\n\n-- Deep nested operations only (depth &gt;= 3)\nSELECT name, depth, COUNT(*) as deep_operation_count\nFROM view_instance('async_events', 'my_process_123')\nWHERE depth &gt;= 3 AND event_type = 'begin'\nGROUP BY name, depth\nORDER BY depth DESC, deep_operation_count DESC;\n\n-- Track async operation lifecycle with depth context\nSELECT time, event_type, name, span_id, parent_span_id, depth\nFROM view_instance('async_events', 'my_process_123')\nWHERE span_id = 12345\nORDER BY time;\n</code></pre>"},{"location":"query-guide/schema-reference/#data-types","title":"Data Types","text":""},{"location":"query-guide/schema-reference/#properties","title":"Properties","text":"<p>Key-value pairs stored as dictionary-encoded JSONB with the following structure:</p> <pre><code>-- Properties structure (optimized JSONB format)\nDictionary(Int32, Binary)\n</code></pre> <p>This format provides: - Dictionary compression - Repeated property sets stored once and referenced by index - JSONB efficiency - Native binary JSON format for fast property access - Storage optimization - Significant memory and storage savings over legacy formats</p> <p>Common properties fields:</p> <ul> <li><code>properties</code> - Event-specific metadata (log properties, metric properties)</li> <li><code>process_properties</code> - Process-wide metadata shared across all events from a process</li> </ul> <p>Querying properties: <pre><code>-- Access property values using property_get function (works with all formats)\nSELECT property_get(process_properties, 'thread-name') as thread_name\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n\n-- Count properties using properties_length\nSELECT properties_length(properties) as prop_count\nFROM log_entries\nWHERE properties_length(properties) &gt; 0;\n</code></pre></p> <p>Legacy Support: Micromegas maintains full backward compatibility. Existing queries using <code>property_get()</code> and <code>properties_length()</code> work unchanged with the new JSONB format.</p>"},{"location":"query-guide/schema-reference/#dictionary-compression","title":"Dictionary Compression","text":"<p>Most string fields use dictionary compression (<code>Dictionary(Int16, Utf8)</code>) for storage efficiency:</p> <ul> <li>Reduces storage space for repeated values</li> <li>Improves query performance</li> <li>Transparent to SQL queries - use as normal strings</li> </ul>"},{"location":"query-guide/schema-reference/#timestamps","title":"Timestamps","text":"<p>All time fields use <code>Timestamp(Nanosecond)</code> precision:</p> <ul> <li>Nanosecond resolution for high-precision timing</li> <li>UTC timezone assumed</li> <li>Compatible with standard SQL time functions</li> </ul>"},{"location":"query-guide/schema-reference/#view-relationships","title":"View Relationships","text":"<p>Views can be joined to combine information:</p> <pre><code>-- Join log entries with process information\nSELECT l.time, l.level, l.msg, p.exe, p.computer\nFROM log_entries l\nJOIN processes p ON l.process_id = p.process_id\nWHERE l.level &lt;= 2;  -- Fatal and Error only\n\n-- Join measures with stream information\nSELECT m.time, m.name, m.value, s.tags\nFROM measures m\nJOIN streams s ON m.stream_id = s.stream_id\nWHERE m.name = 'cpu_usage';\n</code></pre>"},{"location":"query-guide/schema-reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"query-guide/schema-reference/#dictionary-fields","title":"Dictionary Fields","text":"<p>Dictionary-compressed fields are optimized for:</p> <ul> <li>Equality comparisons (<code>field = 'value'</code>)</li> <li>IN clauses (<code>field IN ('val1', 'val2')</code>)</li> <li>LIKE patterns on repeated values</li> </ul>"},{"location":"query-guide/schema-reference/#time-based-queries","title":"Time-based Queries","text":"<p>Always use time ranges for optimal performance: <pre><code>-- Good - uses time index\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n\n-- Avoid - full table scan\nWHERE level &lt;= 3\n</code></pre></p>"},{"location":"query-guide/schema-reference/#view-instances","title":"View Instances","text":"<p>Use <code>view_instance()</code> for process-specific queries: <pre><code>-- Better performance for single process\nSELECT * FROM view_instance('log_entries', 'process_123')\n\n-- Less efficient for single process\nSELECT * FROM log_entries WHERE process_id = 'process_123'\n</code></pre></p>"},{"location":"query-guide/schema-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Functions Reference - SQL functions available for queries</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> </ul>"},{"location":"unreal/","title":"Unreal Engine Integration","text":"<p>Micromegas provides high-performance observability for Unreal Engine applications through a native integration that captures logs, metrics, and traces with minimal overhead.</p>"},{"location":"unreal/#overview","title":"Overview","text":"<p>The Unreal Engine integration consists of:</p> <ul> <li>MicromegasTracing: Extension to Unreal's Core module providing logging, metrics, and span tracking</li> <li>MicromegasTelemetrySink: Plugin adding HTTP transport for sending telemetry to the ingestion service</li> </ul>"},{"location":"unreal/#key-features","title":"Key Features","text":"<ul> <li>Low Overhead: ~20ns per event, matching the Rust implementation's performance</li> <li>Seamless Integration: Automatically captures existing UE_LOG statements</li> <li>Simple Setup: One header file to include: <code>#include \"MicromegasTracing/Macros.h\"</code></li> <li>Comprehensive Telemetry: Logs, metrics, spans, and crash reporting in a unified system</li> <li>Thread-Safe: Asynchronous delivery without blocking the game thread</li> <li>Context Propagation: Global properties automatically attached to all telemetry</li> </ul>"},{"location":"unreal/#quick-start","title":"Quick Start","text":""},{"location":"unreal/#1-install-the-plugin","title":"1. Install the Plugin","text":"<p>Copy the Unreal modules to your project: - <code>unreal/MicromegasTracing</code> \u2192 Your project's Source folder - <code>unreal/MicromegasTelemetrySink</code> \u2192 Your project's Plugins folder</p>"},{"location":"unreal/#2-initialize-telemetry","title":"2. Initialize Telemetry","text":"<p>In your GameInstance or GameMode:</p> <pre><code>#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n\nvoid AMyGameMode::BeginPlay()\n{\n    Super::BeginPlay();\n\n    // Initialize telemetry\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        \"https://your-telemetry-server:9000\",  // Your ingestion server\n        MyAuthProvider                         // Your authentication handler\n    );\n}\n</code></pre>"},{"location":"unreal/#3-add-instrumentation","title":"3. Add Instrumentation","text":"<pre><code>#include \"MicromegasTracing/Macros.h\"\n\nvoid AMyActor::Tick(float DeltaTime)\n{\n    // Trace function execution\n    MICROMEGAS_SPAN_FUNCTION(\"Game\");\n\n    // Log an event\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Actor ticking\"));\n\n    // Record a metric\n    MICROMEGAS_FMETRIC(\"Game\", MicromegasTracing::Verbosity::Med, \n                       TEXT(\"TickTime\"), TEXT(\"ms\"), DeltaTime * 1000);\n}\n</code></pre>"},{"location":"unreal/#4-view-your-data","title":"4. View Your Data","text":"<p>Once your game is running and generating telemetry:</p> <ul> <li>Query your data: Follow the Query Guide to learn SQL querying and Python API usage</li> <li>Visualize traces: Generate Perfetto traces for detailed performance analysis</li> <li>Build dashboards: Create custom analytics and monitoring dashboards</li> </ul>"},{"location":"unreal/#what-gets-captured","title":"What Gets Captured","text":""},{"location":"unreal/#automatic-telemetry","title":"Automatic Telemetry","text":"<ul> <li>UE_LOG statements: All existing Unreal logs are automatically captured</li> <li>Frame metrics: Delta time, frame rate (when MetricPublisher is active)</li> <li>Memory metrics: Physical and virtual memory usage</li> <li>Map changes: Current level/world tracked in context</li> <li>Crashes: Stack traces and context on Windows (requires debug symbols)</li> </ul>"},{"location":"unreal/#manual-instrumentation","title":"Manual Instrumentation","text":"<ul> <li>Custom spans: Track specific operations and their duration</li> <li>Business metrics: Player counts, game state, performance indicators</li> <li>Custom logs: Direct telemetry logging with structured properties</li> <li>Context properties: Session IDs, user IDs, build versions</li> </ul>"},{"location":"unreal/#architecture","title":"Architecture","text":"<pre><code>Game Code\n    \u2193\n[MicromegasTracing Module]\n    \u251c\u2500 Logging API\n    \u251c\u2500 Metrics API\n    \u251c\u2500 Spans API\n    \u2514\u2500 Default Context\n         \u2193\n[MicromegasTelemetrySink Plugin]\n    \u251c\u2500 HTTP Event Sink\n    \u251c\u2500 Flush Monitor\n    \u251c\u2500 Sampling Controller\n    \u2514\u2500 Crash Reporter\n         \u2193\n[Telemetry Ingestion Server]\n    \u251c\u2500 PostgreSQL (metadata)\n    \u2514\u2500 Object Storage (payloads)\n</code></pre>"},{"location":"unreal/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>Instrumentation API - Complete API reference</li> <li>Examples - Common instrumentation patterns</li> </ul>"},{"location":"unreal/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Spans are disabled by default (enable with <code>telemetry.spans.enable 1</code>)</li> <li>Events are buffered in thread-local storage before async delivery</li> <li>The HTTP sink runs on a dedicated thread</li> <li>Use sampling for high-frequency spans to manage data volume</li> <li>Default context operations are expensive - use for infrequent changes</li> </ul>"},{"location":"unreal/#platform-support","title":"Platform Support","text":"<ul> <li>Windows: Full support including crash reporting</li> <li>Linux: Full support</li> <li>Mac: Full support</li> <li>Consoles: Requires network configuration</li> <li>Mobile: Consider battery and bandwidth optimization</li> </ul>"},{"location":"unreal/examples/","title":"Examples","text":"<p>Practical examples of instrumenting common Unreal Engine scenarios with Micromegas.</p>"},{"location":"unreal/examples/#basic-setup","title":"Basic Setup","text":""},{"location":"unreal/examples/#game-instance-initialization","title":"Game Instance Initialization","text":"<p>Complete setup with authentication and default context:</p> <pre><code>// MyGameInstance.h\nUCLASS()\nclass MYGAME_API UMyGameInstance : public UGameInstance\n{\n    GENERATED_BODY()\n\npublic:\n    virtual void Init() override;\n    virtual void Shutdown() override;\n\nprivate:\n    void SetupTelemetryContext();\n};\n\n// MyGameInstance.cpp\n#include \"MyGameInstance.h\"\n#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n#include \"MicromegasTracing/Macros.h\"\n#include \"MicromegasTracing/Dispatch.h\"\n#include \"MicromegasTracing/DefaultContext.h\"\n\nvoid UMyGameInstance::Init()\n{\n    Super::Init();\n\n    // Create auth provider\n    class FSimpleAuthProvider : public ITelemetryAuthenticator\n    {\n    public:\n        virtual ~FSimpleAuthProvider() = default;\n\n        virtual void Init(const MicromegasTracing::EventSinkPtr&amp; InSink) override {}\n\n        virtual bool IsReady() override { return true; }\n\n        virtual bool Sign(IHttpRequest&amp; Request) override\n        {\n            FString ApiKey = GetDefault&lt;UGameSettings&gt;()-&gt;TelemetryApiKey;\n            Request.SetHeader(TEXT(\"Authorization\"), TEXT(\"Bearer \") + ApiKey);\n            return true;\n        }\n    };\n\n    // Initialize telemetry\n    auto Auth = MakeShared&lt;FSimpleAuthProvider&gt;();\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        TEXT(\"https://telemetry.example.com:9000\"),\n        Auth\n    );\n\n    SetupTelemetryContext();\n\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Game instance initialized\"));\n}\n\nvoid UMyGameInstance::SetupTelemetryContext()\n{\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        // Session info\n        Ctx-&gt;Set(FName(\"session_id\"), FName(*FGuid::NewGuid().ToString()));\n        Ctx-&gt;Set(FName(\"timestamp\"), FName(*FDateTime::UtcNow().ToString()));\n\n        // Build info\n        Ctx-&gt;Set(FName(\"build_version\"), FName(TEXT(GAME_VERSION)));\n        Ctx-&gt;Set(FName(\"build_config\"), FName(TEXT(STRINGIFY(UE_BUILD_CONFIGURATION))));\n\n        // Platform info\n        Ctx-&gt;Set(FName(\"platform\"), FName(*UGameplayStatics::GetPlatformName()));\n        Ctx-&gt;Set(FName(\"cpu\"), FName(*FPlatformMisc::GetCPUBrand()));\n        Ctx-&gt;Set(FName(\"gpu\"), FName(*GRHIAdapterName));\n\n        // Player info (if available)\n        if (ULocalPlayer* LocalPlayer = GetFirstGamePlayer())\n        {\n            Ctx-&gt;Set(FName(\"player_id\"), FName(*LocalPlayer-&gt;GetPreferredUniqueNetId().ToString()));\n        }\n    }\n}\n\nvoid UMyGameInstance::Shutdown()\n{\n    MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Game instance shutting down\"));\n\n    // Force flush before shutdown\n    MicromegasTracing::Dispatch::FlushLogStream();\n    MicromegasTracing::Dispatch::FlushMetricStream();\n    MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n\n    Super::Shutdown();\n}\n</code></pre>"},{"location":"unreal/examples/#game-loop-instrumentation","title":"Game Loop Instrumentation","text":""},{"location":"unreal/examples/#game-mode-with-performance-metrics","title":"Game Mode with Performance Metrics","text":"<pre><code>// MyGameMode.cpp\nvoid AMyGameMode::Tick(float DeltaSeconds)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.GameMode\");\n\n    Super::Tick(DeltaSeconds);\n\n    // Frame metrics\n    MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"FrameTime\"), TEXT(\"ms\"), DeltaSeconds * 1000.0f);\n\n    MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"FPS\"), TEXT(\"fps\"), 1.0f / DeltaSeconds);\n\n    // Game state metrics\n    MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Low,\n                       TEXT(\"PlayerCount\"), TEXT(\"count\"), \n                       GetNumPlayers());\n\n    MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Low,\n                       TEXT(\"AICount\"), TEXT(\"count\"),\n                       GetWorld()-&gt;GetNumPawns() - GetNumPlayers());\n\n    // Memory metrics (every 60 frames)\n    static int32 FrameCounter = 0;\n    if (++FrameCounter % 60 == 0)\n    {\n        FPlatformMemoryStats MemStats = FPlatformMemory::GetStats();\n        MICROMEGAS_IMETRIC(\"Memory\", MicromegasTracing::Verbosity::Low,\n                           TEXT(\"WorkingSetSize\"), TEXT(\"bytes\"),\n                           MemStats.UsedPhysical);\n    }\n}\n\nvoid AMyGameMode::HandleMatchIsWaitingToStart()\n{\n    MICROMEGAS_LOG(\"Game.Match\", MicromegasTracing::LogLevel::Info,\n                   TEXT(\"Match waiting to start\"));\n    Super::HandleMatchIsWaitingToStart();\n}\n\nvoid AMyGameMode::HandleMatchHasStarted()\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.Match\");\n    MICROMEGAS_LOG(\"Game.Match\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Match started on map: %s\"), \n                   *GetWorld()-&gt;GetMapName()));\n\n    // Update context with match info\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"match_id\"), FName(*FGuid::NewGuid().ToString()));\n        Ctx-&gt;Set(FName(\"map\"), FName(*GetWorld()-&gt;GetMapName()));\n        Ctx-&gt;Set(FName(\"game_mode\"), FName(*GetClass()-&gt;GetName()));\n    }\n\n    Super::HandleMatchHasStarted();\n}\n</code></pre>"},{"location":"unreal/examples/#actor-and-component-lifecycle","title":"Actor and Component Lifecycle","text":""},{"location":"unreal/examples/#instrumented-actor","title":"Instrumented Actor","text":"<pre><code>// MyActor.cpp\n#include \"MicromegasTracing/Macros.h\"\n\nvoid AMyActor::BeginPlay()\n{\n    MICROMEGAS_SPAN_UOBJECT(\"Actor.Lifecycle\", this);\n\n    Super::BeginPlay();\n\n    MICROMEGAS_LOG(\"Actor\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"%s spawned at %s\"), \n                   *GetName(), *GetActorLocation().ToString()));\n\n    // Track actor spawns by class\n    MICROMEGAS_IMETRIC(\"Actor\", MicromegasTracing::Verbosity::Med,\n                       *FString::Printf(TEXT(\"Spawned.%s\"), *GetClass()-&gt;GetName()),\n                       TEXT(\"count\"), 1);\n}\n\nvoid AMyActor::Tick(float DeltaTime)\n{\n    // Only trace tick for important actors\n    if (bIsImportant)\n    {\n        MICROMEGAS_SPAN_UOBJECT(\"Actor.Tick\", this);\n        Super::Tick(DeltaTime);\n\n        // Actor-specific logic...\n    }\n    else\n    {\n        Super::Tick(DeltaTime);\n    }\n}\n\nvoid AMyActor::EndPlay(const EEndPlayReason::Type EndPlayReason)\n{\n    MICROMEGAS_LOG(\"Actor\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"%s destroyed: %s\"), \n                   *GetName(), \n                   *UEnum::GetValueAsString(EndPlayReason)));\n\n    Super::EndPlay(EndPlayReason);\n}\n\nfloat AMyActor::TakeDamage(float DamageAmount, FDamageEvent const&amp; DamageEvent,\n                           AController* EventInstigator, AActor* DamageCauser)\n{\n    float ActualDamage = Super::TakeDamage(DamageAmount, DamageEvent, \n                                           EventInstigator, DamageCauser);\n\n    MICROMEGAS_LOG(\"Combat\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"%s took %.1f damage from %s\"), \n                   *GetName(), ActualDamage,\n                   DamageCauser ? *DamageCauser-&gt;GetName() : TEXT(\"Unknown\")));\n\n    MICROMEGAS_FMETRIC(\"Combat\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"DamageDealt\"), TEXT(\"points\"), ActualDamage);\n\n    return ActualDamage;\n}\n</code></pre>"},{"location":"unreal/examples/#player-controller","title":"Player Controller","text":""},{"location":"unreal/examples/#player-actions-and-input","title":"Player Actions and Input","text":"<pre><code>// MyPlayerController.cpp\nvoid AMyPlayerController::BeginPlay()\n{\n    Super::BeginPlay();\n\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        // Set player-specific context\n        Ctx-&gt;Set(FName(\"player_name\"), FName(*PlayerState-&gt;GetPlayerName()));\n        Ctx-&gt;Set(FName(\"player_id\"), FName(*GetUniqueID().ToString()));\n    }\n\n    MICROMEGAS_LOG(\"Player\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Player %s joined\"), \n                   *PlayerState-&gt;GetPlayerName()));\n}\n\nvoid AMyPlayerController::SetupInputComponent()\n{\n    Super::SetupInputComponent();\n\n    InputComponent-&gt;BindAction(\"Fire\", IE_Pressed, this, &amp;AMyPlayerController::OnFire);\n    InputComponent-&gt;BindAction(\"Jump\", IE_Pressed, this, &amp;AMyPlayerController::OnJump);\n    InputComponent-&gt;BindAction(\"Interact\", IE_Pressed, this, &amp;AMyPlayerController::OnInteract);\n}\n\nvoid AMyPlayerController::OnFire()\n{\n    MICROMEGAS_LOG(\"Player.Input\", MicromegasTracing::LogLevel::Trace,\n                   TEXT(\"Fire action\"));\n    MICROMEGAS_IMETRIC(\"Player.Actions\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"Fire\"), TEXT(\"count\"), 1);\n\n    // Fire logic...\n}\n\nvoid AMyPlayerController::OnJump()\n{\n    MICROMEGAS_IMETRIC(\"Player.Actions\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"Jump\"), TEXT(\"count\"), 1);\n    // Jump logic...\n}\n\nvoid AMyPlayerController::OnInteract()\n{\n    MICROMEGAS_SPAN_SCOPE(\"Player.Interaction\", \"Interact\");\n\n    FHitResult HitResult;\n    if (GetHitResultUnderCursor(ECC_Pawn, false, HitResult))\n    {\n        if (AActor* HitActor = HitResult.GetActor())\n        {\n            MICROMEGAS_LOG(\"Player.Interaction\", MicromegasTracing::LogLevel::Info,\n                           FString::Printf(TEXT(\"Interacting with %s\"), \n                           *HitActor-&gt;GetName()));\n\n            // Interaction logic...\n        }\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#network-replication","title":"Network Replication","text":""},{"location":"unreal/examples/#network-metrics-and-events","title":"Network Metrics and Events","text":"<pre><code>// MyGameState.cpp\nvoid AMyGameState::Tick(float DeltaSeconds)\n{\n    Super::Tick(DeltaSeconds);\n\n    // Network metrics (every second)\n    TimeSinceLastNetworkUpdate += DeltaSeconds;\n    if (TimeSinceLastNetworkUpdate &gt;= 1.0f)\n    {\n        TimeSinceLastNetworkUpdate = 0.0f;\n\n        if (UNetDriver* NetDriver = GetWorld()-&gt;GetNetDriver())\n        {\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"ClientConnections\"), TEXT(\"count\"),\n                               NetDriver-&gt;ClientConnections.Num());\n\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"TotalNetObjects\"), TEXT(\"count\"),\n                               NetDriver-&gt;GetNetworkObjectList().GetObjects().Num());\n\n            // Bandwidth metrics\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"InBytes\"), TEXT(\"bytes\"),\n                               NetDriver-&gt;InBytes);\n\n            MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"OutBytes\"), TEXT(\"bytes\"),\n                               NetDriver-&gt;OutBytes);\n        }\n    }\n}\n\n// RPC tracking\nvoid AMyGameState::ServerRPC_Implementation(const FString&amp; Data)\n{\n    MICROMEGAS_SPAN_SCOPE(\"Network.RPC\", \"ServerRPC\");\n    MICROMEGAS_IMETRIC(\"Network.RPC\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ServerCalls\"), TEXT(\"count\"), 1);\n\n    // Process RPC...\n}\n\nvoid AMyGameState::ClientRPC_Implementation(const FString&amp; Data)\n{\n    MICROMEGAS_IMETRIC(\"Network.RPC\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ClientCalls\"), TEXT(\"count\"), 1);\n\n    // Process RPC...\n}\n\nvoid AMyGameState::OnRep_ReplicatedProperty()\n{\n    MICROMEGAS_IMETRIC(\"Network.Replication\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"PropertyUpdates\"), TEXT(\"count\"), 1);\n}\n</code></pre>"},{"location":"unreal/examples/#asset-loading-and-streaming","title":"Asset Loading and Streaming","text":""},{"location":"unreal/examples/#content-loading-instrumentation","title":"Content Loading Instrumentation","text":"<pre><code>// MyAssetManager.cpp\nvoid UMyAssetManager::LoadAssetAsync(const FString&amp; AssetPath)\n{\n    MICROMEGAS_SPAN_NAME(\"Content.AsyncLoad\", *AssetPath);\n\n    MICROMEGAS_LOG(\"Content\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"Loading asset: %s\"), *AssetPath));\n\n    FStreamableManager&amp; Streamable = UAssetManager::GetStreamableManager();\n    TSharedPtr&lt;FStreamableHandle&gt; Handle = Streamable.RequestAsyncLoad(\n        FSoftObjectPath(AssetPath),\n        FStreamableDelegate::CreateLambda([AssetPath]()\n        {\n            MICROMEGAS_LOG(\"Content\", MicromegasTracing::LogLevel::Debug,\n                           FString::Printf(TEXT(\"Asset loaded: %s\"), *AssetPath));\n\n            MICROMEGAS_IMETRIC(\"Content\", MicromegasTracing::Verbosity::Med,\n                               TEXT(\"AssetsLoaded\"), TEXT(\"count\"), 1);\n        })\n    );\n}\n\nvoid UMyAssetManager::OnLevelStreamingComplete(ULevelStreaming* StreamedLevel)\n{\n    if (StreamedLevel &amp;&amp; StreamedLevel-&gt;GetLoadedLevel())\n    {\n        int64 SizeBytes = StreamedLevel-&gt;GetLoadedLevel()-&gt;GetOutermost()-&gt;GetFileSize();\n\n        MICROMEGAS_LOG(\"Content.Streaming\", MicromegasTracing::LogLevel::Info,\n                       FString::Printf(TEXT(\"Level streamed: %s (%.2f MB)\"), \n                       *StreamedLevel-&gt;GetWorldAssetPackageFName().ToString(),\n                       SizeBytes / (1024.0f * 1024.0f)));\n\n        MICROMEGAS_IMETRIC(\"Content.Streaming\", MicromegasTracing::Verbosity::Low,\n                           TEXT(\"LevelSize\"), TEXT(\"bytes\"), SizeBytes);\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#ai-and-behavior-trees","title":"AI and Behavior Trees","text":""},{"location":"unreal/examples/#ai-controller-instrumentation","title":"AI Controller Instrumentation","text":"<pre><code>// MyAIController.cpp\nvoid AMyAIController::RunBehaviorTree(UBehaviorTree* BTAsset)\n{\n    MICROMEGAS_SPAN_SCOPE(\"AI.BehaviorTree\", \"RunTree\");\n\n    MICROMEGAS_LOG(\"AI\", MicromegasTracing::LogLevel::Debug,\n                   FString::Printf(TEXT(\"Starting behavior tree: %s\"), \n                   *BTAsset-&gt;GetName()));\n\n    return Super::RunBehaviorTree(BTAsset);\n}\n\nvoid AMyAIController::OnMoveCompleted(FAIRequestID RequestID, EPathFollowingResult::Type Result)\n{\n    MICROMEGAS_LOG(\"AI.Movement\", MicromegasTracing::LogLevel::Trace,\n                   FString::Printf(TEXT(\"AI move completed: %s\"), \n                   *UEnum::GetValueAsString(Result)));\n\n    MICROMEGAS_IMETRIC(\"AI.Movement\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"MovesCompleted\"), TEXT(\"count\"), 1);\n\n    Super::OnMoveCompleted(RequestID, Result);\n}\n\n// BTTask instrumentation\nEBTNodeResult::Type UMyBTTask::ExecuteTask(UBehaviorTreeComponent&amp; OwnerComp, uint8* NodeMemory)\n{\n    MICROMEGAS_SPAN_SCOPE(\"AI.BTTask\", GetNodeName());\n\n    EBTNodeResult::Type Result = Super::ExecuteTask(OwnerComp, NodeMemory);\n\n    MICROMEGAS_LOG(\"AI.BehaviorTree\", MicromegasTracing::LogLevel::Trace,\n                   FString::Printf(TEXT(\"Task %s: %s\"), \n                   *GetNodeName(),\n                   *UEnum::GetValueAsString(Result)));\n\n    return Result;\n}\n</code></pre>"},{"location":"unreal/examples/#profiling-critical-paths","title":"Profiling Critical Paths","text":""},{"location":"unreal/examples/#render-thread-instrumentation","title":"Render Thread Instrumentation","text":"<pre><code>// MySceneProxy.cpp\nvoid FMySceneProxy::GetDynamicMeshElements(...)\n{\n    MICROMEGAS_SPAN_SCOPE(\"Render.SceneProxy\", \"GetDynamicMeshElements\");\n\n    // Expensive rendering operations\n    MICROMEGAS_IMETRIC(\"Render\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"DynamicElements\"), TEXT(\"count\"), Elements.Num());\n}\n</code></pre>"},{"location":"unreal/examples/#physics-simulation","title":"Physics Simulation","text":"<pre><code>// MyPhysicsActor.cpp\nvoid AMyPhysicsActor::SimulatePhysics(float DeltaTime)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Physics.Simulation\");\n\n    double StartTime = FPlatformTime::Seconds();\n\n    // Run physics simulation\n    RunComplexPhysicsSimulation(DeltaTime);\n\n    double SimTime = (FPlatformTime::Seconds() - StartTime) * 1000.0;\n    MICROMEGAS_FMETRIC(\"Physics\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"SimulationTime\"), TEXT(\"ms\"), SimTime);\n\n    if (SimTime &gt; 16.0) // Longer than a frame\n    {\n        MICROMEGAS_LOG(\"Physics\", MicromegasTracing::LogLevel::Warn,\n                       FString::Printf(TEXT(\"Physics simulation took %.2fms\"), SimTime));\n    }\n}\n</code></pre>"},{"location":"unreal/examples/#error-handling-and-debugging","title":"Error Handling and Debugging","text":""},{"location":"unreal/examples/#comprehensive-error-logging","title":"Comprehensive Error Logging","text":"<pre><code>void UMyGameSubsystem::HandleError(const FString&amp; ErrorContext, const FString&amp; ErrorMessage)\n{\n    // Log the error\n    MICROMEGAS_LOG(\"Error\", MicromegasTracing::LogLevel::Error,\n                   FString::Printf(TEXT(\"[%s] %s\"), *ErrorContext, *ErrorMessage));\n\n    // Track error metrics\n    MICROMEGAS_IMETRIC(\"Errors\", MicromegasTracing::Verbosity::Low,\n                       *FString::Printf(TEXT(\"Error.%s\"), *ErrorContext),\n                       TEXT(\"count\"), 1);\n\n    // Add error to context for correlation\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"last_error\"), FName(*ErrorMessage));\n        Ctx-&gt;Set(FName(\"error_time\"), FName(*FDateTime::UtcNow().ToString()));\n    }\n\n    // Force flush for critical errors\n    if (IsCriticalError(ErrorContext))\n    {\n        MicromegasTracing::Dispatch::FlushLogStream();\n        MicromegasTracing::Dispatch::FlushMetricStream();\n        MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n    }\n}\n\n// Assertion handler\nvoid CheckGameState(bool bCondition, const FString&amp; Message)\n{\n    if (!bCondition)\n    {\n        MICROMEGAS_LOG(\"Assert\", MicromegasTracing::LogLevel::Fatal,\n                       FString::Printf(TEXT(\"Assertion failed: %s\"), *Message));\n\n        // Flush before potential crash\n        MicromegasTracing::Dispatch::FlushLogStream();\n        MicromegasTracing::Dispatch::FlushMetricStream();\n        MicromegasTracing::Dispatch::FlushCurrentThreadStream();\n\n        check(false);\n    }\n}\n</code></pre>"},{"location":"unreal/installation/","title":"Installation Guide","text":"<p>This guide walks through installing and configuring the Micromegas Unreal Engine integration.</p>"},{"location":"unreal/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Unreal Engine 4.27+ or 5.0+</li> <li>Visual Studio 2019 or 2022 (Windows)</li> <li>Xcode (Mac)</li> <li>A running Micromegas ingestion server</li> </ul>"},{"location":"unreal/installation/#standard-installation","title":"Standard Installation","text":""},{"location":"unreal/installation/#step-1-copy-the-modules","title":"Step 1: Copy the Modules","text":"<ol> <li> <p>Copy the Core module extension to Unreal's Core module:    <pre><code>micromegas/unreal/MicromegasTracing/Public/MicromegasTracing \u2192 \nYourUnrealEngine/Engine/Source/Runtime/Core/Public/MicromegasTracing\n\nmicromegas/unreal/MicromegasTracing/Private \u2192\nYourUnrealEngine/Engine/Source/Runtime/Core/Private/MicromegasTracing\n</code></pre></p> </li> <li> <p>Copy the plugin to your project:    <pre><code>micromegas/unreal/MicromegasTelemetrySink \u2192 YourProject/Plugins/MicromegasTelemetrySink\n</code></pre></p> </li> </ol>"},{"location":"unreal/installation/#step-2-configure-build-dependencies","title":"Step 2: Configure Build Dependencies","text":"<p>Since MicromegasTracing is now part of the Core module, you only need to add the plugin:</p> <pre><code>// YourGame.Build.cs\npublic class YourGame : ModuleRules\n{\n    public YourGame(ReadOnlyTargetRules Target) : base(Target)\n    {\n        PublicDependencyModuleNames.AddRange(new string[] { \n            \"Core\",  // MicromegasTracing is now included in Core\n            \"CoreUObject\", \n            \"Engine\"\n        });\n\n        PrivateDependencyModuleNames.AddRange(new string[] {\n            \"MicromegasTelemetrySink\"  // Add this plugin\n        });\n    }\n}\n</code></pre>"},{"location":"unreal/installation/#step-3-enable-the-plugin","title":"Step 3: Enable the Plugin","text":"<p>Either:</p> <ul> <li>Via Editor: Go to Edit \u2192 Plugins \u2192 Search for \"MicromegasTelemetrySink\" \u2192 Enable</li> <li>Via .uproject: Add to the Plugins section:   <pre><code>{\n  \"Name\": \"MicromegasTelemetrySink\",\n  \"Enabled\": true\n}\n</code></pre></li> </ul>"},{"location":"unreal/installation/#step-4-build-the-project","title":"Step 4: Build the Project","text":"<p>Regenerate project files and build:</p> <ol> <li>Right-click your <code>.uproject</code> file \u2192 \"Generate Visual Studio project files\"</li> <li>Open the solution in Visual Studio/Xcode</li> <li>Build the project</li> </ol>"},{"location":"unreal/installation/#development-setup-windows","title":"Development Setup (Windows)","text":"<p>For active development on Micromegas while testing in Unreal, use hard links to avoid copying files:</p>"},{"location":"unreal/installation/#step-1-set-environment-variables","title":"Step 1: Set Environment Variables","text":"<pre><code>set MICROMEGAS_UNREAL_ROOT_DIR=C:\\Program Files\\Epic Games\\UE_5.3\nset MICROMEGAS_UNREAL_TELEMETRY_MODULE_DIR=C:\\YourProject\\Plugins\n</code></pre>"},{"location":"unreal/installation/#step-2-run-the-hard-link-script","title":"Step 2: Run the Hard Link Script","text":"<pre><code>cd micromegas/build\npython unreal_hard_link_windows.py\n</code></pre> <p>This creates hard links that:</p> <ul> <li>Link MicromegasTracing into Unreal Engine's Core module</li> <li>Link MicromegasTelemetrySink plugin to your project  </li> <li>Allow you to edit Micromegas source and see changes immediately in Unreal</li> </ul>"},{"location":"unreal/installation/#initial-configuration","title":"Initial Configuration","text":""},{"location":"unreal/installation/#basic-setup","title":"Basic Setup","text":"<p>In your <code>GameInstance</code> or <code>GameMode</code> class:</p> <pre><code>// YourGameInstance.h\n#pragma once\n#include \"Engine/GameInstance.h\"\n#include \"YourGameInstance.generated.h\"\n\nUCLASS()\nclass YOURGAME_API UYourGameInstance : public UGameInstance\n{\n    GENERATED_BODY()\n\npublic:\n    virtual void Init() override;\n};\n\n// YourGameInstance.cpp\n#include \"YourGameInstance.h\"\n#include \"MicromegasTelemetrySink/MicromegasTelemetrySinkModule.h\"\n#include \"MicromegasTracing/Dispatch.h\"\n#include \"MicromegasTracing/DefaultContext.h\"\n\nvoid UYourGameInstance::Init()\n{\n    Super::Init();\n\n    // Initialize telemetry\n    FString ServerUrl = TEXT(\"https://telemetry.yourcompany.com:9000\");\n\n    // Create authentication provider (implement your auth logic)\n    auto AuthProvider = MakeShared&lt;FMyTelemetryAuthenticator&gt;();\n\n    // Initialize the sink\n    IMicromegasTelemetrySinkModule::LoadModuleChecked().InitTelemetry(\n        ServerUrl, \n        AuthProvider\n    );\n\n    // Set default context properties\n    if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n    {\n        Ctx-&gt;Set(FName(\"build_version\"), FName(TEXT(\"1.0.0\")));\n        Ctx-&gt;Set(FName(\"platform\"), FName(*UGameplayStatics::GetPlatformName()));\n        Ctx-&gt;Set(FName(\"session_id\"), FName(*FGuid::NewGuid().ToString()));\n    }\n\n    UE_LOG(LogTemp, Log, TEXT(\"Telemetry initialized\"));\n}\n</code></pre>"},{"location":"unreal/installation/#authentication-provider","title":"Authentication Provider","text":"<p>Implement the authentication interface:</p> <pre><code>// MyTelemetryAuthenticator.h\n#pragma once\n#include \"MicromegasTelemetrySink/TelemetryAuthenticator.h\"\n#include \"Interfaces/IHttpRequest.h\"\n\nclass FMyTelemetryAuthenticator : public ITelemetryAuthenticator\n{\npublic:\n    virtual ~FMyTelemetryAuthenticator() = default;\n\n    virtual void Init(const MicromegasTracing::EventSinkPtr&amp; InSink) override\n    {\n        // Initialize authenticator if needed\n    }\n\n    virtual bool IsReady() override\n    {\n        // Return true when authentication is ready\n        return true;\n    }\n\n    virtual bool Sign(IHttpRequest&amp; Request) override\n    {\n        // Add authentication to the HTTP request\n        Request.SetHeader(TEXT(\"Authorization\"), TEXT(\"Bearer your-api-key-here\"));\n        return true;\n    }\n};\n</code></pre>"},{"location":"unreal/installation/#configuration-options","title":"Configuration Options","text":""},{"location":"unreal/installation/#compile-time-settings","title":"Compile-Time Settings","text":"<p>In <code>MicromegasTelemetrySinkModule.cpp</code>:</p> <pre><code>// Enable telemetry on startup (default: 1)\n#define MICROMEGAS_ENABLE_TELEMETRY_ON_START 1\n\n// Enable crash reporting on Windows (default: 1 on Windows, 0 elsewhere)\n#define MICROMEGAS_CRASH_REPORTING 1\n</code></pre>"},{"location":"unreal/installation/#runtime-console-commands","title":"Runtime Console Commands","text":"<p>Available console commands for runtime control:</p> <ul> <li><code>telemetry.enable</code> - Initialize the telemetry system</li> <li><code>telemetry.flush</code> - Force flush all pending events</li> <li><code>telemetry.spans.enable 1</code> - Enable span recording (disabled by default)</li> <li><code>telemetry.spans.enable 0</code> - Disable span recording</li> <li><code>telemetry.spans.all 1</code> - Record all spans without sampling</li> </ul>"},{"location":"unreal/installation/#verifying-installation","title":"Verifying Installation","text":""},{"location":"unreal/installation/#test-basic-logging","title":"Test Basic Logging","text":"<p>Add to any Actor or GameMode:</p> <pre><code>#include \"MicromegasTracing/Macros.h\"\n\nvoid ATestActor::BeginPlay()\n{\n    Super::BeginPlay();\n\n    // This should appear in your telemetry\n    MICROMEGAS_LOG(\"Test\", MicromegasTracing::LogLevel::Info, \n                   TEXT(\"Micromegas telemetry is working!\"));\n\n    // This metric should be recorded\n    MICROMEGAS_IMETRIC(\"Test\", MicromegasTracing::Verbosity::Med, \n                       TEXT(\"TestCounter\"), TEXT(\"count\"), 1);\n}\n</code></pre>"},{"location":"unreal/installation/#check-server-connection","title":"Check Server Connection","text":"<ol> <li>Run your game in the editor</li> <li>Open the console (` key)</li> <li>Type: <code>telemetry.flush</code></li> <li>Check your ingestion server logs for incoming requests</li> <li>Query your data using the Python client or CLI tools</li> </ol>"},{"location":"unreal/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"unreal/installation/#windows","title":"Windows","text":"<ul> <li>Crash reporting is enabled by default</li> <li>Requires debug symbols (<code>.pdb</code> files) for meaningful stack traces</li> <li>Windows Defender may flag the first network connection - add an exception if needed</li> </ul>"},{"location":"unreal/installation/#linux","title":"Linux","text":"<ul> <li>Ensure your ingestion server is accessible from the game server</li> <li>Check firewall rules for port 9000 (or your configured port)</li> </ul>"},{"location":"unreal/installation/#mac","title":"Mac","text":"<ul> <li>Code signing may be required for shipping builds</li> <li>Network permissions needed for telemetry upload</li> </ul>"},{"location":"unreal/installation/#consoles","title":"Consoles","text":"<ul> <li>Special network configuration required</li> <li>Contact your platform representative for network policy compliance</li> </ul>"},{"location":"unreal/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Instrumentation API - Learn about logging, metrics, and spans</li> <li>Examples - See common usage patterns</li> </ul>"},{"location":"unreal/instrumentation-api/","title":"Instrumentation API Reference","text":"<p>Complete reference for the Micromegas Unreal Engine instrumentation API.</p>"},{"location":"unreal/instrumentation-api/#header-file","title":"Header File","text":"<p>All instrumentation macros are available by including:</p> <pre><code>#include \"MicromegasTracing/Macros.h\"\n</code></pre>"},{"location":"unreal/instrumentation-api/#logging-api","title":"Logging API","text":""},{"location":"unreal/instrumentation-api/#micromegas_log","title":"MICROMEGAS_LOG","text":"<p>Records a log entry with dynamic string content.</p> <pre><code>MICROMEGAS_LOG(target, level, message)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Log target/category (e.g., \"Game\", \"Network\", \"AI\")</li> <li><code>level</code> (MicromegasTracing::LogLevel): Severity level</li> <li><code>message</code> (FString): The log message</li> </ul> <p>Log Levels:</p> <ul> <li><code>MicromegasTracing::LogLevel::Fatal</code> - Critical errors causing shutdown</li> <li><code>MicromegasTracing::LogLevel::Error</code> - Errors requiring attention</li> <li><code>MicromegasTracing::LogLevel::Warn</code> - Warning conditions</li> <li><code>MicromegasTracing::LogLevel::Info</code> - Informational messages</li> <li><code>MicromegasTracing::LogLevel::Debug</code> - Debug information</li> <li><code>MicromegasTracing::LogLevel::Trace</code> - Detailed trace information</li> </ul> <p>Example: <pre><code>MICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, \n               TEXT(\"Player connected\"));\n\nMICROMEGAS_LOG(\"Network\", MicromegasTracing::LogLevel::Error,\n               FString::Printf(TEXT(\"Connection failed: %s\"), *ErrorMessage));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_log_properties","title":"MICROMEGAS_LOG_PROPERTIES","text":"<p>Records a log entry with additional structured properties.</p> <pre><code>MICROMEGAS_LOG_PROPERTIES(target, level, properties, message)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Log target/category</li> <li><code>level</code> (MicromegasTracing::LogLevel): Severity level</li> <li><code>properties</code> (PropertySet*): Additional key-value properties</li> <li><code>message</code> (FString): The log message</li> </ul> <p>Example: <pre><code>PropertySet* Props = CreatePropertySet();\nProps-&gt;Add(\"player_id\", \"12345\");\nProps-&gt;Add(\"action\", \"login\");\n\nMICROMEGAS_LOG_PROPERTIES(\"Game\", MicromegasTracing::LogLevel::Info, \n                         Props, TEXT(\"Player action recorded\"));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#ue_log-integration","title":"UE_LOG Integration","text":"<p>All existing <code>UE_LOG</code> statements are automatically captured by Micromegas when the log interop is initialized. No code changes required.</p> <pre><code>// These are automatically sent to telemetry\nUE_LOG(LogTemp, Warning, TEXT(\"This is captured by Micromegas\"));\nUE_LOG(LogGameMode, Error, TEXT(\"So is this\"));\n</code></pre>"},{"location":"unreal/instrumentation-api/#metrics-api","title":"Metrics API","text":""},{"location":"unreal/instrumentation-api/#micromegas_imetric","title":"MICROMEGAS_IMETRIC","text":"<p>Records an integer metric value.</p> <pre><code>MICROMEGAS_IMETRIC(target, level, name, unit, expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Metric target/category</li> <li><code>level</code> (MicromegasTracing::Verbosity): Verbosity level</li> <li><code>name</code> (const TCHAR*): Metric name</li> <li><code>unit</code> (const TCHAR*): Unit of measurement</li> <li><code>expression</code> (int64): Value or expression to record</li> </ul> <p>Verbosity Levels:</p> <ul> <li><code>MicromegasTracing::Verbosity::Low</code> - Critical metrics only</li> <li><code>MicromegasTracing::Verbosity::Med</code> - Standard metrics</li> <li><code>MicromegasTracing::Verbosity::High</code> - Detailed metrics</li> </ul> <p>Common Units:</p> <ul> <li><code>TEXT(\"count\")</code> - Simple counter</li> <li><code>TEXT(\"bytes\")</code> - Memory/data size</li> <li><code>TEXT(\"ms\")</code> - Milliseconds</li> <li><code>TEXT(\"percent\")</code> - Percentage (0-100)</li> <li><code>TEXT(\"ticks\")</code> - Will be automatically converted into nanoseconds</li> </ul> <p>Example: <pre><code>MICROMEGAS_IMETRIC(\"Game\", MicromegasTracing::Verbosity::Med,\n                   TEXT(\"PlayerCount\"), TEXT(\"count\"), \n                   GetWorld()-&gt;GetNumPlayerControllers());\n\nMICROMEGAS_IMETRIC(\"Memory\", MicromegasTracing::Verbosity::Low,\n                   TEXT(\"TextureMemory\"), TEXT(\"bytes\"),\n                   GetTextureMemoryUsage());\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_fmetric","title":"MICROMEGAS_FMETRIC","text":"<p>Records a floating-point metric value.</p> <pre><code>MICROMEGAS_FMETRIC(target, level, name, unit, expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Metric target/category</li> <li><code>level</code> (MicromegasTracing::Verbosity): Verbosity level</li> <li><code>name</code> (const TCHAR*): Metric name</li> <li><code>unit</code> (const TCHAR*): Unit of measurement</li> <li><code>expression</code> (double): Value or expression to record</li> </ul> <p>Example: <pre><code>MICROMEGAS_FMETRIC(\"Performance\", MicromegasTracing::Verbosity::Med,\n                   TEXT(\"FrameTime\"), TEXT(\"ms\"), \n                   DeltaTime * 1000.0);\n\nMICROMEGAS_FMETRIC(\"Game\", MicromegasTracing::Verbosity::High,\n                   TEXT(\"HealthPercent\"), TEXT(\"percent\"),\n                   (Health / MaxHealth) * 100.0);\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#spanstracing-api","title":"Spans/Tracing API","text":"<p>Important: Spans are disabled by default. Enable with console command: <code>telemetry.spans.enable 1</code>. Use reasonable sampling strategy for high-frequency spans.</p>"},{"location":"unreal/instrumentation-api/#micromegas_span_function","title":"MICROMEGAS_SPAN_FUNCTION","text":"<p>Traces the current function's execution time using the function name as the span name.</p> <pre><code>MICROMEGAS_SPAN_FUNCTION(target)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> </ul> <p>Example: <pre><code>void AMyActor::ComplexCalculation()\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Game.Physics\");\n    // Function is automatically traced\n    // ... complex physics calculations ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_scope","title":"MICROMEGAS_SPAN_SCOPE","text":"<p>Creates a named scope span with a static name.</p> <pre><code>MICROMEGAS_SPAN_SCOPE(target, name)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>name</code> (const char*): Static span name</li> </ul> <p>Example: <pre><code>void ProcessAI()\n{\n    {\n        MICROMEGAS_SPAN_SCOPE(\"AI\", \"Pathfinding\");\n        // ... pathfinding code ...\n    }\n\n    {\n        MICROMEGAS_SPAN_SCOPE(\"AI\", \"DecisionTree\");\n        // ... decision tree evaluation ...\n    }\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_name","title":"MICROMEGAS_SPAN_NAME","text":"<p>Creates a span with a dynamic name (must be statically allocated).</p> <pre><code>MICROMEGAS_SPAN_NAME(target, name_expression)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>name_expression</code>: Expression returning a statically allocated string (e.g., FName)</li> </ul> <p>Example: <pre><code>void ProcessAsset(const FString&amp; AssetPath)\n{\n    FName AssetName(*AssetPath);\n    MICROMEGAS_SPAN_NAME(\"Content\", AssetName);\n    // ... process asset ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_uobject","title":"MICROMEGAS_SPAN_UOBJECT","text":"<p>Creates a span named after a UObject.</p> <pre><code>MICROMEGAS_SPAN_UOBJECT(target, object)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>object</code> (UObject*): The UObject whose name to use</li> </ul> <p>Example: <pre><code>void AMyActor::Tick(float DeltaTime)\n{\n    MICROMEGAS_SPAN_UOBJECT(\"Game.Actors\", this);\n    Super::Tick(DeltaTime);\n    // ... tick logic ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#micromegas_span_name_conditional","title":"MICROMEGAS_SPAN_NAME_CONDITIONAL","text":"<p>Creates a span conditionally.</p> <pre><code>MICROMEGAS_SPAN_NAME_CONDITIONAL(target, condition, name)\n</code></pre> <p>Parameters:</p> <ul> <li><code>target</code> (const char*): Span target/category</li> <li><code>condition</code> (bool): Whether to create the span</li> <li><code>name</code>: Span name if condition is true</li> </ul> <p>Example: <pre><code>void RenderFrame(bool bDetailedProfiling)\n{\n    MICROMEGAS_SPAN_NAME_CONDITIONAL(\"Render\", bDetailedProfiling, \n                                      TEXT(\"DetailedFrame\"));\n    // ... rendering code ...\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#default-context-api","title":"Default Context API","text":"<p>The Default Context allows setting global properties that are automatically attached to all telemetry.</p>"},{"location":"unreal/instrumentation-api/#accessing-the-default-context","title":"Accessing the Default Context","text":"<pre><code>MicromegasTracing::DefaultContext* Ctx = \n    MicromegasTracing::Dispatch::GetDefaultContext();\n</code></pre>"},{"location":"unreal/instrumentation-api/#set","title":"Set","text":"<p>Adds or updates a context property.</p> <pre><code>void Set(FName Key, FName Value)\n</code></pre> <p>Example: <pre><code>if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n{\n    Ctx-&gt;Set(FName(\"user_id\"), FName(*UserId));\n    Ctx-&gt;Set(FName(\"session_id\"), FName(*SessionId));\n    Ctx-&gt;Set(FName(\"map\"), FName(*GetWorld()-&gt;GetMapName()));\n}\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#unset","title":"Unset","text":"<p>Removes a context property.</p> <pre><code>void Unset(FName Key)\n</code></pre> <p>Example: <pre><code>Ctx-&gt;Unset(FName(\"temp_flag\"));\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#clear","title":"Clear","text":"<p>Removes all context properties.</p> <pre><code>void Clear()\n</code></pre> <p>Example: <pre><code>// Clear context on logout\nCtx-&gt;Clear();\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#copy","title":"Copy","text":"<p>Copies current context to a map.</p> <pre><code>void Copy(TMap&lt;FName, FName&gt;&amp; Out) const\n</code></pre> <p>Example: <pre><code>TMap&lt;FName, FName&gt; CurrentContext;\nCtx-&gt;Copy(CurrentContext);\n// Examine or log current context\n</code></pre></p>"},{"location":"unreal/instrumentation-api/#console-commands","title":"Console Commands","text":"<p>Runtime control commands available in the Unreal console:</p>"},{"location":"unreal/instrumentation-api/#telemetryenable","title":"telemetry.enable","text":"<p>Initializes the telemetry system if not already enabled.</p> <pre><code>telemetry.enable\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryflush","title":"telemetry.flush","text":"<p>Forces immediate flush of all pending telemetry events.</p> <pre><code>telemetry.flush\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryspansenable","title":"telemetry.spans.enable","text":"<p>Enables or disables span recording.</p> <pre><code>telemetry.spans.enable 1  // Enable spans\ntelemetry.spans.enable 0  // Disable spans\n</code></pre>"},{"location":"unreal/instrumentation-api/#telemetryspansall","title":"telemetry.spans.all","text":"<p>Enables recording of all spans without sampling.</p> <pre><code>telemetry.spans.all 1  // Record all spans\ntelemetry.spans.all 0  // Use sampling\n</code></pre>"},{"location":"unreal/instrumentation-api/#best-practices","title":"Best Practices","text":""},{"location":"unreal/instrumentation-api/#performance","title":"Performance","text":"<ol> <li>Use sampling for high-frequency spans - It's OK to keep spans enabled with reasonable sampling</li> <li>Use appropriate verbosity - Lower verbosity for high-frequency metrics</li> <li>Batch operations - Let the system batch; avoid frequent flushes</li> <li>Static strings - Use TEXT() macro for string literals</li> <li>Limit context cardinality - Context keys/values are never freed</li> </ol>"},{"location":"unreal/instrumentation-api/#error-handling","title":"Error Handling","text":"<p>Always check for null pointers when using the context API:</p> <pre><code>if (auto* Ctx = MicromegasTracing::Dispatch::GetDefaultContext())\n{\n    // Safe to use Ctx\n    Ctx-&gt;Set(FName(\"key\"), FName(\"value\"));\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#thread-safety","title":"Thread Safety","text":"<p>All Micromegas APIs are thread-safe and can be called from any thread:</p> <pre><code>// Safe from game thread\nMICROMEGAS_LOG(\"Game\", MicromegasTracing::LogLevel::Info, TEXT(\"Game thread\"));\n\n// Safe from render thread\nMICROMEGAS_LOG(\"Render\", MicromegasTracing::LogLevel::Info, TEXT(\"Render thread\"));\n\n// Safe from worker threads\nParallelFor(NumItems, [](int32 Index)\n{\n    MICROMEGAS_IMETRIC(\"Worker\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"ItemProcessed\"), TEXT(\"count\"), 1);\n});\n</code></pre>"},{"location":"unreal/instrumentation-api/#integration-examples","title":"Integration Examples","text":""},{"location":"unreal/instrumentation-api/#with-gameplay-abilities","title":"With Gameplay Abilities","text":"<pre><code>void UMyGameplayAbility::ActivateAbility(...)\n{\n    MICROMEGAS_SPAN_NAME(\"Abilities\", GetFName());\n    MICROMEGAS_LOG(\"Abilities\", MicromegasTracing::LogLevel::Info,\n                   FString::Printf(TEXT(\"Ability %s activated\"), *GetName()));\n\n    Super::ActivateAbility(...);\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#with-animation","title":"With Animation","text":"<pre><code>void UAnimInstance::NativeUpdateAnimation(float DeltaSeconds)\n{\n    MICROMEGAS_SPAN_FUNCTION(\"Animation\");\n    MICROMEGAS_FMETRIC(\"Animation\", MicromegasTracing::Verbosity::High,\n                       TEXT(\"UpdateTime\"), TEXT(\"ms\"), DeltaSeconds * 1000);\n\n    Super::NativeUpdateAnimation(DeltaSeconds);\n}\n</code></pre>"},{"location":"unreal/instrumentation-api/#with-networking","title":"With Networking","text":"<pre><code>void AMyPlayerController::ClientRPC_Implementation()\n{\n    MICROMEGAS_LOG(\"Network\", MicromegasTracing::LogLevel::Debug,\n                   TEXT(\"RPC received\"));\n    MICROMEGAS_IMETRIC(\"Network\", MicromegasTracing::Verbosity::Med,\n                       TEXT(\"RPCCount\"), TEXT(\"count\"), 1);\n}\n</code></pre>"}]}