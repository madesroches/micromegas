{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Micromegas Documentation","text":"<p>Welcome to Micromegas, a unified observability platform designed for high-performance telemetry collection and analytics.</p>"},{"location":"#what-is-micromegas","title":"What is Micromegas?","text":"<p>Micromegas is a comprehensive observability solution that provides:</p> <ul> <li>High-performance instrumentation with 20ns overhead per event</li> <li>Unified data collection for logs, metrics, spans, and traces</li> <li>Cost-efficient storage using object storage for raw data</li> <li>Powerful SQL analytics built on Apache DataFusion</li> <li>Real-time and historical analysis capabilities</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#high-performance","title":"\ud83d\ude80 High Performance","text":"<ul> <li>Ultra-low overhead telemetry collection (20ns per event)</li> <li>Supports up to 100k events/second per process</li> <li>Thread-local storage for minimal performance impact</li> </ul>"},{"location":"#cost-effective","title":"\ud83d\udcb0 Cost Effective","text":"<ul> <li>Raw data stored in cheap object storage (S3/GCS)</li> <li>Metadata in PostgreSQL for fast queries</li> <li>Pay only for what you query with on-demand ETL</li> </ul>"},{"location":"#unified-observability","title":"\ud83d\udd0d Unified Observability","text":"<ul> <li>Logs, metrics, traces, and spans in a single queryable format</li> <li>SQL interface compatible with existing analytics tools</li> <li>Grafana plugin for visualization and dashboards</li> </ul>"},{"location":"#modern-architecture","title":"\ud83c\udfd7\ufe0f Modern Architecture","text":"<ul> <li>Apache Arrow FlightSQL for efficient data transfer</li> <li>DataFusion-powered analytics engine</li> <li>Lakehouse architecture with Parquet optimization</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with Micromegas in just a few steps:</p> <ol> <li>Getting Started Guide - Set up your first Micromegas installation</li> <li>Query Guide - Learn how to query your observability data</li> <li>Architecture Overview - Understand the system design</li> </ol>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#application-performance-monitoring","title":"Application Performance Monitoring","text":"<p>Monitor your applications with detailed performance metrics, error tracking, and distributed tracing.</p>"},{"location":"#infrastructure-observability","title":"Infrastructure Observability","text":"<p>Collect and analyze system metrics, logs, and performance data across your entire infrastructure.</p>"},{"location":"#cost-effective-analytics","title":"Cost-Effective Analytics","text":"<p>Store massive amounts of telemetry data cost-effectively while maintaining fast query performance.</p>"},{"location":"#development-debugging","title":"Development &amp; Debugging","text":"<p>Use high-frequency instrumentation to debug performance issues and understand application behavior.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the guides in this documentation</li> <li>GitHub Issues: Report bugs or request features</li> <li>Community: Join discussions and get support</li> </ul>"},{"location":"#license","title":"License","text":"<p>Micromegas is open source software. See the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing to Micromegas","text":"<p>We welcome contributions to the Micromegas project! This guide will help you get started with contributing code, documentation, or reporting issues.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to maintain a respectful and inclusive environment for all contributors.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre></p> </li> <li> <p>Set up development environment:    Follow the Getting Started Guide to set up your local environment.</p> </li> <li> <p>Install development dependencies:    <pre><code># Rust development\ncd rust\ncargo build\n\n# Python development\ncd python/micromegas\npoetry install\n\n# Documentation\npip install -r docs/docs-requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#before-you-start","title":"Before You Start","text":"<ol> <li>Check existing issues on GitHub Issues</li> <li>Open an issue to discuss your proposed changes if it's a significant feature</li> <li>Fork the repository and create a feature branch</li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></p> </li> <li> <p>Follow coding standards (see AI Guidelines):</p> </li> <li>Rust: Run <code>cargo fmt</code> before any commit</li> <li>Python: Use <code>black</code> for formatting</li> <li>Dependencies: Keep alphabetical order in Cargo.toml</li> <li> <p>Error handling: Use <code>expect()</code> with descriptive messages instead of <code>unwrap()</code></p> </li> <li> <p>Write tests:    <pre><code># Rust tests\ncd rust\ncargo test\n\n# Python tests\ncd python/micromegas\npytest\n</code></pre></p> </li> <li> <p>Run CI pipeline locally:    <pre><code>cd rust\npython3 ../build/rust_ci.py\n</code></pre></p> </li> <li> <p>Commit with clear messages:    <pre><code>git commit -m \"Add histogram generation for span duration analysis\"\n</code></pre></p> </li> </ol> <p>Note: Never include AI-generated credits or co-author tags in commit messages.</p> <ol> <li>Push and create Pull Request:    <pre><code>git push origin feature/your-feature-name\n</code></pre></li> </ol> <p>Before creating a PR, run:    <pre><code>git log --oneline main..HEAD\n</code></pre>    to list all commits in your branch.</p>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Automated checks: Ensure all CI checks pass</li> <li>Code review: Maintainers will review your changes</li> <li>Address feedback: Make requested changes if needed</li> <li>Merge: Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#contributing-documentation","title":"Contributing Documentation","text":""},{"location":"contributing/#mkdocs-documentation","title":"MkDocs Documentation","text":"<p>The main documentation uses MkDocs with Material theme:</p> <ol> <li>Edit documentation:</li> <li>Files are in <code>docs/</code> directory</li> <li>Use Markdown format</li> <li> <p>Follow existing structure and style</p> </li> <li> <p>Preview changes:    <pre><code>mkdocs serve\n# Visit http://localhost:8000\n</code></pre></p> </li> <li> <p>Build documentation:    <pre><code>python docs/build-docs.py\n</code></pre></p> </li> </ol>"},{"location":"contributing/#documentation-guidelines","title":"Documentation Guidelines","text":"<ul> <li>Clear and concise: Write for your audience</li> <li>Code examples: Include working examples with expected output</li> <li>Cross-references: Link to related sections</li> <li>Consistent formatting: Follow existing patterns</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include the following information:</p> <ul> <li>Environment: OS, Rust version, Python version</li> <li>Micromegas version: Git commit or release version</li> <li>Steps to reproduce: Clear, minimal reproduction steps</li> <li>Expected vs actual behavior: What should happen vs what happens</li> <li>Logs/errors: Include relevant error messages or logs</li> <li>Configuration: Relevant environment variables or config</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<ul> <li>Use case: Describe the problem you're trying to solve</li> <li>Proposed solution: Your suggested approach</li> <li>Alternatives: Other approaches you've considered</li> <li>Impact: Who would benefit from this feature</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#architecture-understanding","title":"Architecture Understanding","text":"<p>Familiarize yourself with the architecture overview:</p> <ul> <li>High-performance instrumentation (20ns overhead)</li> <li>Lakehouse architecture with object storage</li> <li>DataFusion-powered analytics</li> <li>FlightSQL protocol for efficient data transfer</li> </ul>"},{"location":"contributing/#key-areas-for-contribution","title":"Key Areas for Contribution","text":"<ol> <li>Core Rust Libraries:</li> <li>Tracing instrumentation improvements</li> <li>Analytics engine enhancements</li> <li> <p>Performance optimizations</p> </li> <li> <p>Services:</p> </li> <li>Ingestion service features</li> <li>FlightSQL server improvements</li> <li> <p>Admin CLI enhancements</p> </li> <li> <p>Client Libraries:</p> </li> <li>Python API improvements</li> <li> <p>New language bindings</p> </li> <li> <p>Documentation:</p> </li> <li>Query examples and patterns</li> <li>Performance guidance</li> <li> <p>Integration guides</p> </li> <li> <p>Testing:</p> </li> <li>Unit test coverage</li> <li>Integration tests</li> <li>Performance benchmarks</li> </ol>"},{"location":"contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Benchmarking: Include benchmarks for performance-critical changes</li> <li>Memory usage: Consider memory implications of new features</li> <li>Backwards compatibility: Maintain API compatibility when possible</li> </ul>"},{"location":"contributing/#security","title":"Security","text":"<ul> <li>No secrets in code: Never commit API keys, passwords, or tokens</li> <li>Input validation: Validate all external inputs</li> <li>Dependencies: Keep dependencies updated and minimal</li> </ul>"},{"location":"contributing/#community","title":"Community","text":""},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check the documentation first</li> <li>GitHub Issues: Search existing issues before creating new ones</li> <li>Discussions: Use GitHub Discussions for questions and ideas</li> </ul>"},{"location":"contributing/#stay-updated","title":"Stay Updated","text":"<ul> <li>Watch the repository for updates</li> <li>Follow releases for new features and bug fixes</li> <li>Join discussions about future directions</li> </ul>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - Git commit history - Release notes for significant contributions - Special thanks in documentation</p> <p>Thank you for contributing to Micromegas! Your contributions help make observability more accessible and cost-effective for everyone.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will walk you through setting up Micromegas on your local workstation for testing and development purposes.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Docker - For running PostgreSQL</li> <li>Python 3.8+ - For the client API and setup scripts</li> <li>Rust - For building Micromegas services</li> </ul>"},{"location":"getting-started/#environment-setup","title":"Environment Setup","text":"<p>Set the following environment variables for local development:</p> <pre><code># Database credentials (used by setup scripts)\nexport MICROMEGAS_DB_USERNAME=your_username\nexport MICROMEGAS_DB_PASSWD=your_password\n\n# Service endpoints\nexport MICROMEGAS_TELEMETRY_URL=http://localhost:9000\nexport MICROMEGAS_SQL_CONNECTION_STRING=postgres://your_username:your_password@localhost:5432\n\n# Object storage (replace with your local path)\nexport MICROMEGAS_OBJECT_STORE_URI=file:///path/to/local/storage\n</code></pre> <p>Object Storage Path</p> <p>Choose a local directory for object storage, e.g., <code>/tmp/micromegas-storage</code> or <code>C:\\temp\\micromegas-storage</code> on Windows.</p>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre>"},{"location":"getting-started/#2-start-postgresql-database","title":"2. Start PostgreSQL Database","text":"<p>Start a local PostgreSQL instance using Docker:</p> <pre><code>cd local_test_env/db\npython run.py\n</code></pre> <p>This will: - Pull the PostgreSQL Docker image - Start the database container - Set up the initial schema</p>"},{"location":"getting-started/#3-start-core-services","title":"3. Start Core Services","text":"<p>You'll need to start three services in separate terminals:</p>"},{"location":"getting-started/#terminal-1-ingestion-server","title":"Terminal 1: Ingestion Server","text":"<pre><code>cd rust\ncargo run -p telemetry-ingestion-srv -- --listen-endpoint-http 127.0.0.1:9000\n</code></pre>"},{"location":"getting-started/#terminal-2-flightsql-server","title":"Terminal 2: FlightSQL Server","text":"<pre><code>cd rust\ncargo run -p flight-sql-srv -- --disable-auth\n</code></pre>"},{"location":"getting-started/#terminal-3-maintenance-daemon","title":"Terminal 3: Maintenance Daemon","text":"<pre><code>cd rust\ncargo run -p telemetry-admin -- crond\n</code></pre> <p>Service Roles</p> <ul> <li>Ingestion Server: Receives telemetry data from applications</li> <li>FlightSQL Server: Provides SQL query interface for analytics</li> <li>Maintenance Daemon: Handles background processing and view materialization</li> </ul>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":""},{"location":"getting-started/#install-python-client","title":"Install Python Client","text":"<pre><code>pip install micromegas\n</code></pre>"},{"location":"getting-started/#test-with-sample-query","title":"Test with Sample Query","text":"<p>Create a test script to verify everything is working:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to local Micromegas instance\nclient = micromegas.connect()\n\n# Set up time range for query\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(days=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute query and display results\ndf = client.query(sql, begin, end)\nprint(f\"Found {len(df)} log entries\")\nprint(df.head())\n</code></pre> <p>If you see a DataFrame with log entries (or an empty DataFrame if no data has been ingested yet), your installation is working correctly!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have Micromegas running locally, you can:</p> <ol> <li>Learn to Query Data - Explore the SQL interface and available data</li> <li>Understand the Architecture - Learn how Micromegas components work together</li> <li>Instrument Your Application - Start collecting telemetry from your own applications</li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#common-issues","title":"Common Issues","text":"<p>Connection refused when querying : Make sure all three services are running and the FlightSQL server is listening on the default port.</p> <p>Database connection errors : Verify your PostgreSQL container is running and the connection string environment variable is correct.</p> <p>Empty query results : This is normal for a fresh installation. You'll need to instrument an application to start collecting telemetry data.</p> <p>Build errors : Ensure you have the latest Rust toolchain installed and try <code>cargo update</code> in the <code>rust/</code> directory.</p>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the service logs in each terminal for error messages</li> <li>Verify all environment variables are set correctly</li> <li>Create an issue on GitHub with details about your setup</li> </ol>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>Micromegas is built on a modern lakehouse architecture designed for high-performance observability data collection and analytics.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#data-collection","title":"Data Collection","text":"<ul> <li>Tracing Library: Ultra-low overhead (20ns per event) instrumentation</li> <li>Telemetry Sink: Event collection and transmission</li> <li>HTTP Gateway: REST API for telemetry ingestion</li> </ul>"},{"location":"architecture/#data-storage","title":"Data Storage","text":"<ul> <li>PostgreSQL: Metadata, process information, and stream definitions</li> <li>Object Storage: Raw telemetry data in efficient binary format</li> <li>Lakehouse: Materialized Parquet views for fast analytics</li> </ul>"},{"location":"architecture/#analytics-engine","title":"Analytics Engine","text":"<ul> <li>DataFusion: SQL query engine with vectorized execution</li> <li>FlightSQL: High-performance query protocol</li> <li>Apache Arrow: Columnar data format for efficient transfer</li> </ul>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Instrumentation: Applications emit telemetry events</li> <li>Collection: Events batched and sent to ingestion service</li> <li>Storage: Metadata in PostgreSQL, payloads in object storage</li> <li>Materialization: Views created on-demand from raw data</li> <li>Query: SQL interface provides analytics capabilities</li> </ol>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<ul> <li>High-frequency collection: Support for 100k+ events/second per process</li> <li>Cost-efficient storage: Cheap object storage for raw data</li> <li>On-demand processing: ETL only when querying data</li> <li>Unified observability: Logs, metrics, and traces in single format</li> </ul> <p>More architecture details coming soon...</p>"},{"location":"development/build/","title":"Build Guide","text":"<p>This guide covers building Micromegas from source and setting up a development environment.</p>"},{"location":"development/build/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust - Latest stable version</li> <li>Python 3.8+</li> <li>Docker - For running PostgreSQL</li> <li>Git</li> </ul>"},{"location":"development/build/#building-from-source","title":"Building from Source","text":""},{"location":"development/build/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/madesroches/micromegas.git\ncd micromegas\n</code></pre>"},{"location":"development/build/#2-build-rust-components","title":"2. Build Rust Components","text":"<pre><code>cd rust\n\n# Build all components\ncargo build\n\n# Build with optimizations\ncargo build --release\n\n# Build specific component\ncargo build -p telemetry-ingestion-srv\n</code></pre>"},{"location":"development/build/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\ncargo test\n\n# Run tests with output\ncargo test -- --nocapture\n\n# Run specific test\ncargo test -p micromegas-tracing\n</code></pre>"},{"location":"development/build/#4-format-and-lint","title":"4. Format and Lint","text":"<pre><code># Format code (required before commits)\ncargo fmt\n\n# Run linter\ncargo clippy --workspace -- -D warnings\n\n# Run full CI pipeline\npython3 ../build/rust_ci.py\n</code></pre>"},{"location":"development/build/#python-client-development","title":"Python Client Development","text":""},{"location":"development/build/#1-set-up-environment","title":"1. Set Up Environment","text":"<pre><code>cd python/micromegas\n\n# Install Poetry (if not already installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install dependencies\npoetry install\n\n# Activate virtual environment\npoetry shell\n</code></pre>"},{"location":"development/build/#2-run-tests","title":"2. Run Tests","text":"<pre><code># Run tests\npytest\n\n# Run with coverage\npytest --cov=micromegas\n\n# Run specific test file\npytest tests/test_client.py\n</code></pre>"},{"location":"development/build/#3-format-code","title":"3. Format Code","text":"<pre><code># Format with black (required before commits)\nblack .\n\n# Check formatting\nblack --check .\n</code></pre>"},{"location":"development/build/#documentation-development","title":"Documentation Development","text":""},{"location":"development/build/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Install MkDocs and theme\npip install -r docs/docs-requirements.txt\n\n# Or use the build script\npython docs/build-docs.py\n</code></pre>"},{"location":"development/build/#2-development-server","title":"2. Development Server","text":"<pre><code># Start development server\nmkdocs serve\n\n# Visit http://localhost:8000\n# Changes automatically reload\n</code></pre>"},{"location":"development/build/#3-build-static-site","title":"3. Build Static Site","text":"<pre><code># Build documentation\nmkdocs build\n\n# Output in site/ directory\npython -m http.server 8000 --directory site\n</code></pre>"},{"location":"development/build/#ide-setup","title":"IDE Setup","text":""},{"location":"development/build/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - rust-analyzer - Rust language support - Python - Python language support - Even Better TOML - TOML file support - Error Lens - Inline error display</p>"},{"location":"development/build/#settings","title":"Settings","text":"<p>Add to <code>.vscode/settings.json</code>: <pre><code>{\n    \"rust-analyzer.cargo.features\": \"all\",\n    \"python.defaultInterpreterPath\": \"./python/micromegas/.venv/bin/python\",\n    \"python.formatting.provider\": \"black\"\n}\n</code></pre></p>"},{"location":"development/build/#common-build-tasks","title":"Common Build Tasks","text":""},{"location":"development/build/#full-clean-build","title":"Full Clean Build","text":"<pre><code># Clean Rust build artifacts\ncd rust\ncargo clean\n\n# Clean Python artifacts\ncd ../python/micromegas\npoetry env remove --all\npoetry install\n</code></pre>"},{"location":"development/build/#release-build","title":"Release Build","text":"<pre><code>cd rust\n\n# Build optimized release\ncargo build --release\n\n# Run optimized tests\ncargo test --release\n</code></pre>"},{"location":"development/build/#cross-platform-build","title":"Cross-Platform Build","text":"<pre><code># Add target\nrustup target add x86_64-pc-windows-gnu\n\n# Build for target\ncargo build --target x86_64-pc-windows-gnu\n</code></pre>"},{"location":"development/build/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/build/#common-issues","title":"Common Issues","text":"<p>Rust compilation errors: - Ensure you have the latest stable Rust: <code>rustup update</code> - Clear build cache: <code>cargo clean</code></p> <p>Python dependency conflicts: - Remove and recreate environment: <code>poetry env remove --all &amp;&amp; poetry install</code></p> <p>Database connection issues: - Ensure PostgreSQL container is running - Check environment variables are set correctly</p> <p>Permission errors on Windows: - Run PowerShell as Administrator - Use Windows Subsystem for Linux (WSL)</p>"},{"location":"development/build/#build-environment","title":"Build Environment","text":"<p>Check your build environment:</p> <pre><code># Rust version\nrustc --version\n\n# Cargo version  \ncargo --version\n\n# Python version\npython --version\n\n# Docker version\ndocker --version\n</code></pre>"},{"location":"development/build/#performance-builds","title":"Performance Builds","text":""},{"location":"development/build/#optimized-release","title":"Optimized Release","text":"<pre><code>cd rust\n\n# Maximum optimization\ncargo build --release\n\n# With debug symbols for profiling\ncargo build --profile release-debug\n</code></pre>"},{"location":"development/build/#profiling-build","title":"Profiling Build","text":"<pre><code># Build with profiling\ncargo build --profile profiling\n\n# Enable specific features\ncargo build --features \"profiling,metrics\"\n</code></pre>"},{"location":"development/build/#continuous-integration","title":"Continuous Integration","text":"<p>The CI pipeline runs:</p> <ol> <li>Format check: <code>cargo fmt --check</code></li> <li>Linting: <code>cargo clippy --workspace -- -D warnings</code> </li> <li>Tests: <code>cargo test --workspace</code></li> <li>Documentation: <code>cargo doc --workspace</code></li> </ol> <p>Run locally with: <pre><code>cd rust\npython3 ../build/rust_ci.py\n</code></pre></p>"},{"location":"development/build/#next-steps","title":"Next Steps","text":"<ul> <li>Contributing Guide - How to contribute to the project</li> <li>Getting Started - Set up a development instance</li> <li>Architecture Overview - Understand the system design</li> </ul>"},{"location":"query-guide/","title":"Query Guide Overview","text":"<p>Micromegas provides a powerful SQL interface for querying observability data including logs, metrics, spans, and traces. Micromegas SQL is an extension of Apache DataFusion SQL - you can use all standard DataFusion SQL features plus Micromegas-specific functions and views optimized for observability workloads.</p>"},{"location":"query-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"query-guide/#sql-engine","title":"SQL Engine","text":"<p>Micromegas uses Apache DataFusion as its SQL engine, which means you get:</p> <ul> <li>Full SQL standard compliance</li> <li>Advanced query optimization</li> <li>Vectorized execution engine</li> <li>Columnar data processing with Apache Arrow</li> </ul>"},{"location":"query-guide/#data-architecture","title":"Data Architecture","text":"<ul> <li>Raw data stored in object storage (S3/GCS) in Parquet format</li> <li>Metadata stored in PostgreSQL for fast lookups</li> <li>Views provide logical organization of telemetry data</li> <li>On-demand ETL processes data only when queried</li> </ul>"},{"location":"query-guide/#available-interfaces","title":"Available Interfaces","text":""},{"location":"query-guide/#python-api","title":"Python API","text":"<p>The primary interface for querying Micromegas data programmatically. All queries return pandas DataFrames, making it easy to work with results using the pandas ecosystem:</p> <pre><code>import micromegas\nclient = micromegas.connect()\ndf = client.query(\"SELECT * FROM log_entries LIMIT 10;\")\n</code></pre>"},{"location":"query-guide/#grafana-plugin","title":"Grafana Plugin","text":"<p>Use the same SQL capabilities in Grafana dashboards through the Micromegas Grafana plugin.</p>"},{"location":"query-guide/#direct-flightsql","title":"Direct FlightSQL","text":"<p>Advanced users can connect directly to the FlightSQL endpoint using Apache Arrow clients.</p>"},{"location":"query-guide/#data-views","title":"Data Views","text":"<p>Micromegas organizes telemetry data into several queryable views:</p> View Description <code>processes</code> Process metadata and system information <code>streams</code> Data stream information within processes <code>log_entries</code> Application log messages with levels and context <code>measures</code> Numeric metrics and performance measurements <code>thread_spans</code> Synchronous execution spans and timing <code>async_events</code> Asynchronous event lifecycle tracking"},{"location":"query-guide/#query-capabilities","title":"Query Capabilities","text":""},{"location":"query-guide/#standard-sql-features","title":"Standard SQL Features","text":"<ul> <li>SELECT, FROM, WHERE, ORDER BY, GROUP BY</li> <li>JOINs between views</li> <li>Aggregation functions (COUNT, SUM, AVG, etc.)</li> <li>Window functions and CTEs</li> <li>Complex filtering and sorting</li> </ul>"},{"location":"query-guide/#observability-extensions","title":"Observability Extensions","text":"<ul> <li>Time-range filtering for performance</li> <li>Process-scoped view instances</li> <li>Histogram generation functions</li> <li>Log level filtering and analysis</li> <li>Span relationship queries</li> </ul>"},{"location":"query-guide/#performance-features","title":"Performance Features","text":"<ul> <li>Query streaming for large datasets</li> <li>Predicate pushdown to storage layer</li> <li>Automatic view materialization</li> <li>Memory-efficient processing</li> </ul>"},{"location":"query-guide/#getting-started","title":"Getting Started","text":"<ol> <li>Quick Start - Basic queries to get you started</li> <li>Python API - Complete API reference and examples</li> <li>Schema Reference - Detailed view and field documentation</li> <li>Functions Reference - Available SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Advanced Features - View materialization and custom views</li> </ol>"},{"location":"query-guide/#best-practices","title":"Best Practices","text":""},{"location":"query-guide/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<p>For performance and memory efficiency, always specify time ranges in your queries:</p> <pre><code># Good - uses time range\ndf = client.query(sql, begin_time, end_time)\n\n# Avoid - queries all data\ndf = client.query(sql)  # Can be slow and memory-intensive\n</code></pre>"},{"location":"query-guide/#start-simple","title":"Start Simple","text":"<p>Begin with basic queries and add complexity incrementally:</p> <pre><code>-- Start with this\nSELECT * FROM log_entries LIMIT 10;\n\n-- Then add filtering\nSELECT * FROM log_entries WHERE level &lt;= 3 LIMIT 10;\n\n-- Then add time range\nSELECT * FROM log_entries \nWHERE level &lt;= 3 AND time &gt;= NOW() - INTERVAL '1 hour'\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/#use-process-scoped-views","title":"Use Process-Scoped Views","text":"<p>For better performance when analyzing specific processes:</p> <pre><code>-- Instead of filtering the global view\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n\n-- Use a process-scoped view instance\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre> <p>Ready to start querying? Head to the Quick Start guide!</p>"},{"location":"query-guide/advanced-features/","title":"Advanced Features","text":"<p>Advanced Micromegas features including view materialization, custom views, and system administration.</p>"},{"location":"query-guide/advanced-features/#view-materialization","title":"View Materialization","text":"<p>Micromegas uses a lakehouse architecture with on-demand view materialization for optimal performance.</p>"},{"location":"query-guide/advanced-features/#jit-view-processing","title":"JIT View Processing","text":"<ul> <li>Raw data stored in object storage (S3/GCS)</li> <li>Views materialized on-demand when queried</li> <li>Automatic caching for frequently accessed data</li> </ul>"},{"location":"query-guide/advanced-features/#global-views-vs-view-instances","title":"Global Views vs View Instances","text":"<ul> <li>Global views: <code>log_entries</code>, <code>measures</code> - span all processes</li> <li>View instances: <code>view_instance('log_entries', 'process_id')</code> - process-scoped for performance</li> </ul>"},{"location":"query-guide/advanced-features/#architecture-benefits","title":"Architecture Benefits","text":""},{"location":"query-guide/advanced-features/#datalake-lakehouse-query","title":"Datalake \u2192 Lakehouse \u2192 Query","text":"<ul> <li>Datalake (S3): Custom binary format, cheap storage, fast writes</li> <li>Lakehouse (Parquet): Columnar format, fast analytics, industry standard</li> <li>Query Engine (DataFusion): SQL engine optimized for analytical workloads</li> </ul>"},{"location":"query-guide/advanced-features/#tail-sampling-support","title":"Tail Sampling Support","text":"<ul> <li>Heavy data streams remain unprocessed until queried</li> <li>Cheap to store in S3, cheap to delete unused data</li> <li>Use low-frequency streams (logs, metrics) to decide sampling of high-frequency streams (spans)</li> </ul> <p>More advanced features documentation coming soon...</p>"},{"location":"query-guide/functions-reference/","title":"Functions Reference","text":"<p>This page provides a complete reference to all SQL functions available in Micromegas queries, including both standard DataFusion functions and Micromegas-specific extensions.</p>"},{"location":"query-guide/functions-reference/#micromegas-extensions","title":"Micromegas Extensions","text":""},{"location":"query-guide/functions-reference/#table-functions","title":"Table Functions","text":"<p>Table functions return tables that can be used in FROM clauses.</p>"},{"location":"query-guide/functions-reference/#view_instanceview_name-identifier","title":"<code>view_instance(view_name, identifier)</code>","text":"<p>Creates a process or stream-scoped view instance for better performance.</p> <p>Syntax: <pre><code>view_instance(view_name, identifier)\n</code></pre></p> <p>Parameters: - <code>view_name</code> (<code>Utf8</code>): Name of the view ('log_entries', 'measures', 'thread_spans', 'async_events') - <code>identifier</code> (<code>Utf8</code>): Process ID (for most views) or Stream ID (for thread_spans)</p> <p>Returns: Schema depends on the view type (see Schema Reference)</p> <p>Examples: <pre><code>-- Get logs for a specific process\nSELECT time, level, msg\nFROM view_instance('log_entries', 'my_process_123')\nWHERE level &lt;= 3;\n\n-- Get spans for a specific stream\nSELECT name, duration\nFROM view_instance('thread_spans', 'stream_456')\nWHERE duration &gt; 1000000;  -- &gt; 1ms\n</code></pre></p>"},{"location":"query-guide/functions-reference/#list_partitions","title":"<code>list_partitions()</code>","text":"<p>Lists available data partitions in the lakehouse.</p> <p>Syntax: <pre><code>SELECT * FROM list_partitions()\n</code></pre></p> <p>Returns: | Column | Type | Description | |--------|------|-------------| | <code>view_set_name</code> | <code>Utf8</code> | Name of the view set | | <code>view_instance_id</code> | <code>Utf8</code> | Instance identifier | | <code>begin_insert_time</code> | <code>Timestamp(Nanosecond)</code> | Partition start time | | <code>end_insert_time</code> | <code>Timestamp(Nanosecond)</code> | Partition end time | | <code>min_event_time</code> | <code>Timestamp(Nanosecond)</code> | Earliest event time | | <code>max_event_time</code> | <code>Timestamp(Nanosecond)</code> | Latest event time | | <code>updated</code> | <code>Timestamp(Nanosecond)</code> | Last update time | | <code>file_path</code> | <code>Utf8</code> | Partition file path | | <code>file_size</code> | <code>Int64</code> | File size in bytes |</p> <p>Example: <pre><code>-- View partition information\nSELECT view_set_name, view_instance_id, file_size\nFROM list_partitions()\nORDER BY updated DESC;\n</code></pre></p>"},{"location":"query-guide/functions-reference/#scalar-functions","title":"Scalar Functions","text":""},{"location":"query-guide/functions-reference/#property_getproperties-key","title":"<code>property_get(properties, key)</code>","text":"<p>Extracts a value from a properties map.</p> <p>Syntax: <pre><code>property_get(properties, key)\n</code></pre></p> <p>Parameters: - <code>properties</code> (<code>List&lt;Struct&gt;</code>): Properties map field - <code>key</code> (<code>Utf8</code>): Property key to extract</p> <p>Returns: <code>Utf8</code> - Property value or NULL if not found</p> <p>Examples: <pre><code>-- Get thread name from process properties\nSELECT time, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n\n-- Filter by custom property\nSELECT time, name, value\nFROM measures\nWHERE property_get(properties, 'source') = 'system_monitor';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#make_histogramvalues-bins","title":"<code>make_histogram(values, bins)</code>","text":"<p>Creates histogram data from numeric values.</p> <p>Syntax: <pre><code>make_histogram(values, bins)\n</code></pre></p> <p>Parameters: - <code>values</code> (<code>Float64</code>): Column of numeric values - <code>bins</code> (<code>Int32</code>): Number of histogram bins</p> <p>Returns: Histogram structure with buckets and counts</p> <p>Example: <pre><code>-- Create histogram of response times\nSELECT make_histogram(duration, 20) as duration_histogram\nFROM view_instance('thread_spans', 'web_server_123')\nWHERE name = 'handle_request';\n</code></pre></p>"},{"location":"query-guide/functions-reference/#standard-sql-functions","title":"Standard SQL Functions","text":"<p>Micromegas supports all standard DataFusion SQL functions. Here are the most commonly used categories:</p>"},{"location":"query-guide/functions-reference/#time-functions","title":"Time Functions","text":""},{"location":"query-guide/functions-reference/#now","title":"<code>NOW()</code>","text":"<p>Returns the current timestamp.</p> <pre><code>SELECT NOW() as current_time;\n</code></pre>"},{"location":"query-guide/functions-reference/#date_truncprecision-timestamp","title":"<code>date_trunc(precision, timestamp)</code>","text":"<p>Truncates timestamp to specified precision.</p> <p>Precisions: <code>year</code>, <code>month</code>, <code>day</code>, <code>hour</code>, <code>minute</code>, <code>second</code></p> <pre><code>-- Group by hour\nSELECT date_trunc('hour', time) as hour, COUNT(*) as log_count\nFROM log_entries\nGROUP BY date_trunc('hour', time);\n</code></pre>"},{"location":"query-guide/functions-reference/#interval","title":"<code>INTERVAL</code>","text":"<p>Creates time intervals for date arithmetic.</p> <pre><code>-- Last 24 hours\nWHERE time &gt;= NOW() - INTERVAL '24 hours'\n\n-- Last week\nWHERE time &gt;= NOW() - INTERVAL '7 days'\n\n-- Custom intervals\nWHERE time &gt;= NOW() - INTERVAL '30 minutes'\n</code></pre>"},{"location":"query-guide/functions-reference/#aggregation-functions","title":"Aggregation Functions","text":""},{"location":"query-guide/functions-reference/#count","title":"<code>COUNT(*)</code>","text":"<p>Counts all rows.</p> <pre><code>SELECT COUNT(*) as total_logs FROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#countcolumn","title":"<code>COUNT(column)</code>","text":"<p>Counts non-null values in a column.</p> <pre><code>SELECT COUNT(msg) as non_null_messages FROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#sumcolumn","title":"<code>SUM(column)</code>","text":"<p>Sums numeric values.</p> <pre><code>SELECT SUM(value) as total_memory FROM measures WHERE name = 'memory_usage';\n</code></pre>"},{"location":"query-guide/functions-reference/#avgcolumn","title":"<code>AVG(column)</code>","text":"<p>Calculates average of numeric values.</p> <pre><code>SELECT AVG(duration) as avg_duration FROM view_instance('thread_spans', 'process_123');\n</code></pre>"},{"location":"query-guide/functions-reference/#mincolumn-maxcolumn","title":"<code>MIN(column)</code> / <code>MAX(column)</code>","text":"<p>Finds minimum and maximum values.</p> <pre><code>SELECT MIN(time) as earliest, MAX(time) as latest FROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#stddevcolumn","title":"<code>STDDEV(column)</code>","text":"<p>Calculates standard deviation.</p> <pre><code>SELECT STDDEV(value) as memory_variance FROM measures WHERE name = 'memory_usage';\n</code></pre>"},{"location":"query-guide/functions-reference/#string-functions","title":"String Functions","text":""},{"location":"query-guide/functions-reference/#like-ilike","title":"<code>LIKE</code> / <code>ILIKE</code>","text":"<p>Pattern matching (ILIKE is case-insensitive).</p> <pre><code>-- Case sensitive\nSELECT * FROM log_entries WHERE msg LIKE '%error%';\n\n-- Case insensitive\nSELECT * FROM log_entries WHERE msg ILIKE '%ERROR%';\n</code></pre>"},{"location":"query-guide/functions-reference/#regexp_matchstring-pattern","title":"<code>REGEXP_MATCH(string, pattern)</code>","text":"<p>Regular expression matching.</p> <pre><code>SELECT * FROM log_entries \nWHERE REGEXP_MATCH(msg, '^ERROR: [0-9]+');\n</code></pre>"},{"location":"query-guide/functions-reference/#lengthstring","title":"<code>LENGTH(string)</code>","text":"<p>Returns string length.</p> <pre><code>SELECT msg, LENGTH(msg) as msg_length FROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#substringstring-start-length","title":"<code>SUBSTRING(string, start, length)</code>","text":"<p>Extracts substring.</p> <pre><code>SELECT SUBSTRING(msg, 1, 50) as short_msg FROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#conditional-functions","title":"Conditional Functions","text":""},{"location":"query-guide/functions-reference/#case-when","title":"<code>CASE WHEN</code>","text":"<p>Conditional logic.</p> <pre><code>SELECT \n    level,\n    CASE \n        WHEN level &lt;= 2 THEN 'Critical'\n        WHEN level = 3 THEN 'Warning'\n        ELSE 'Info'\n    END as severity\nFROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#coalescevalue1-value2","title":"<code>COALESCE(value1, value2, ...)</code>","text":"<p>Returns first non-null value.</p> <pre><code>SELECT COALESCE(property_get(properties, 'thread'), 'unknown') as thread_name\nFROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#window-functions","title":"Window Functions","text":""},{"location":"query-guide/functions-reference/#row_number","title":"<code>ROW_NUMBER()</code>","text":"<p>Assigns row numbers within partitions.</p> <pre><code>SELECT \n    time, msg,\n    ROW_NUMBER() OVER (PARTITION BY process_id ORDER BY time) as row_num\nFROM log_entries;\n</code></pre>"},{"location":"query-guide/functions-reference/#rank-dense_rank","title":"<code>RANK()</code> / <code>DENSE_RANK()</code>","text":"<p>Ranks values within partitions.</p> <pre><code>SELECT \n    name, duration,\n    RANK() OVER (ORDER BY duration DESC) as performance_rank\nFROM view_instance('thread_spans', 'process_123');\n</code></pre>"},{"location":"query-guide/functions-reference/#lag-lead","title":"<code>LAG()</code> / <code>LEAD()</code>","text":"<p>Access previous/next row values.</p> <pre><code>SELECT \n    time, value,\n    LAG(value) OVER (ORDER BY time) as previous_value\nFROM measures\nWHERE name = 'cpu_usage';\n</code></pre>"},{"location":"query-guide/functions-reference/#array-functions","title":"Array Functions","text":""},{"location":"query-guide/functions-reference/#array_aggcolumn","title":"<code>ARRAY_AGG(column)</code>","text":"<p>Aggregates values into an array.</p> <pre><code>SELECT process_id, ARRAY_AGG(DISTINCT target) as targets\nFROM log_entries\nGROUP BY process_id;\n</code></pre>"},{"location":"query-guide/functions-reference/#unnestarray","title":"<code>UNNEST(array)</code>","text":"<p>Expands array into rows.</p> <pre><code>SELECT UNNEST(['error', 'warn', 'info']) as log_level;\n</code></pre>"},{"location":"query-guide/functions-reference/#advanced-query-patterns","title":"Advanced Query Patterns","text":""},{"location":"query-guide/functions-reference/#histogram-analysis","title":"Histogram Analysis","text":"<pre><code>-- Create performance histogram\nSELECT make_histogram(duration / 1000000.0, 10) as response_time_ms_histogram\nFROM view_instance('thread_spans', 'web_server')\nWHERE name = 'handle_request'\n  AND duration &gt; 1000000;  -- &gt; 1ms\n</code></pre>"},{"location":"query-guide/functions-reference/#property-extraction-and-filtering","title":"Property Extraction and Filtering","text":"<pre><code>-- Find logs with specific thread names\nSELECT time, level, msg, property_get(process_properties, 'thread-name') as thread\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') LIKE '%worker%'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/functions-reference/#time-based-aggregation","title":"Time-based Aggregation","text":"<pre><code>-- Hourly error counts\nSELECT \n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/functions-reference/#performance-analysis","title":"Performance Analysis","text":"<pre><code>-- Top 10 slowest functions with statistics\nSELECT \n    name,\n    COUNT(*) as call_count,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    STDDEV(duration) / 1000000.0 as stddev_ms\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 100000  -- &gt; 0.1ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/functions-reference/#datafusion-reference","title":"DataFusion Reference","text":"<p>For complete documentation of all standard SQL functions, see the Apache DataFusion SQL Reference.</p>"},{"location":"query-guide/functions-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> <li>Schema Reference - Complete view and field reference</li> </ul>"},{"location":"query-guide/performance/","title":"Performance Guide","text":"<p>Guidelines for writing efficient Micromegas SQL queries and avoiding common performance pitfalls.</p>"},{"location":"query-guide/performance/#critical-performance-rules","title":"Critical Performance Rules","text":""},{"location":"query-guide/performance/#1-always-use-time-ranges-via-python-api","title":"1. Always Use Time Ranges (via Python API)","text":"<p>\u26a1 Performance Tip: Always specify time ranges through the Python API parameters, not in SQL WHERE clauses.</p> <p>\u274c Inefficient - SQL time filter: <pre><code># Analytics server scans ALL partitions, then filters in SQL\nsql = \"\"\"\n    SELECT COUNT(*) FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '1 hour';\n\"\"\"\nresult = client.query(sql)  # No time range parameters!\n</code></pre></p> <p>\u2705 Efficient - API time range: <pre><code>import datetime\n\n# Analytics server eliminates irrelevant partitions BEFORE query execution\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\nsql = \"SELECT COUNT(*) FROM log_entries;\"\nresult = client.query(sql, begin, end)  # \u2b50 Time range in API\n</code></pre></p> <p>Why API time ranges are faster: - Partition Elimination: Analytics server removes entire partitions from consideration before SQL execution - Metadata Optimization: Uses partition metadata to skip irrelevant data files - Memory Efficiency: Only loads relevant data into query engine memory - Network Efficiency: Transfers only relevant data over FlightSQL</p> <p>Performance Impact: - API time range: Query considers only 1-2 partitions - SQL time filter: Query scans all partitions, then filters millions of rows</p>"},{"location":"query-guide/performance/#2-use-process-scoped-views","title":"2. Use Process-Scoped Views","text":"<p>\u274c Less Efficient: <pre><code>-- Scans all data then filters\nSELECT * FROM log_entries WHERE process_id = 'my_process';\n</code></pre></p> <p>\u2705 More Efficient: <pre><code>-- Uses optimized process partition\nSELECT * FROM view_instance('log_entries', 'my_process');\n</code></pre></p>"},{"location":"query-guide/performance/#query-optimization","title":"Query Optimization","text":""},{"location":"query-guide/performance/#predicate-pushdown","title":"Predicate Pushdown","text":"<p>Micromegas automatically pushes filters down to the storage layer when possible:</p> <pre><code>-- These filters are pushed to Parquet reader for efficiency\nWHERE time &gt;= NOW() - INTERVAL '1 day'\n  AND level &lt;= 3\n  AND process_id = 'my_process'\n</code></pre>"},{"location":"query-guide/performance/#memory-considerations","title":"Memory Considerations","text":"<p>Use LIMIT for exploration: <pre><code>-- Good for testing queries\nSELECT * FROM log_entries LIMIT 1000;\n</code></pre></p> <p>Use streaming for large results: <pre><code># Python API for large datasets\nfor batch in client.query_stream(sql, begin, end):\n    process_batch(batch.to_pandas())\n</code></pre></p> <p>More performance guidance coming soon...</p>"},{"location":"query-guide/python-api/","title":"Python API Reference","text":"<p>The Micromegas Python client provides a simple but powerful interface for querying observability data using SQL. This page covers all client methods, connection options, and advanced features.</p>"},{"location":"query-guide/python-api/#installation","title":"Installation","text":"<p>Install the Micromegas Python client from PyPI:</p> <pre><code>pip install micromegas\n</code></pre>"},{"location":"query-guide/python-api/#basic-usage","title":"Basic Usage","text":""},{"location":"query-guide/python-api/#connection","title":"Connection","text":"<pre><code>import micromegas\n\n# Connect to local Micromegas instance (default)\nclient = micromegas.connect()\n\n# Connect to remote instance\nclient = micromegas.connect(endpoint=\"http://your-server:8080\")\n</code></pre> <p>The <code>connect()</code> function automatically discovers your Micromegas FlightSQL endpoint or uses the provided endpoint URL.</p>"},{"location":"query-guide/python-api/#simple-queries","title":"Simple Queries","text":"<pre><code>import datetime\n\n# Set up time range\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Execute query with time range\nsql = \"SELECT * FROM log_entries LIMIT 10;\"\ndf = client.query(sql, begin, end)\nprint(df)\n</code></pre>"},{"location":"query-guide/python-api/#client-methods","title":"Client Methods","text":""},{"location":"query-guide/python-api/#querysql-beginnone-endnone","title":"<code>query(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a pandas DataFrame.</p> <p>Parameters: - <code>sql</code> (str): SQL query string - <code>begin</code> (datetime, optional): \u26a1 Recommended - Start time for partition elimination - <code>end</code> (datetime, optional): \u26a1 Recommended - End time for partition elimination</p> <p>Returns: - <code>pandas.DataFrame</code>: Query results</p> <p>Performance Note: Using <code>begin</code> and <code>end</code> parameters instead of SQL time filters allows the analytics server to eliminate entire partitions before query execution, providing significant performance improvements.</p> <p>Example: <pre><code># \u2705 EFFICIENT: API time range enables partition elimination\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\", begin, end)  # \u2b50 Time range in API parameters\n\n# \u274c INEFFICIENT: SQL time filter scans all partitions\ndf = client.query(\"\"\"\n    SELECT time, process_id, level, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '1 hour'  -- Server scans ALL partitions\n      AND level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 100;\n\"\"\")  # Missing API time parameters!\n\n# \u2705 OK: Query without time range (for metadata queries)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n</code></pre></p>"},{"location":"query-guide/python-api/#query_streamsql-beginnone-endnone","title":"<code>query_stream(sql, begin=None, end=None)</code>","text":"<p>Execute a SQL query and return results as a stream of Apache Arrow RecordBatch objects. Use this for large datasets to avoid memory issues.</p> <p>Parameters: - <code>sql</code> (str): SQL query string - <code>begin</code> (datetime, optional): \u26a1 Recommended - Start time for partition elimination - <code>end</code> (datetime, optional): \u26a1 Recommended - End time for partition elimination</p> <p>Returns: - Iterator of <code>pyarrow.RecordBatch</code>: Stream of result batches</p> <p>Example: <pre><code>import pyarrow as pa\n\n# Stream large dataset\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n    ORDER BY time DESC;\n\"\"\"\n\nfor record_batch in client.query_stream(sql, begin, end):\n    # record_batch is a pyarrow.RecordBatch\n    print(f\"Batch shape: {record_batch.num_rows} x {record_batch.num_columns}\")\n    print(f\"Schema: {record_batch.schema}\")\n\n    # Convert to pandas for analysis\n    df = record_batch.to_pandas()\n\n    # Process this batch\n    error_logs = df[df['level'] &lt;= 3]\n    if not error_logs.empty:\n        print(f\"Found {len(error_logs)} errors in this batch\")\n        # Process errors...\n\n    # Memory is automatically freed after each batch\n</code></pre></p>"},{"location":"query-guide/python-api/#working-with-results","title":"Working with Results","text":""},{"location":"query-guide/python-api/#pandas-dataframes","title":"pandas DataFrames","text":"<p>All <code>query()</code> results are pandas DataFrames, giving you access to the full pandas ecosystem:</p> <pre><code># Basic DataFrame operations\nresult = client.query(\"SELECT process_id, exe, start_time FROM processes;\")\n\n# Inspect the data\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Filter and analyze\nrecent = result[result['start_time'] &gt; datetime.datetime.now() - datetime.timedelta(days=1)]\nprint(f\"Recent processes: {len(recent)}\")\n\n# Group and aggregate\nby_exe = result.groupby('exe').size().sort_values(ascending=False)\nprint(\"Processes by executable:\")\nprint(by_exe.head())\n</code></pre>"},{"location":"query-guide/python-api/#pyarrow-recordbatch","title":"pyarrow RecordBatch","text":"<p>Streaming queries return Apache Arrow RecordBatch objects:</p> <pre><code>for batch in client.query_stream(sql, begin, end):\n    # RecordBatch properties\n    print(f\"Rows: {batch.num_rows}\")\n    print(f\"Columns: {batch.num_columns}\")\n    print(f\"Schema: {batch.schema}\")\n\n    # Access individual columns\n    time_column = batch.column('time')\n    level_column = batch.column('level')\n\n    # Convert to pandas (zero-copy operation)\n    df = batch.to_pandas()\n\n    # Convert to other formats\n    table = batch.to_pylist()  # List of dictionaries\n    numpy_dict = batch.to_pydict()  # Dictionary of numpy arrays\n</code></pre>"},{"location":"query-guide/python-api/#advanced-features","title":"Advanced Features","text":""},{"location":"query-guide/python-api/#query-streaming-benefits","title":"Query Streaming Benefits","text":"<p>Use <code>query_stream()</code> for large datasets to:</p> <ul> <li>Reduce memory usage: Process data in chunks instead of loading everything</li> <li>Improve responsiveness: Start processing before the query completes</li> <li>Handle large results: Query datasets larger than available RAM</li> </ul> <pre><code># Example: Process week of data in batches\ntotal_errors = 0\ntotal_rows = 0\n\nfor batch in client.query_stream(\"\"\"\n    SELECT level, msg FROM log_entries \n    WHERE time &gt;= NOW() - INTERVAL '7 days'\n\"\"\", begin, end):\n    df = batch.to_pandas()\n    errors_in_batch = len(df[df['level'] &lt;= 2])\n\n    total_errors += errors_in_batch\n    total_rows += len(df)\n\n    print(f\"Batch: {len(df)} rows, {errors_in_batch} errors\")\n\nprint(f\"Total: {total_rows} rows, {total_errors} errors\")\n</code></pre>"},{"location":"query-guide/python-api/#flightsql-protocol-benefits","title":"FlightSQL Protocol Benefits","text":"<p>Micromegas uses Apache Arrow FlightSQL for optimal performance:</p> <ul> <li>Columnar data transfer: Orders of magnitude faster than JSON</li> <li>Binary protocol: No serialization/deserialization overhead  </li> <li>Native compression: Efficient network utilization</li> <li>Vectorized operations: Optimized for analytical workloads</li> <li>Zero-copy operations: Direct memory mapping from network buffers</li> </ul>"},{"location":"query-guide/python-api/#connection-configuration","title":"Connection Configuration","text":"<pre><code># Default connection (auto-discovery)\nclient = micromegas.connect()\n\n# Explicit endpoint\nclient = micromegas.connect(endpoint=\"http://analytics.mycompany.com:8080\")\n\n# Connection with timeout\nclient = micromegas.connect(\n    endpoint=\"http://remote-server:8080\",\n    timeout=30.0\n)\n</code></pre>"},{"location":"query-guide/python-api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    df = client.query(\"SELECT * FROM log_entries;\", begin, end)\nexcept Exception as e:\n    print(f\"Query failed: {e}\")\n\n# Check for empty results\nif df.empty:\n    print(\"No data found for this time range\")\nelse:\n    print(f\"Found {len(df)} rows\")\n</code></pre>"},{"location":"query-guide/python-api/#performance-tips","title":"Performance Tips","text":""},{"location":"query-guide/python-api/#use-time-ranges","title":"Use Time Ranges","text":"<p>Always specify time ranges for better performance:</p> <pre><code># \u2705 Good - efficient\ndf = client.query(sql, begin, end)\n\n# \u274c Avoid - can be slow\ndf = client.query(sql)\n</code></pre>"},{"location":"query-guide/python-api/#streaming-for-large-results","title":"Streaming for Large Results","text":"<p>Use streaming for queries that might return large datasets:</p> <pre><code># If you expect &gt; 100MB of results, use streaming\nif expected_result_size_mb &gt; 100:\n    for batch in client.query_stream(sql, begin, end):\n        process_batch(batch.to_pandas())\nelse:\n    df = client.query(sql, begin, end)\n    process_dataframe(df)\n</code></pre>"},{"location":"query-guide/python-api/#limit-result-size","title":"Limit Result Size","text":"<p>Add LIMIT clauses for exploratory queries:</p> <pre><code># Good for exploration\ndf = client.query(\"SELECT * FROM log_entries LIMIT 1000;\", begin, end)\n\n# Then remove limit for production queries\ndf = client.query(\"SELECT * FROM log_entries WHERE level &lt;= 2;\", begin, end)\n</code></pre>"},{"location":"query-guide/python-api/#integration-examples","title":"Integration Examples","text":""},{"location":"query-guide/python-api/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Query data\ndf = client.query(\"\"\"\n    SELECT time, name, value \n    FROM measures \n    WHERE name = 'cpu_usage'\n\"\"\", begin, end)\n\n# Plot time series\nplt.figure(figsize=(12, 6))\nplt.plot(df['time'], df['value'])\nplt.title('CPU Usage Over Time')\nplt.xlabel('Time')\nplt.ylabel('CPU Usage %')\nplt.show()\n</code></pre>"},{"location":"query-guide/python-api/#data-pipeline","title":"Data Pipeline","text":"<pre><code>import pandas as pd\n\ndef extract_metrics(process_id, hours=24):\n    \"\"\"Extract metrics for a specific process.\"\"\"\n    end = datetime.datetime.now(datetime.timezone.utc)\n    begin = end - datetime.timedelta(hours=hours)\n\n    sql = f\"\"\"\n        SELECT time, name, value, unit\n        FROM view_instance('measures', '{process_id}')\n        WHERE time &gt;= '{begin.isoformat()}'\n        ORDER BY time;\n    \"\"\"\n\n    return client.query(sql, begin, end)\n\ndef analyze_performance(df):\n    \"\"\"Analyze performance metrics.\"\"\"\n    metrics = {}\n    for name in df['name'].unique():\n        data = df[df['name'] == name]['value']\n        metrics[name] = {\n            'mean': data.mean(),\n            'max': data.max(),\n            'min': data.min(),\n            'std': data.std()\n        }\n    return metrics\n\n# Use in pipeline\nprocess_metrics = extract_metrics('my-service-123')\nperformance_summary = analyze_performance(process_metrics)\nprint(performance_summary)\n</code></pre>"},{"location":"query-guide/python-api/#next-steps","title":"Next Steps","text":"<ul> <li>Schema Reference - Understand available views and fields</li> <li>Functions Reference - Learn about SQL functions</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries</li> </ul>"},{"location":"query-guide/query-patterns/","title":"Query Patterns","text":"<p>Common patterns and examples for querying observability data with Micromegas SQL.</p>"},{"location":"query-guide/query-patterns/#error-tracking-and-debugging","title":"Error Tracking and Debugging","text":""},{"location":"query-guide/query-patterns/#recent-errors","title":"Recent Errors","text":"<pre><code>-- Get all errors from the last hour\nSELECT time, process_id, target, msg\nFROM log_entries\nWHERE level &lt;= 2  -- Fatal and Error\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n</code></pre>"},{"location":"query-guide/query-patterns/#error-trends","title":"Error Trends","text":"<pre><code>-- Hourly error counts for trend analysis\nSELECT \n    date_trunc('hour', time) as hour,\n    COUNT(*) as error_count\nFROM log_entries\nWHERE level &lt;= 2\n  AND time &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', time)\nORDER BY hour;\n</code></pre>"},{"location":"query-guide/query-patterns/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"query-guide/query-patterns/#slow-operations","title":"Slow Operations","text":"<pre><code>-- Find slowest function calls\nSELECT \n    name,\n    AVG(duration) / 1000000.0 as avg_ms,\n    MAX(duration) / 1000000.0 as max_ms,\n    COUNT(*) as call_count\nFROM view_instance('thread_spans', 'my_process')\nWHERE duration &gt; 10000000  -- &gt; 10ms\nGROUP BY name\nORDER BY avg_ms DESC\nLIMIT 10;\n</code></pre>"},{"location":"query-guide/query-patterns/#resource-usage","title":"Resource Usage","text":"<pre><code>-- CPU and memory trends\nSELECT \n    date_trunc('minute', time) as minute,\n    name,\n    AVG(value) as avg_value,\n    unit\nFROM measures\nWHERE name IN ('cpu_usage', 'memory_usage')\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY minute, name, unit\nORDER BY minute, name;\n</code></pre> <p>More patterns coming soon...</p>"},{"location":"query-guide/quick-start/","title":"Quick Start","text":"<p>Get up and running with Micromegas SQL queries in minutes. This guide shows you the essential patterns for querying your observability data.</p>"},{"location":"query-guide/quick-start/#basic-connection","title":"Basic Connection","text":"<p>All Micromegas queries start by connecting to the analytics service:</p> <pre><code>import datetime\nimport micromegas\n\n# Connect to Micromegas analytics service\nclient = micromegas.connect()\n</code></pre> <p>The <code>connect()</code> function automatically discovers your local Micromegas instance or connects to a configured remote endpoint.</p>"},{"location":"query-guide/quick-start/#your-first-query","title":"Your First Query","text":"<p>Let's query recent log entries to see what data is available:</p> <pre><code># Set up time range for queries\nnow = datetime.datetime.now(datetime.timezone.utc)\nbegin = now - datetime.timedelta(hours=1)\nend = now\n\n# Query recent log entries\nsql = \"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 4\n    ORDER BY time DESC\n    LIMIT 10;\n\"\"\"\n\n# Execute the query\nlogs = client.query(sql, begin, end)\nprint(logs)\nprint(f\"Result type: {type(logs)}\")  # pandas.DataFrame\n</code></pre> <p>Key points: - \u26a1 Important: Always specify time range via API parameters (<code>begin</code>, <code>end</code>) for best performance - Results are returned as pandas DataFrames - <code>level &lt;= 4</code> filters to show errors and warnings (see log levels) - Use API time parameters instead of SQL time filters for partition elimination</p>"},{"location":"query-guide/quick-start/#understanding-return-types","title":"Understanding Return Types","text":"<p>All queries return pandas DataFrames:</p> <pre><code># Query returns a pandas DataFrame\nresult = client.query(\"SELECT process_id, exe FROM processes LIMIT 5;\")\n\n# Access DataFrame properties\nprint(f\"Shape: {result.shape}\")\nprint(f\"Columns: {result.columns.tolist()}\")\nprint(f\"Data types:\\n{result.dtypes}\")\n\n# Use pandas operations\nfiltered = result[result['exe'].str.contains('analytics')]\nprint(filtered.head())\n</code></pre> <p>This makes it easy to work with results using the entire pandas ecosystem for analysis, visualization, and data processing.</p>"},{"location":"query-guide/quick-start/#essential-query-patterns","title":"Essential Query Patterns","text":""},{"location":"query-guide/quick-start/#1-process-information","title":"1. Process Information","text":"<p>Get an overview of processes sending telemetry:</p> <pre><code>processes = client.query(\"\"\"\n    SELECT process_id, exe, computer, start_time\n    FROM processes\n    ORDER BY start_time DESC\n    LIMIT 10;\n\"\"\")\nprint(processes)\n</code></pre>"},{"location":"query-guide/quick-start/#2-recent-log-entries","title":"2. Recent Log Entries","text":"<p>Query logs with error filtering:</p> <pre><code>error_logs = client.query(\"\"\"\n    SELECT time, process_id, level, target, msg\n    FROM log_entries\n    WHERE level &lt;= 3  -- Fatal, Error, Warn\n    ORDER BY time DESC\n    LIMIT 50;\n\"\"\", begin, end)\nprint(error_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#3-performance-metrics","title":"3. Performance Metrics","text":"<p>Query numeric measurements:</p> <pre><code>metrics = client.query(\"\"\"\n    SELECT time, process_id, name, value, unit\n    FROM measures\n    WHERE name LIKE '%cpu%'\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(metrics)\n</code></pre>"},{"location":"query-guide/quick-start/#4-process-specific-data","title":"4. Process-Specific Data","text":"<p>Use view instances for better performance when focusing on specific processes:</p> <pre><code>process_id = \"your_process_id_here\"  # Replace with actual process ID\n\nprocess_logs = client.query(f\"\"\"\n    SELECT time, level, target, msg\n    FROM view_instance('log_entries', '{process_id}')\n    WHERE level &lt;= 3\n    ORDER BY time DESC\n    LIMIT 20;\n\"\"\", begin, end)\nprint(process_logs)\n</code></pre>"},{"location":"query-guide/quick-start/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Common filters: - <code>level &lt;= 2</code> - Only fatal and error messages - <code>level &lt;= 3</code> - Fatal, error, and warning messages - <code>level &lt;= 4</code> - All messages except debug and trace</p>"},{"location":"query-guide/quick-start/#time-range-best-practices","title":"Time Range Best Practices","text":""},{"location":"query-guide/quick-start/#always-use-time-ranges","title":"Always Use Time Ranges","text":"<pre><code># \u2705 Good - efficient and memory-safe\ndf = client.query(sql, begin_time, end_time)\n\n# \u274c Avoid - can be slow and memory-intensive\ndf = client.query(sql)  # Queries ALL data\n</code></pre>"},{"location":"query-guide/quick-start/#common-time-ranges","title":"Common Time Ranges","text":"<pre><code>now = datetime.datetime.now(datetime.timezone.utc)\n\n# Last hour\nbegin = now - datetime.timedelta(hours=1)\n\n# Last day\nbegin = now - datetime.timedelta(days=1)\n\n# Last week\nbegin = now - datetime.timedelta(weeks=1)\n\n# Custom range\nbegin = datetime.datetime(2024, 1, 1, tzinfo=datetime.timezone.utc)\nend = datetime.datetime(2024, 1, 2, tzinfo=datetime.timezone.utc)\n</code></pre>"},{"location":"query-guide/quick-start/#safe-queries-without-time-ranges","title":"Safe Queries Without Time Ranges","text":"<p>Some queries are safe to run without time ranges because they operate on small metadata tables:</p> <pre><code># Process information (typically small dataset)\nprocesses = client.query(\"SELECT process_id, exe FROM processes LIMIT 10;\")\n\n# Stream metadata\nstreams = client.query(\"SELECT stream_id, process_id FROM streams LIMIT 10;\")\n\n# Count queries (use with caution on large datasets)\ncount = client.query(\"SELECT COUNT(*) FROM log_entries;\")\n</code></pre> <p>Performance Impact</p> <p>Avoid querying <code>log_entries</code>, <code>measures</code>, <code>thread_spans</code>, or <code>async_events</code> without time ranges on production systems with large datasets.</p>"},{"location":"query-guide/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you can run basic queries:</p> <ol> <li>Explore the Python API - Learn about streaming and advanced features</li> <li>Review the Schema - Understand all available fields and data types</li> <li>Try Query Patterns - Common observability query patterns</li> <li>Optimize Performance - Learn to write efficient queries</li> </ol>"},{"location":"query-guide/quick-start/#quick-reference","title":"Quick Reference","text":""},{"location":"query-guide/quick-start/#essential-views","title":"Essential Views","text":"<ul> <li><code>processes</code> - Process metadata</li> <li><code>log_entries</code> - Application logs</li> <li><code>measures</code> - Numeric metrics</li> <li><code>thread_spans</code> - Execution timing</li> <li><code>async_events</code> - Async operation tracking</li> </ul>"},{"location":"query-guide/quick-start/#key-functions","title":"Key Functions","text":"<ul> <li><code>view_instance('view_name', 'process_id')</code> - Process-scoped views</li> <li><code>property_get(properties, 'key')</code> - Extract property values</li> <li><code>make_histogram(values, bins)</code> - Create histograms</li> </ul>"},{"location":"query-guide/quick-start/#time-functions","title":"Time Functions","text":"<ul> <li><code>NOW()</code> - Current timestamp</li> <li><code>INTERVAL '1 hour'</code> - Time duration</li> <li><code>date_trunc('hour', time)</code> - Truncate to time boundary</li> </ul>"},{"location":"query-guide/schema-reference/","title":"Schema Reference","text":"<p>This page provides a complete reference to all views, data types, and field definitions available in Micromegas SQL queries.</p>"},{"location":"query-guide/schema-reference/#views-overview","title":"Views Overview","text":"<p>Micromegas organizes telemetry data into several views that can be queried using SQL:</p> View Description Use Cases <code>processes</code> Process metadata and system information System overview, process tracking <code>streams</code> Data stream information within processes Stream debugging, data flow analysis <code>blocks</code> Core telemetry block metadata Low-level data inspection <code>log_entries</code> Application log messages with levels Error tracking, debugging, monitoring <code>measures</code> Numeric metrics and performance data Performance monitoring, alerting <code>thread_spans</code> Synchronous execution spans and timing Performance profiling, call tracing <code>async_events</code> Asynchronous event lifecycle tracking Async operation monitoring"},{"location":"query-guide/schema-reference/#core-views","title":"Core Views","text":""},{"location":"query-guide/schema-reference/#processes","title":"<code>processes</code>","text":"<p>Contains metadata about processes that have sent telemetry data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the process <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>realname</code> <code>Dictionary(Int16, Utf8)</code> Real name of the user <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>distro</code> <code>Dictionary(Int16, Utf8)</code> Operating system distribution <code>cpu_brand</code> <code>Dictionary(Int16, Utf8)</code> CPU brand information <code>tsc_frequency</code> <code>UInt64</code> Time stamp counter frequency <code>start_time</code> <code>Timestamp(Nanosecond)</code> Process start time <code>start_ticks</code> <code>UInt64</code> Process start time in ticks <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the process data was first inserted <code>parent_process_id</code> <code>Dictionary(Int16, Utf8)</code> Parent process identifier <code>properties</code> <code>Map</code> Additional process metadata <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the process data was last updated <p>Example Queries: <pre><code>-- Get all processes from the last day\nSELECT process_id, exe, computer, start_time\nFROM processes\nWHERE start_time &gt;= NOW() - INTERVAL '1 day'\nORDER BY start_time DESC;\n\n-- Find processes by executable name\nSELECT process_id, exe, username, computer\nFROM processes\nWHERE exe LIKE '%analytics%';\n</code></pre></p>"},{"location":"query-guide/schema-reference/#streams","title":"<code>streams</code>","text":"<p>Contains information about data streams within processes.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Unique identifier for the stream <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Reference to the parent process <code>dependencies_metadata</code> Various Stream dependency metadata <code>objects_metadata</code> Various Stream object metadata <code>tags</code> Various Stream tags <code>properties</code> Various Stream properties <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was first inserted <code>last_update_time</code> <code>Timestamp(Nanosecond)</code> When the stream data was last updated <p>Example Queries: <pre><code>-- Get streams for a specific process\nSELECT stream_id, tags, properties\nFROM streams\nWHERE process_id = 'my_process_123';\n\n-- Join streams with process information\nSELECT s.stream_id, s.tags, p.exe, p.computer\nFROM streams s\nJOIN processes p ON s.process_id = p.process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#blocks","title":"<code>blocks</code>","text":"<p>Core table containing telemetry block metadata with joined process and stream information.</p> Field Type Description <code>block_id</code> <code>Utf8</code> Unique identifier for the block <code>stream_id</code> <code>Utf8</code> Stream identifier <code>process_id</code> <code>Utf8</code> Process identifier <code>begin_time</code> <code>Timestamp(Nanosecond)</code> Block start time <code>begin_ticks</code> <code>Int64</code> Block start time in ticks <code>end_time</code> <code>Timestamp(Nanosecond)</code> Block end time <code>end_ticks</code> <code>Int64</code> Block end time in ticks <code>nb_objects</code> <code>Int32</code> Number of objects in block <code>object_offset</code> <code>Int64</code> Offset to objects in storage <code>payload_size</code> <code>Int64</code> Size of block payload <code>insert_time</code> <code>Timestamp(Nanosecond)</code> When block was inserted <p>Joined Process Fields: | Field | Type | Description | |-------|------|-------------| | <code>processes.start_time</code> | <code>Timestamp(Nanosecond)</code> | Process start time | | <code>processes.start_ticks</code> | <code>Int64</code> | Process start ticks | | <code>processes.tsc_frequency</code> | <code>Int64</code> | Time stamp counter frequency | | <code>processes.exe</code> | <code>Utf8</code> | Executable name | | <code>processes.username</code> | <code>Utf8</code> | User who ran the process | | <code>processes.realname</code> | <code>Utf8</code> | Real name of the user | | <code>processes.computer</code> | <code>Utf8</code> | Computer/hostname | | <code>processes.distro</code> | <code>Utf8</code> | Operating system distribution | | <code>processes.cpu_brand</code> | <code>Utf8</code> | CPU brand information |</p> <p>Example Queries: <pre><code>-- Analyze block sizes and object counts\nSELECT \n    process_id,\n    AVG(payload_size) as avg_block_size,\n    AVG(nb_objects) as avg_objects_per_block,\n    COUNT(*) as total_blocks\nFROM blocks\nWHERE insert_time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#observability-data-views","title":"Observability Data Views","text":""},{"location":"query-guide/schema-reference/#log_entries","title":"<code>log_entries</code>","text":"<p>Text-based log entries with levels and structured data.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Log entry timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>level</code> <code>Int32</code> Log level (see Log Levels) <code>msg</code> <code>Utf8</code> Log message <code>properties</code> <code>List&lt;Struct&gt;</code> Log-specific properties <code>process_properties</code> <code>List&lt;Struct&gt;</code> Process-specific properties"},{"location":"query-guide/schema-reference/#log-levels","title":"Log Levels","text":"<p>Micromegas uses numeric log levels for efficient filtering:</p> Level Name Description 1 Fatal Critical errors that cause application termination 2 Error Errors that don't stop execution but need attention 3 Warn Warning conditions that might cause problems 4 Info Informational messages about normal operation 5 Debug Detailed information for debugging 6 Trace Very detailed tracing information <p>Example Queries: <pre><code>-- Get recent error and warning logs\nSELECT time, process_id, level, target, msg\nFROM log_entries\nWHERE level &lt;= 3  -- Fatal, Error, Warn\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time DESC;\n\n-- Count logs by level for a specific process\nSELECT level, COUNT(*) as count\nFROM view_instance('log_entries', 'my_process_123')\nWHERE time &gt;= NOW() - INTERVAL '1 day'\nGROUP BY level\nORDER BY level;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#measures","title":"<code>measures</code>","text":"<p>Numerical measurements and counters.</p> Field Type Description <code>process_id</code> <code>Dictionary(Int16, Utf8)</code> Process identifier <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>insert_time</code> <code>Timestamp(Nanosecond)</code> Block insertion time <code>exe</code> <code>Dictionary(Int16, Utf8)</code> Executable name <code>username</code> <code>Dictionary(Int16, Utf8)</code> User who ran the process <code>computer</code> <code>Dictionary(Int16, Utf8)</code> Computer/hostname <code>time</code> <code>Timestamp(Nanosecond)</code> Measurement timestamp <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>name</code> <code>Dictionary(Int16, Utf8)</code> Metric name <code>unit</code> <code>Dictionary(Int16, Utf8)</code> Measurement unit <code>value</code> <code>Float64</code> Metric value <code>properties</code> <code>List&lt;Struct&gt;</code> Metric-specific properties <code>process_properties</code> <code>List&lt;Struct&gt;</code> Process-specific properties <p>Example Queries: <pre><code>-- Get CPU metrics over time\nSELECT time, value, unit\nFROM measures\nWHERE name = 'cpu_usage'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nORDER BY time;\n\n-- Aggregate memory usage by process\nSELECT \n    process_id,\n    AVG(value) as avg_memory,\n    MAX(value) as peak_memory,\n    unit\nFROM measures\nWHERE name LIKE '%memory%'\n  AND time &gt;= NOW() - INTERVAL '1 hour'\nGROUP BY process_id, unit;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#thread_spans","title":"<code>thread_spans</code>","text":"<p>Derived view for analyzing span durations and hierarchies. Access via <code>view_instance('thread_spans', stream_id)</code>.</p> Field Type Description <code>id</code> <code>Int64</code> Span identifier <code>parent</code> <code>Int64</code> Parent span identifier <code>depth</code> <code>UInt32</code> Nesting depth in call tree <code>hash</code> <code>UInt32</code> Span hash for deduplication <code>begin</code> <code>Timestamp(Nanosecond)</code> Span start time <code>end</code> <code>Timestamp(Nanosecond)</code> Span end time <code>duration</code> <code>Int64</code> Span duration in nanoseconds <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>line</code> <code>UInt32</code> Line number <p>Example Queries: <pre><code>-- Get slowest functions in a stream\nSELECT name, AVG(duration) as avg_duration_ns, COUNT(*) as call_count\nFROM view_instance('thread_spans', 'stream_123')\nWHERE duration &gt; 1000000  -- &gt; 1ms\nGROUP BY name\nORDER BY avg_duration_ns DESC\nLIMIT 10;\n\n-- Analyze call hierarchy\nSELECT depth, name, duration\nFROM view_instance('thread_spans', 'stream_123')\nWHERE parent = 42  -- specific parent span\nORDER BY begin;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#async_events","title":"<code>async_events</code>","text":"<p>Asynchronous span events for tracking async operations.</p> Field Type Description <code>stream_id</code> <code>Dictionary(Int16, Utf8)</code> Thread stream identifier <code>block_id</code> <code>Dictionary(Int16, Utf8)</code> Block identifier <code>time</code> <code>Timestamp(Nanosecond)</code> Event timestamp <code>event_type</code> <code>Dictionary(Int16, Utf8)</code> \"begin\" or \"end\" <code>span_id</code> <code>Int64</code> Async span identifier <code>parent_span_id</code> <code>Int64</code> Parent span identifier <code>name</code> <code>Dictionary(Int16, Utf8)</code> Span name (function) <code>filename</code> <code>Dictionary(Int16, Utf8)</code> Source file <code>target</code> <code>Dictionary(Int16, Utf8)</code> Module/target <code>line</code> <code>UInt32</code> Line number <p>Example Queries: <pre><code>-- Find async operations that took longest\nSELECT \n    span_id,\n    name,\n    MAX(time) - MIN(time) as duration_ns\nFROM view_instance('async_events', 'my_process_123')\nGROUP BY span_id, name\nHAVING COUNT(*) = 2  -- Both begin and end events\nORDER BY duration_ns DESC\nLIMIT 10;\n\n-- Track async operation lifecycle\nSELECT time, event_type, name, span_id, parent_span_id\nFROM view_instance('async_events', 'my_process_123')\nWHERE span_id = 12345\nORDER BY time;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#data-types","title":"Data Types","text":""},{"location":"query-guide/schema-reference/#properties","title":"Properties","text":"<p>Key-value pairs stored as <code>List&lt;Struct&gt;</code> with the following structure:</p> <pre><code>-- Properties structure\nList&lt;Struct&lt;\n    key: Utf8,\n    value: Utf8\n&gt;&gt;\n</code></pre> <p>Common properties fields: - <code>properties</code> - Event-specific metadata (log properties, metric properties) - <code>process_properties</code> - Process-wide metadata shared across all events from a process</p> <p>Querying properties: <pre><code>-- Access property values using property_get function\nSELECT property_get(process_properties, 'thread-name') as thread_name\nFROM log_entries\nWHERE property_get(process_properties, 'thread-name') IS NOT NULL;\n</code></pre></p>"},{"location":"query-guide/schema-reference/#dictionary-compression","title":"Dictionary Compression","text":"<p>Most string fields use dictionary compression (<code>Dictionary(Int16, Utf8)</code>) for storage efficiency:</p> <ul> <li>Reduces storage space for repeated values</li> <li>Improves query performance</li> <li>Transparent to SQL queries - use as normal strings</li> </ul>"},{"location":"query-guide/schema-reference/#timestamps","title":"Timestamps","text":"<p>All time fields use <code>Timestamp(Nanosecond)</code> precision:</p> <ul> <li>Nanosecond resolution for high-precision timing</li> <li>UTC timezone assumed</li> <li>Compatible with standard SQL time functions</li> </ul>"},{"location":"query-guide/schema-reference/#view-relationships","title":"View Relationships","text":"<p>Views can be joined to combine information:</p> <pre><code>-- Join log entries with process information\nSELECT l.time, l.level, l.msg, p.exe, p.computer\nFROM log_entries l\nJOIN processes p ON l.process_id = p.process_id\nWHERE l.level &lt;= 2;  -- Fatal and Error only\n\n-- Join measures with stream information\nSELECT m.time, m.name, m.value, s.tags\nFROM measures m\nJOIN streams s ON m.stream_id = s.stream_id\nWHERE m.name = 'cpu_usage';\n</code></pre>"},{"location":"query-guide/schema-reference/#performance-considerations","title":"Performance Considerations","text":""},{"location":"query-guide/schema-reference/#dictionary-fields","title":"Dictionary Fields","text":"<p>Dictionary-compressed fields are optimized for: - Equality comparisons (<code>field = 'value'</code>) - IN clauses (<code>field IN ('val1', 'val2')</code>) - LIKE patterns on repeated values</p>"},{"location":"query-guide/schema-reference/#time-based-queries","title":"Time-based Queries","text":"<p>Always use time ranges for optimal performance: <pre><code>-- Good - uses time index\nWHERE time &gt;= NOW() - INTERVAL '1 hour'\n\n-- Avoid - full table scan\nWHERE level &lt;= 3\n</code></pre></p>"},{"location":"query-guide/schema-reference/#view-instances","title":"View Instances","text":"<p>Use <code>view_instance()</code> for process-specific queries: <pre><code>-- Better performance for single process\nSELECT * FROM view_instance('log_entries', 'process_123')\n\n-- Less efficient for single process\nSELECT * FROM log_entries WHERE process_id = 'process_123'\n</code></pre></p>"},{"location":"query-guide/schema-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Functions Reference - SQL functions available for queries</li> <li>Query Patterns - Common observability query patterns</li> <li>Performance Guide - Optimize your queries for best performance</li> </ul>"},{"location":"venv/lib/python3.12/site-packages/backrefs-5.9.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>The MIT License (MIT)</p> <p>Copyright (c) 2015 - 2025 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib/python3.12/site-packages/idna-3.10.dist-info/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright    notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimer in the    documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib/python3.12/site-packages/markdown-3.8.2.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version)</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib/python3.12/site-packages/mkdocs_get_deps-0.2.0.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2023 Oleh Prypin oleh@pryp.in</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib/python3.12/site-packages/mkdocs_material_extensions-1.3.1.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2021 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib/python3.12/site-packages/pip-25.2.dist-info/licenses/src/pip/_vendor/idna/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright    notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimer in the    documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/","title":"License","text":""},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#pymdown-extensions","title":"PyMdown Extensions","text":"<p>The MIT License (MIT)</p> <p>Copyright (c) 2014 - 2025 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#superfences","title":"SuperFences","text":"<p><code>superfences.py</code> is derived from Python Markdown's fenced_code extension.</p> <pre><code>Fenced Code Extension for Python Markdown\n =========================================\nThis extension adds Fenced Code Blocks to Python-Markdown.\nSee &lt;https://python-markdown.github.io/extensions/fenced_code_blocks/&gt;\nfor documentation.\nOriginal code Copyright 2007-2008 [Waylan Limberg](http://achinghead.com/).\nAll changes Copyright 2008-2014 The Python Markdown Project\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#highlight","title":"Highlight","text":"<p><code>highlight.py</code> is derived from Python Markdown's CodeHilite extension.</p> <pre><code>CodeHilite Extension for Python-Markdown\n ========================================\nAdds code/syntax highlighting to standard Python-Markdown code blocks.\nSee &lt;https://python-markdown.github.io/extensions/code_hilite/&gt;\nfor documentation.\nOriginal code Copyright 2006-2008 [Waylan Limberg](http://achinghead.com/).\nAll changes Copyright 2008-2014 The Python Markdown Project\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#fancylists","title":"FancyLists","text":"<p><code>fancylists.py</code> is derived from Python Markdown's list handler.</p> <pre><code>Started by Manfred Stienstra (http://www.dwerg.net/).\nMaintained for a few years by Yuri Takhteyev (http://www.freewisdom.org).\nCurrently maintained by Waylan Limberg (https://github.com/waylan),\nDmitry Shachnev (https://github.com/mitya57) and Isaac Muse (https://github.com/facelessuser).\n\nCopyright 2007-2023 The Python Markdown Project (v. 1.7 and later)\nCopyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b)\nCopyright 2004 Manfred Stienstra (the original version)\n\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#gemoji-index","title":"Gemoji Index","text":"<p><code>gemoji_db.py</code> is generated from Gemoji's source code: @github/gemoji.</p> <pre><code>Copyright (c) 2013 GitHub, Inc.\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \"Software\"), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"venv/lib/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#emojione-index","title":"EmojiOne Index","text":"<p><code>emoji1_db.py</code> is generated from EmojiOne's source code: @Ranks/emojione</p> <pre><code>EmojiOne Non-Artwork\n\nApplies to the JavaScript, JSON, PHP, CSS, HTML files, and everything else not covered under the artwork license above.\nLicense: MIT\nComplete Legal Terms: http://opensource.org/licenses/MIT\n</code></pre>"},{"location":"venv/lib64/python3.12/site-packages/backrefs-5.9.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>The MIT License (MIT)</p> <p>Copyright (c) 2015 - 2025 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib64/python3.12/site-packages/idna-3.10.dist-info/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright    notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimer in the    documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib64/python3.12/site-packages/markdown-3.8.2.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright 2007, 2008 The Python Markdown Project (v. 1.7 and later) Copyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b) Copyright 2004 Manfred Stienstra (the original version)</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib64/python3.12/site-packages/mkdocs_get_deps-0.2.0.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2023 Oleh Prypin oleh@pryp.in</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib64/python3.12/site-packages/mkdocs_material_extensions-1.3.1.dist-info/licenses/LICENSE/","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2021 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib64/python3.12/site-packages/pip-25.2.dist-info/licenses/src/pip/_vendor/idna/LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright    notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimer in the    documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/","title":"License","text":""},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#pymdown-extensions","title":"PyMdown Extensions","text":"<p>The MIT License (MIT)</p> <p>Copyright (c) 2014 - 2025 Isaac Muse</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#superfences","title":"SuperFences","text":"<p><code>superfences.py</code> is derived from Python Markdown's fenced_code extension.</p> <pre><code>Fenced Code Extension for Python Markdown\n =========================================\nThis extension adds Fenced Code Blocks to Python-Markdown.\nSee &lt;https://python-markdown.github.io/extensions/fenced_code_blocks/&gt;\nfor documentation.\nOriginal code Copyright 2007-2008 [Waylan Limberg](http://achinghead.com/).\nAll changes Copyright 2008-2014 The Python Markdown Project\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#highlight","title":"Highlight","text":"<p><code>highlight.py</code> is derived from Python Markdown's CodeHilite extension.</p> <pre><code>CodeHilite Extension for Python-Markdown\n ========================================\nAdds code/syntax highlighting to standard Python-Markdown code blocks.\nSee &lt;https://python-markdown.github.io/extensions/code_hilite/&gt;\nfor documentation.\nOriginal code Copyright 2006-2008 [Waylan Limberg](http://achinghead.com/).\nAll changes Copyright 2008-2014 The Python Markdown Project\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#fancylists","title":"FancyLists","text":"<p><code>fancylists.py</code> is derived from Python Markdown's list handler.</p> <pre><code>Started by Manfred Stienstra (http://www.dwerg.net/).\nMaintained for a few years by Yuri Takhteyev (http://www.freewisdom.org).\nCurrently maintained by Waylan Limberg (https://github.com/waylan),\nDmitry Shachnev (https://github.com/mitya57) and Isaac Muse (https://github.com/facelessuser).\n\nCopyright 2007-2023 The Python Markdown Project (v. 1.7 and later)\nCopyright 2004, 2005, 2006 Yuri Takhteyev (v. 0.2-1.6b)\nCopyright 2004 Manfred Stienstra (the original version)\n\nLicense: [BSD](http://www.opensource.org/licenses/bsd-license.php)\n</code></pre>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#gemoji-index","title":"Gemoji Index","text":"<p><code>gemoji_db.py</code> is generated from Gemoji's source code: @github/gemoji.</p> <pre><code>Copyright (c) 2013 GitHub, Inc.\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation\nfiles (the \"Software\"), to deal in the Software without\nrestriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following\nconditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nWHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n</code></pre>"},{"location":"venv/lib64/python3.12/site-packages/pymdown_extensions-10.16.1.dist-info/licenses/LICENSE/#emojione-index","title":"EmojiOne Index","text":"<p><code>emoji1_db.py</code> is generated from EmojiOne's source code: @Ranks/emojione</p> <pre><code>EmojiOne Non-Artwork\n\nApplies to the JavaScript, JSON, PHP, CSS, HTML files, and everything else not covered under the artwork license above.\nLicense: MIT\nComplete Legal Terms: http://opensource.org/licenses/MIT\n</code></pre>"}]}