# Plan: Admin Feature to Retire Partitions with Outdated Schemas

## Overview

Add admin DataFusion User-Defined Table Functions (UDTFs) that allow easy identification and retirement of partitions with outdated schemas. These functions will be callable from the Python API and allow for schema evolution by cleaning up partitions that were created with older schema versions.

## Current State Analysis

### Existing Infrastructure

1. **Schema Hash System**: Each view has a `file_schema_hash` that identifies the schema version
   - Stored in `lakehouse_partitions.file_schema_hash` column
   - Generated by each view's `get_file_schema_hash()` method
   - Used to detect when partitions are out of date

2. **Existing Retire Functionality**: 
   - `retire_partitions()` function in `rust/analytics/src/lakehouse/write_partition.rs`
   - Admin CLI command `retire-partitions` in `rust/telemetry-admin-cli/src/telemetry_admin.rs`
   - Table function for SQL-based retirement in `rust/analytics/src/lakehouse/retire_partitions_table_function.rs`

3. **DataFusion Table Function Registration**:
   - UDTFs registered in `rust/analytics/src/lakehouse/query.rs:register_lakehouse_functions()`
   - Existing functions: `list_partitions`, `retire_partitions`, `materialize_partitions`, etc.
   - Called via `ctx.register_udtf()` method

4. **Database Schema**: 
   - `lakehouse_partitions` table tracks all partitions with their schema hashes
   - Indexed by view names and time ranges

### Gap Analysis

The current `retire_partitions` table function requires manual specification of:
- `view_set_name`
- `view_instance_id` 
- `begin` time
- `end` time

There's no automated way to:
- Identify partitions with outdated schemas
- Bulk retire all outdated partitions across views
- Get a report of schema versions across the system

## Proposed Solution

### Phase 1: Schema Analysis UDTFs

Add new DataFusion User-Defined Table Functions for schema analysis:

1. **`list_view_sets()`**: Show available view sets from the ViewFactory with their current schema information
   ```sql
   SELECT * FROM list_view_sets();
   ```
   Output: Table with columns: view_set_name, current_schema_hash, has_view_maker, global_instance_available
   
   Note: This shows one row per view set (like "log_entries", "measures") with information about whether that view set has a global instance and supports creating specific instances via ViewMaker.

2. **`list_outdated_partitions([view_set_name])`**: Identify partitions with outdated schemas
   ```sql
   SELECT * FROM list_outdated_partitions();
   SELECT * FROM list_outdated_partitions('log_entries');
   ```
   Output: Table with columns: view_set_name, view_instance_id, outdated_schema_hash, current_schema_hash, partition_count

### Phase 2: Automated Retirement UDTFs

3. **`retire_incompatible_partitions(view_set_name)`**: Retire partitions with incompatible schemas
   ```sql
   SELECT * FROM retire_incompatible_partitions('log_entries'); -- Retire log_entries partitions
   SELECT * FROM retire_incompatible_partitions('measures'); -- Retire measures partitions
   ```

## Technical Implementation

### File Changes Required

1. **`rust/analytics/src/lakehouse/list_view_sets_table_function.rs`** (new file)
   - `ListViewSetsTableFunction`: UDTF implementation
   - `ListViewSetsTableProvider`: TableProvider for schema versions

2. **`rust/analytics/src/lakehouse/list_outdated_partitions_table_function.rs`** (new file)
   - `ListOutdatedPartitionsTableFunction`: UDTF implementation
   - `ListOutdatedPartitionsTableProvider`: TableProvider for outdated partitions

3. **`rust/analytics/src/lakehouse/retire_incompatible_partitions_table_function.rs`** (new file)
   - `RetireIncompatiblePartitionsTableFunction`: UDTF implementation
   - `RetireIncompatiblePartitionsTableProvider`: TableProvider for bulk retirement

4. **`rust/analytics/src/lakehouse/partition_management.rs`** (new file)
   - Shared logic for partition analysis and management operations
   - `get_current_schema_versions()`: Query current schema hashes from view factory
   - `find_outdated_partitions()`: Compare partition schema hashes with current
   - `retire_incompatible_partitions_impl()`: Bulk retirement logic
   - Could also house existing partition UDFs like `list_partitions` and `retire_partitions`

5. **`rust/analytics/src/lakehouse/mod.rs`**
   - Export new modules

6. **`rust/analytics/src/lakehouse/query.rs`**
   - Register new UDTFs in `register_lakehouse_functions()`

### Database Queries

#### Current View Sets Query
```sql
-- This queries the ViewFactory directly to get available view sets
-- Implementation will iterate through:
-- 1. ViewFactory.get_global_views() - for global view instances  
-- 2. ViewFactory.view_sets keys - for available view set names with ViewMakers
-- Returns view metadata from the View trait methods:
--   - get_view_set_name()
--   - get_view_instance_id() 
--   - get_file_schema_hash()
--   - get_file_schema()
```

#### Outdated Partitions Query  
```sql
-- Application will join with current schema hashes from view factory
SELECT p.view_set_name, p.view_instance_id, p.file_schema_hash, COUNT(*) as partition_count
FROM lakehouse_partitions p
WHERE p.file_schema_hash != $1 -- current_schema_hash parameter
GROUP BY p.view_set_name, p.view_instance_id, p.file_schema_hash
ORDER BY p.view_set_name, p.view_instance_id;
```

### UDTF Implementation Pattern

Following the existing pattern from `list_partitions_table_function.rs`:

```rust
#[derive(Debug)]
pub struct ListViewSetsTableFunction {
    lake: Arc<DataLakeConnection>,
    view_factory: Arc<ViewFactory>,
}

impl TableFunctionImpl for ListViewSetsTableFunction {
    fn call(&self, args: &[Expr]) -> datafusion::error::Result<Arc<dyn TableProvider>> {
        // Parse arguments (optional view_set_name filter)
        Ok(Arc::new(ListViewSetsTableProvider {
            lake: self.lake.clone(),
            view_factory: self.view_factory.clone(),
            view_set_filter: parsed_view_set,
        }))
    }
}

#[async_trait]
impl TableProvider for ListViewSetsTableProvider {
    fn schema(&self) -> SchemaRef {
        Arc::new(Schema::new(vec![
            Field::new("view_set_name", DataType::Utf8, false),
            Field::new("current_schema_hash", DataType::Binary, false),
            Field::new("has_view_maker", DataType::Boolean, false),
            Field::new("global_instance_available", DataType::Boolean, false),
        ]))
    }
    
    async fn scan(&self, ...) -> Result<Arc<dyn ExecutionPlan>> {
        // Implementation:
        // 1. Collect all unique view set names from ViewFactory.get_global_views()
        // 2. Collect view set names from ViewFactory.view_sets keys (ViewMakers)  
        // 3. For each unique view set name:
        //    - Get schema hash from global view instance (if available) or create temp instance
        //    - Check if view set has ViewMaker (supports non-global instances)
        //    - Check if view set has global instance available
        // 4. Build RecordBatch with one row per view set and return MemoryExec
    }
}
```

### Safety Features

1. **Transactional Operations**: All retirement operations use database transactions
2. **Detailed Logging**: Log all retirement operations for audit trail via Logger trait
3. **Parameter Validation**: Validate schema hashes and view names
4. **Concurrent Safety**: Handle concurrent partition updates during retirement

## Implementation Steps

### Step 1: Partition Management Infrastructure
1. Create `partition_management.rs` module with shared logic
2. Implement schema version discovery from view factory
3. Add utility functions for comparing current vs stored schema hashes
4. Consider moving existing partition UDFs (`list_partitions`, `retire_partitions`) to this module for better organization

### Step 2: Analysis UDTFs  
1. Implement `list_view_sets_table_function.rs`
2. Implement `list_outdated_partitions_table_function.rs`
3. Register functions in `query.rs`
4. Add unit tests

### Step 3: Retirement UDTFs
1. Implement `retire_incompatible_partitions_table_function.rs`
2. Add comprehensive logging and error handling
3. Add integration tests

### Step 4: Testing & Documentation
1. Add unit tests for partition management functions
2. Add integration tests for UDTF functionality
3. Update analytics service documentation

## Usage Examples

### From Python API
```python
import micromegas

# Connect to analytics service
client = micromegas.AnalyticsClient('localhost:32010')

# See current schema versions across all views
schema_versions = client.sql("SELECT * FROM list_view_sets()")

# Find outdated partitions for log_entries view
outdated = client.sql("SELECT * FROM list_outdated_partitions('log_entries')")

# Retire incompatible partitions for measures view
result = client.sql("SELECT * FROM retire_incompatible_partitions('measures')")
```

### Direct SQL Usage
```sql
-- See current schema versions across all views
SELECT * FROM list_view_sets();

-- Find outdated partitions for log_entries view
SELECT * FROM list_outdated_partitions('log_entries');

-- Retire incompatible partitions for measures view
SELECT * FROM retire_incompatible_partitions('measures');
```

## Function Registration

Update `rust/analytics/src/lakehouse/query.rs:register_lakehouse_functions()`:

```rust
pub fn register_lakehouse_functions(
    ctx: &SessionContext,
    runtime: Arc<RuntimeEnv>,
    lake: Arc<DataLakeConnection>,
    part_provider: Arc<dyn QueryPartitionProvider>,
    query_range: Option<TimeRange>,
    view_factory: Arc<ViewFactory>,
    object_store: Arc<dyn ObjectStore>,
) {
    // ... existing registrations ...
    
    // Schema management functions
    ctx.register_udtf(
        "list_view_sets",
        Arc::new(ListViewSetsTableFunction::new(
            lake.clone(),
            view_factory.clone(),
        )),
    );
    ctx.register_udtf(
        "list_outdated_partitions",
        Arc::new(ListOutdatedPartitionsTableFunction::new(
            lake.clone(),
            view_factory.clone(),
        )),
    );
    ctx.register_udtf(
        "retire_incompatible_partitions",
        Arc::new(RetireIncompatiblePartitionsTableFunction::new(
            lake.clone(),
            view_factory.clone(),
        )),
    );
}
```

## Benefits

1. **API Integration**: Direct access from Python API for automation
2. **SQL Flexibility**: Use standard SQL queries for complex analysis
3. **Operational Efficiency**: Automate identification and cleanup of outdated partitions
4. **Schema Evolution**: Enable safe schema changes by cleaning up old versions  
5. **Storage Optimization**: Remove partitions that can't be queried due to schema mismatches
6. **Data Consistency**: Ensure all partitions use current schema versions
7. **Observability**: Provide visibility into schema version distribution via SQL

## Current Implementation Status

### âœ… Phase 1 - Completed (list_view_sets)

1. **`catalog.rs`** (COMPLETED - renamed from partition_management.rs)
   - Location: `rust/analytics/src/lakehouse/catalog.rs`
   - Provides `list_view_sets()` function (renamed from get_current_schema_versions)
   - Defines `ViewSetInfo` struct (renamed from ViewSetSchemaInfo) with schema metadata
   - Integrates with ViewFactory to discover available view sets and their schemas
   - Now includes `schema` field showing full schema as string

2. **`list_view_sets_table_function.rs`** (COMPLETED)
   - Location: `rust/analytics/src/lakehouse/list_view_sets_table_function.rs`
   - Implements `ListViewSetsTableFunction` UDTF
   - Returns: view_set_name, current_schema_hash, schema, has_view_maker, global_instance_available
   - Registered as `list_view_sets` table function in DataFusion

3. **ViewMaker Trait Enhancement** (COMPLETED)
   - Added `get_schema_hash()` and `get_schema()` methods to ViewMaker trait
   - Avoids need to instantiate views just to get schema information
   - All ViewMaker implementations updated:
     - `LogViewMaker`: SCHEMA_VERSION = 4
     - `MetricsViewMaker`: SCHEMA_VERSION = 4  
     - `ThreadSpansViewMaker`: SCHEMA_VERSION = 0
     - `AsyncEventsViewMaker`: SCHEMA_VERSION = 1
   - Each view has centralized SCHEMA_VERSION constant

4. **ViewFactory Enhancement** (COMPLETED)
   - Added `get_view_sets()` method to provide public access to view makers
   - Catalog functions can now get schema info without creating view instances

5. **Registration in DataFusion** (COMPLETED)
   - Updated `query.rs` to register the `list_view_sets` UDTF
   - Updated `mod.rs` to export the new modules
   - Successfully compiles and integrates with existing lakehouse functions

### ðŸ”„ Phase 2 - Next Steps

**1. Unit Tests for Catalog** (IMMEDIATE NEXT STEP)
- Add tests to existing analytics test suite in tests folder
- Test `list_view_sets()` function with real ViewFactory
- Test ViewSetInfo struct population for all view types
- Verify schema hashes match expected values
- Test UDTF execution through DataFusion SQL
- Verify all view sets are discovered correctly
- Test schema string formatting

**2. Implementation of `list_outdated_partitions`:**
- Implement `list_outdated_partitions_table_function.rs`
- Add database queries to compare current vs stored schema hashes
- Register the function in DataFusion
- Write integration tests

**3. Implementation of `retire_incompatible_partitions`:**
- Implement `retire_incompatible_partitions_table_function.rs`
- Add bulk retirement logic with proper error handling
- Integrate with existing retirement infrastructure
- Write integration tests with rollback capabilities

### ðŸ“Š Usage

The `list_view_sets()` function is now available in SQL:
```sql
-- See current schema versions across all view sets
SELECT * FROM list_view_sets();
```

Expected output columns:
- `view_set_name`: Name of the view set (e.g., "log_entries", "measures")  
- `current_schema_hash`: Binary hash identifying current schema version
- `schema`: Full schema as a formatted string
- `has_view_maker`: Boolean indicating if view set supports non-global instances
- `global_instance_available`: Boolean indicating if a global instance exists

## Risks & Mitigations

1. **Data Loss Risk**: Mitigated by transactional operations and comprehensive logging
2. **Performance Impact**: Mitigated by efficient database queries and batched operations
3. **Schema Detection Issues**: Mitigated by robust error handling and logging
4. **Concurrent Operations**: Mitigated by proper transaction handling
5. **Parameter Validation**: Mitigated by thorough input validation
