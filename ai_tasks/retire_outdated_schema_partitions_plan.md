# Plan: Admin Feature to Retire Partitions with Outdated Schemas

## Overview

Add admin DataFusion User-Defined Table Functions (UDTFs) that allow easy identification and retirement of partitions with outdated schemas. These functions will be callable from the Python API and allow for schema evolution by cleaning up partitions that were created with older schema versions.

## Definitions

**Outdated Partitions**: Partitions that should be deleted due to lifecycle management policies, typically based on age, storage constraints, or business requirements. These partitions may have current schemas but are removed for operational reasons.

**Incompatible Partitions**: Partitions that have a schema different from the latest/current schema version for their view set. These partitions cannot be queried correctly alongside current partitions due to schema mismatches and should be retired to enable schema evolution.

**Schema Hash**: A version identifier (currently stored as arrays like `[4]`) that uniquely identifies the schema version used when a partition was created. Found in `lakehouse_partitions.file_schema_hash` column.

**Retirement**: The process of removing partition metadata from the database and optionally deleting the associated data files from object storage. This is an irreversible operation.

## Current State Analysis

### Existing Infrastructure

1. **Schema Hash System**: Each view has a `file_schema_hash` that identifies the schema version
   - Stored in `lakehouse_partitions.file_schema_hash` column
   - Generated by each view's `get_file_schema_hash()` method
   - Used to detect when partitions are out of date

2. **Existing Retire Functionality**: 
   - `retire_partitions()` function in `rust/analytics/src/lakehouse/write_partition.rs`
   - Admin CLI command `retire-partitions` in `rust/telemetry-admin-cli/src/telemetry_admin.rs`
   - Table function for SQL-based retirement in `rust/analytics/src/lakehouse/retire_partitions_table_function.rs`

3. **DataFusion Table Function Registration**:
   - UDTFs registered in `rust/analytics/src/lakehouse/query.rs:register_lakehouse_functions()`
   - Existing functions: `list_partitions`, `retire_partitions`, `materialize_partitions`, etc.
   - Called via `ctx.register_udtf()` method

4. **Database Schema**: 
   - `lakehouse_partitions` table tracks all partitions with their schema hashes
   - Indexed by view names and time ranges

## Implementation Plan

### Phase 1: Schema Discovery (‚úÖ COMPLETED)

**`list_view_sets()`** - Show available view sets with their current schema information
```sql
SELECT * FROM list_view_sets();
```
Output columns:
- `view_set_name`: Name of the view set (e.g., "log_entries", "measures")
- `current_schema_hash`: Binary hash identifying current schema version  
- `schema`: Full schema as a formatted string
- `has_view_maker`: Boolean indicating if view set supports non-global instances
- `global_instance_available`: Boolean indicating if a global instance exists

**Implementation details:**
- Created `catalog.rs` with `list_view_sets()` function and `ViewSetInfo` struct
- Created `list_view_sets_table_function.rs` UDTF implementation
- Enhanced `ViewMaker` trait with `get_schema_hash()` and `get_schema()` methods
- Updated all ViewMaker implementations with SCHEMA_VERSION constants
- Added `get_view_sets()` to ViewFactory for catalog access
- Registered in DataFusion query context

### Phase 2: Partition Analysis (‚úÖ COMPLETED)

**Unit Tests Added** - `/home/mad/micromegas/rust/analytics/tests/catalog_tests.rs`
- ‚úÖ `test_list_view_sets_catalog()` - Tests basic functionality and column structure
- ‚úÖ `test_view_sets_schema_hash_consistency()` - Validates schema versions (version numbers like `[4]`, not SHA-256 hashes)
- ‚úÖ `test_view_sets_properties()` - Tests view set properties (global_instance_available, has_view_maker)
- ‚úÖ Tests work without database connection using minimal ViewFactory setup
- ‚úÖ Verified log_entries and measures view sets both have schema version `[4]` and are view-maker-only

**Key Discovery**: Schema "hashes" are actually version numbers (e.g., `[4]`) not cryptographic hashes

### Phase 3: Incompatible Partition Discovery (üìù PLANNED)

**`micromegas.admin.list_incompatible_partitions(client, [view_set_name])`** - Function to identify partitions with incompatible schemas

```python
# Usage examples
import micromegas
import micromegas.admin

client = micromegas.connect()

# List all incompatible partitions across all view sets
incompatible = micromegas.admin.list_incompatible_partitions(client)

# List incompatible partitions for specific view set
incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')
```

Returns pandas DataFrame with columns:
- `view_set_name`: View set name
- `view_instance_id`: Instance ID (e.g., process_id or 'global')
- `incompatible_schema_hash`: The old schema hash in the partition
- `current_schema_hash`: The current schema hash from ViewFactory
- `partition_count`: Number of incompatible partitions with this schema
- `total_size_bytes`: Total size in bytes of all incompatible partitions in this group
- `file_paths`: Array of file paths for each incompatible partition (for precise retirement)

**Implementation approach:**
- Create new `micromegas/admin.py` module for administrative utilities
- Function takes `FlightSQLClient` instance as first parameter
- Uses SQL JOIN between `list_partitions()` and `list_view_sets()` UDTFs
- Server-side filtering and aggregation for optimal performance
- No Rust code required - leverages existing infrastructure
- Optional view_set_name parameter for filtering

### Phase 4: Automated Retirement (üìù PLANNED)

**`micromegas.admin.retire_incompatible_partitions(client, [view_set_name])`** - Function to retire partitions with incompatible schemas

```python
# Usage examples
import micromegas
import micromegas.admin

client = micromegas.connect()

# Preview what would be retired (use list function)
preview = micromegas.admin.list_incompatible_partitions(client)
print(f"Would retire {preview['partition_count'].sum()} partitions")

# Retire incompatible partitions for specific view set
result = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')

# Retire all incompatible partitions (use with caution)
result = micromegas.admin.retire_incompatible_partitions(client)
```

Returns pandas DataFrame with columns:
- `view_set_name`: View set that was processed
- `view_instance_id`: Instance ID of partitions retired
- `partitions_retired`: Count of partitions successfully retired
- `storage_freed_bytes`: Total bytes freed from storage
- `retirement_messages`: Array of detailed messages for each partition retirement attempt

**Implementation approach:**
- Add `retire_incompatible_partitions()` function to `micromegas/admin.py` module
- Function takes `FlightSQLClient` instance as first parameter
- Function first calls `list_incompatible_partitions()` to identify targets
- For each incompatible partition group, constructs time-based retirement calls
- Uses existing `retire_partitions()` UDTF via SQL for actual retirement
- Aggregates results and provides summary statistics
- No new Rust code required - orchestrates existing functionality

## Technical Details

### Python Implementation Logic

The Python functions in `micromegas/admin.py` leverage existing UDTFs and perform data processing server-side using SQL:

1. **`list_incompatible_partitions(client, view_set_name=None)` Flow:**
   ```python
   def list_incompatible_partitions(client, view_set_name=None):
       # Build SQL query with join and aggregation
       view_filter = ""
       if view_set_name:
           view_filter = f"AND p.view_set_name = '{view_set_name}'"
       
       sql = f"""
       SELECT 
           p.view_set_name,
           p.view_instance_id, 
           p.file_schema_hash as incompatible_schema_hash,
           vs.current_schema_hash,
           COUNT(*) as partition_count,
           SUM(p.file_size) as total_size_bytes
       FROM list_partitions() p
       JOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name
       WHERE p.file_schema_hash != vs.current_schema_hash
           {view_filter}
       GROUP BY p.view_set_name, p.view_instance_id, p.file_schema_hash, vs.current_schema_hash
       ORDER BY p.view_set_name, p.view_instance_id
       """
       
       return client.query(sql)
   ```

2. **`retire_incompatible_partitions(client, view_set_name=None)` Flow:**
   ```python
   def retire_incompatible_partitions(client, view_set_name=None):
       # First identify incompatible partitions
       incompatible = list_incompatible_partitions(client, view_set_name)
       
       results = []
       
       # For each group, determine time range and call existing retire_partitions UDTF
       for _, group in incompatible.iterrows():
           # Query partition time ranges 
           time_ranges = client.query(f"""
               SELECT MIN(begin_insert_time) as min_time, MAX(end_insert_time) as max_time
               FROM list_partitions()
               WHERE view_set_name='{group.view_set_name}' 
                 AND view_instance_id='{group.view_instance_id}'
                 AND file_schema_hash='{group.incompatible_schema_hash}'
           """)
           
           # Call existing retire_partitions UDTF
           retirement_result = client.query(f"""
               SELECT * FROM retire_partitions(
                   '{group.view_set_name}', 
                   '{group.view_instance_id}',
                   '{time_ranges.min_time[0]}',
                   '{time_ranges.max_time[0]}'
               )
           """)
           
           # Aggregate results
           results.append({
               'view_set_name': group.view_set_name,
               'view_instance_id': group.view_instance_id,
               'partitions_retired': group.partition_count,
               'storage_freed_bytes': retirement_result.get('storage_freed_bytes', 0)
           })
       
       return pd.DataFrame(results)
   ```

### Safety Features

1. **Preview First**: Use `list_incompatible_partitions()` to preview what would be retired before calling retirement function
2. **Existing UDTF Safety**: Leverages existing `retire_partitions()` UDTF which includes transaction safety
3. **Parameter Validation**: Validate view set names and schema hash formats
4. **Detailed Logging**: Python function logs all operations and provides summary statistics
5. **Granular Control**: Optional view_set_name filtering prevents bulk operations across all views
6. **Explicit Action**: No dry-run option forces deliberate decision to retire partitions

## Usage Examples

### Python API Usage
```python
import micromegas
import micromegas.admin

# Connect to analytics service
client = micromegas.connect()

# See current schema versions across all views (existing UDTF)
schema_versions = client.query("SELECT * FROM list_view_sets()")
print(schema_versions)

# Find incompatible partitions for log_entries view
incompatible = micromegas.admin.list_incompatible_partitions(client, 'log_entries')
print(f"Found {len(incompatible)} groups of incompatible partitions")
print(f"Total incompatible partitions: {incompatible['partition_count'].sum()}")
print(f"Total size to be freed: {incompatible['total_size_bytes'].sum() / (1024**3):.2f} GB")

# Preview what would be retired
print("Preview of partitions to be retired:")
print(incompatible[['view_set_name', 'view_instance_id', 'partition_count', 'total_size_bytes']])

# Actually retire incompatible partitions (after reviewing preview)
if input("Proceed with retirement? (yes/no): ") == "yes":
    result = micromegas.admin.retire_incompatible_partitions(client, 'log_entries')
    print(f"Retired {result['partitions_retired'].sum()} partitions")
    print(f"Freed {result['storage_freed_bytes'].sum()} bytes")

# Find all incompatible partitions across all view sets
all_incompatible = micromegas.admin.list_incompatible_partitions(client)
for vs in all_incompatible['view_set_name'].unique():
    vs_incompatible = all_incompatible[all_incompatible['view_set_name'] == vs]
    total_partitions = vs_incompatible['partition_count'].sum()
    total_size_gb = vs_incompatible['total_size_bytes'].sum() / (1024**3)
    print(f"{vs}: {total_partitions} incompatible partitions ({total_size_gb:.2f} GB)")
```

### Direct SQL Usage (for advanced users)
```sql
-- See current schema versions across all views  
SELECT * FROM list_view_sets();

-- List all incompatible partitions with size information and file paths
SELECT 
    p.view_set_name,
    p.view_instance_id, 
    p.file_schema_hash as incompatible_schema_hash,
    vs.current_schema_hash,
    COUNT(*) as partition_count,
    SUM(p.file_size) as total_size_bytes,
    SUM(p.file_size) / (1024.0 * 1024.0 * 1024.0) as total_size_gb,
    ARRAY_AGG(p.file_path) as file_paths
FROM list_partitions() p
JOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name
WHERE p.file_schema_hash != vs.current_schema_hash
GROUP BY p.view_set_name, p.view_instance_id, p.file_schema_hash, vs.current_schema_hash
ORDER BY total_size_bytes DESC;

-- List incompatible partitions for specific view set with file paths
SELECT 
    p.view_set_name,
    p.view_instance_id, 
    p.file_schema_hash as incompatible_schema_hash,
    vs.current_schema_hash,
    COUNT(*) as partition_count,
    SUM(p.file_size) as total_size_bytes,
    ARRAY_AGG(p.file_path) as file_paths
FROM list_partitions() p
JOIN list_view_sets() vs ON p.view_set_name = vs.view_set_name
WHERE p.file_schema_hash != vs.current_schema_hash
    AND p.view_set_name = 'log_entries'
GROUP BY p.view_set_name, p.view_instance_id, p.file_schema_hash, vs.current_schema_hash
ORDER BY p.view_set_name, p.view_instance_id;

-- Get time ranges for specific incompatible partition group (for manual retirement)
SELECT 
    MIN(begin_insert_time) as min_time, 
    MAX(end_insert_time) as max_time
FROM list_partitions()
WHERE view_set_name = 'log_entries' 
    AND view_instance_id = 'process-123'
    AND file_schema_hash = '[3]';

-- Manually retire specific incompatible partitions (use existing UDTF)
SELECT * FROM retire_partitions('log_entries', 'process-123', '2024-01-01T00:00:00Z', '2024-01-02T00:00:00Z');

-- Summary statistics of incompatible partitions by view set
SELECT 
    vs.view_set_name,
    vs.current_schema_hash,
    COUNT(DISTINCT p.file_schema_hash) as incompatible_schema_count,
    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN 1 ELSE 0 END) as incompatible_partition_count,
    SUM(CASE WHEN p.file_schema_hash != vs.current_schema_hash THEN p.file_size ELSE 0 END) as incompatible_size_bytes
FROM list_view_sets() vs
LEFT JOIN list_partitions() p ON vs.view_set_name = p.view_set_name
GROUP BY vs.view_set_name, vs.current_schema_hash
HAVING incompatible_partition_count > 0
ORDER BY incompatible_size_bytes DESC;
```

## Benefits

1. **Minimal Rust Development**: Only required Phase 1 catalog infrastructure, remaining phases use Python
2. **Faster Implementation**: Leverages existing `list_partitions()` and completed `list_view_sets()` infrastructure
3. **API Integration**: Direct access from Python API for automation scripts
4. **Safety First**: Dry-run default prevents accidental data loss
5. **Operational Efficiency**: Automate identification and cleanup of incompatible partitions
6. **Schema Evolution**: Enable safe schema changes by cleaning up old versions  
7. **Storage Optimization**: Remove partitions that can't be queried due to schema mismatches
8. **Data Consistency**: Ensure all partitions use current schema versions
9. **Flexible Filtering**: Optional view_set_name parameter for targeted cleanup
10. **SQL Compatibility**: Advanced users can still use direct SQL with existing UDTFs

## Risks & Mitigations

1. **Data Loss Risk**: Mitigated by dry-run default, existing UDTF transaction safety, and comprehensive logging
2. **Performance Impact**: Mitigated by leveraging existing efficient UDTFs and optional view filtering
3. **Schema Detection Issues**: Mitigated by using existing proven `list_view_sets()` infrastructure 
4. **Concurrent Operations**: Mitigated by existing `retire_partitions()` UDTF transaction handling
5. **Parameter Validation**: Mitigated by Python input validation and error handling
6. **Server-Side Processing**: SQL JOIN and aggregation performed efficiently on the server
7. **üö® CRITICAL SAFETY ISSUE - Mixed Schema Retirement**: The current `retire_incompatible_partitions()` implementation has a serious bug where it retires ALL partitions in a time range, not just incompatible ones. This could accidentally delete compatible partitions that happen to exist in the same time range as incompatible partitions.

### Critical Issue Details:
**Problem**: The current Phase 4 implementation uses time-based retirement:
```python
# DANGEROUS: This retires ALL partitions in time range, not just incompatible ones
retirement_sql = f"""
    SELECT * FROM retire_partitions(
        '{escaped_view_set}', 
        '{escaped_view_instance}',
        '{min_time}',
        '{max_time}'
    )
"""
```

**Why this is dangerous**: 
- Compatible partitions with current schemas may exist in the same time ranges
- The `retire_partitions()` UDTF retires by time range, not by schema hash
- This could cause data loss of perfectly valid, queryable partitions

**Required Fix**: Need to implement partition-specific retirement that targets only the exact incompatible partitions, not time ranges. This requires either:

**SELECTED SOLUTION - Option A**: Implement file-path-specific retirement UDF
- Create new `retire_partition_by_file()` UDTF that retires a single partition by its file path
- Modify `list_incompatible_partitions()` to return file paths for each incompatible partition  
- Modify `retire_incompatible_partitions()` to retire partitions individually by file path
- Ensures surgical precision - only exact incompatible partitions are retired, no risk of collateral damage

### Implementation Plan for Option A:

**Phase 4a: Enhance list_incompatible_partitions() to return file paths**
```python
# Updated return columns to include file_path for precise retirement
return pd.DataFrame(columns=[
    'view_set_name', 'view_instance_id', 'incompatible_schema_hash',
    'current_schema_hash', 'partition_count', 'total_size_bytes',
    'file_paths'  # NEW: Array of file paths for each incompatible partition
])
```

**Phase 4b: Create retire_partition_by_file() Rust UDF**
```rust
// New UDF in rust/analytics/src/lakehouse/
// Signature: retire_partition_by_file(file_path: String) -> String
// Retires single partition by exact file path match
// Returns descriptive message: "SUCCESS: Retired partition <file_path>" or "ERROR: Partition not found: <file_path>"
```

**Phase 4c: Update retire_incompatible_partitions() for safety**
```python
def retire_incompatible_partitions(client, view_set_name=None):
    # Get incompatible partitions with their file paths
    incompatible = list_incompatible_partitions(client, view_set_name)
    
    results = []
    for _, group in incompatible.iterrows():
        retired_count = 0
        retirement_messages = []
        
        # Retire each individual file path
        for file_path in group['file_paths']:
            result = client.query(f"SELECT retire_partition_by_file('{file_path}') as message")
            message = result['message'].iloc[0]
            retirement_messages.append(message)
            
            if message.startswith("SUCCESS:"):
                retired_count += 1
            else:
                print(f"Warning: {message}")  # Log errors but continue
        
        results.append({
            'view_set_name': group['view_set_name'],
            'view_instance_id': group['view_instance_id'], 
            'partitions_retired': retired_count,
            'storage_freed_bytes': group['total_size_bytes'] if retired_count == len(group['file_paths']) else 0,
            'retirement_messages': retirement_messages  # Include all messages for debugging
        })
    
    return pd.DataFrame(results)
```

**Benefits of Option A:**
- **Surgical precision**: Only targets exact incompatible partitions
- **Zero risk**: Impossible to accidentally retire compatible partitions  
- **Performance**: Efficient individual partition retirement
- **Detailed feedback**: Descriptive messages for each retirement operation (success/failure reasons)
- **Auditability**: Clear one-to-one mapping of file path to retirement action with full message logs
- **Error resilience**: Continues processing even if some partitions fail to retire
- **Rollback friendly**: Could theoretically support undelete if needed

**Status**: üöß **IMPLEMENTATION REQUIRED** 
1. **First, verify `list_partitions()` schema**: Confirm the exact column name for file paths (likely `file_path`, `path`, or `partition_path`)
2. **Modify `list_incompatible_partitions()` SQL**: Add file paths using `ARRAY_AGG(p.<file_path_column>)` (‚úÖ DataFusion supports ARRAY_AGG)
3. **Create `retire_partition_by_file()` Rust UDF**: Scalar function returning descriptive messages
4. **Update `retire_incompatible_partitions()`**: Use file-path-based retirement
5. **Update tests**: Validate new safe retirement approach

**Note on ARRAY_AGG**: ‚úÖ **DataFusion supports ARRAY_AGG** as confirmed by Apache DataFusion documentation. It can be used with ordering: `ARRAY_AGG(column ORDER BY other_column)` and supports complex data types as of DataFusion 34.0.0 (2024).

## Summary of Revised Plan

**Key Changes from Original:**
- **Phase 3**: Changed from Rust UDTF to Python function using existing `list_partitions()` and `list_view_sets()`
- **Phase 4**: Changed from Rust UDTF to Python function orchestrating existing `retire_partitions()` UDTF
- **Minimal new Rust code**: Only Phase 1 catalog work required, remaining phases leverage existing infrastructure

**Implementation Strategy:**
1. ‚úÖ **Phase 1 Complete**: `list_view_sets()` UDTF provides current schema versions
2. ‚úÖ **Phase 2 Complete**: Partition Analysis and Unit Tests added
3. ‚úÖ **Phase 3 Complete**: `micromegas.admin.list_incompatible_partitions()` function implemented in Python
4. üöß **Phase 4 NEEDS REWORK**: Current implementation unsafe - implementing file-path-based retirement:
   - **Phase 4a**: üìù Enhance `list_incompatible_partitions()` to return file paths using `ARRAY_AGG(p.file_path)`
   - **Phase 4b**: üìù Create `retire_partition_by_file()` Rust UDF (scalar function) for surgical partition retirement
   - **Phase 4c**: üìù Rewrite `retire_incompatible_partitions()` to use file-path-based retirement
   - **Phase 4d**: üìù Update tests to validate new safe retirement approach

**Benefits of Python Approach:**
- Faster development for Phases 3-4 (no additional Rust compilation, testing, registration)
- Leverages existing and Phase 1 UDTFs
- More intuitive API for Python users
- Easier to extend with additional logic and safety features
- Maintains consistency with other client library methods
